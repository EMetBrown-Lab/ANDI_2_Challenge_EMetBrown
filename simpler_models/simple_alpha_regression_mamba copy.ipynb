{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "# Function to pad an array to a specific shape\n",
    "def to_shape(a, shape):\n",
    "    # Unpack the target shape\n",
    "    y_, x_ = shape\n",
    "\n",
    "    # Get the current shape of the array\n",
    "    y, x = a.shape\n",
    "\n",
    "    # Calculate the padding needed in the y and x directions\n",
    "    y_pad = y_ - y\n",
    "    x_pad = x_ - x\n",
    "    output = np.zeros()\n",
    "    # Pad the array using numpy's pad function\n",
    "    return np.pad(\n",
    "        a,\n",
    "        [(0, 1), (0, 1)],\n",
    "        # Calculate the padding for each dimension\n",
    "        #((y_pad // 2, y_pad // 2 + y_pad % 2), (x_pad // 2, x_pad // 2 + x_pad % 2)),\n",
    "        mode=\"constant\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Function to pad data and labels to a specific shape\n",
    "def apply_padding(data_df, N, T_max):\n",
    "    # Define the final shape of the data and labels\n",
    "    final_shape = (N, T_max, 3)\n",
    "\n",
    "    # Initialize the final data and labels with zeros\n",
    "    final_data = np.zeros(final_shape)\n",
    "    final_label = np.zeros((N, T_max, 3))\n",
    "\n",
    "    # Select a random subset of trajectory indices\n",
    "    if len(data_df[\"traj_idx\"].unique()) < N:\n",
    "        selected_ids = np.random.choice(\n",
    "            data_df[\"traj_idx\"].unique(), size=N, replace=True\n",
    "        )\n",
    "    else:\n",
    "        selected_ids = np.random.choice(\n",
    "            data_df[\"traj_idx\"].unique(), size=N, replace=False\n",
    "        )\n",
    "\n",
    "    # Iterate over the selected trajectory indices\n",
    "    for n, id in enumerate(selected_ids):\n",
    "        # Filter the data for the current trajectory index\n",
    "        exp = data_df[data_df[\"traj_idx\"] == id]\n",
    "        \n",
    "        # Extract the data and labels for the current trajectory\n",
    "        data = exp[[\"frame\", \"x\", \"y\"]].to_numpy()\n",
    "        data[:,0] = data[:,0] - data[0,0] + 1 #putting first frame rate to 1\n",
    "        data[:,1] = data[:,1] - data[0,1] #putting initial position to 0\n",
    "        data[:,2] = data[:,2] - data[0,2] #putting initital position to 0        # print(exp[\"frame\"])\n",
    "        label = exp[[\"alpha\", \"D\", \"state\"]].to_numpy()\n",
    "        ## adding one to the states\n",
    "        label[:,2] = label[:,2] + 1\n",
    "        # If the data is longer than T_max, truncate it\n",
    "        if data.shape[0] > T_max:\n",
    "            final_data[n, :, :] = data[:T_max, :]\n",
    "            final_label[n, :, :] = label[:T_max, :]\n",
    "\n",
    "        # Otherwise, pad the data to T_max\n",
    "        else:\n",
    "            # print((label.shape, T_max))\n",
    "            final_data[n, :data.shape[0], :] = data\n",
    "            final_label[n, :data.shape[0], :] = label\n",
    "\n",
    "    # Return the padded data and labels\n",
    "    return final_data, final_label\n",
    "\n",
    "\n",
    "# Define a function to normalize data\n",
    "def normalize_df(data):\n",
    "    # Calculate displacement in x and y directions\n",
    "    # Normalize by substring mean and dividing by variance.\n",
    "\n",
    "    displacement_x = []\n",
    "    displacement_y = []\n",
    "    for _, group in data.groupby(\"traj_idx\"):\n",
    "        x = np.asarray(group[\"x\"])\n",
    "        y = np.asarray(group[\"y\"])\n",
    "        d_x = x[1:] - x[:-1]\n",
    "        d_y = y[1:] - y[:-1]\n",
    "        displacement_x = displacement_x + list(d_x)\n",
    "        displacement_y = displacement_y + list(d_y)\n",
    "\n",
    "    # Calculate variance in x and y directions\n",
    "    variance_x = np.sqrt(np.std(displacement_x))\n",
    "    variance_y = np.sqrt(np.std(displacement_y))\n",
    "\n",
    "    # Normalize data\n",
    "    data.loc[:, \"x\"] = (data[\"x\"] - data[\"x\"].mean()) / variance_x\n",
    "    data.loc[:, \"y\"] = (data[\"y\"] - data[\"y\"].mean()) / variance_y\n",
    "\n",
    "\n",
    "def normalize_np(data):\n",
    "\n",
    "    displacement_x = []\n",
    "    displacement_y = []\n",
    "    for n in range(data.shape[0]):\n",
    "        x = data[n, :, 1]\n",
    "        y = data[n, :, 2]\n",
    "        d_x = x[1:] - x[:-1]\n",
    "        d_y = y[1:] - y[:-1]\n",
    "        displacement_x = displacement_x + list(d_x)\n",
    "        displacement_y = displacement_y + list(d_y)\n",
    "\n",
    "    # Calculate variance in x and y directions\n",
    "    variance_x = np.sqrt(np.std(displacement_x))\n",
    "    variance_y = np.sqrt(np.std(displacement_y))\n",
    "\n",
    "    # Normalize data\n",
    "\n",
    "    data[:, :, 1] = (data[:, :, 1] - np.mean(data[:, :, 1])) / variance_x\n",
    "    data[:, :, 2] = (data[:, :, 2] - np.mean(data[:, :, 2])) / variance_x\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Define a function to list directory tree with pathlib\n",
    "def list_directory_tree_with_pathlib(starting_directory):\n",
    "    path_object = Path(starting_directory)\n",
    "    folders = []\n",
    "    for file_path in path_object.rglob(\"*.csv\"):\n",
    "        folders.append(file_path)\n",
    "    return folders\n",
    "\n",
    "\n",
    "# Define a custom dataset class for all data\n",
    "@dataclass\n",
    "class Dataset_all_data(Dataset):\n",
    "    # Initialize filenames and transform flag\n",
    "    # Pad value should be a tuple such as (N, Tmax)\n",
    "    filenames: list\n",
    "    transform: bool = False\n",
    "    pad: None | tuple = None\n",
    "    noise: bool = False\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of files\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read csv file and extract data and label\n",
    "        df = pd.read_csv(self.filenames[idx])\n",
    "\n",
    "        if self.pad is None:\n",
    "            data = df[[\"traj_idx\", \"frame\", \"x\", \"y\"]]\n",
    "            label = np.asarray(df[[\"alpha\", \"D\"]])\n",
    "            label_2 = np.asarray(df[\"state\"])\n",
    "\n",
    "        else:\n",
    "            if len(self.pad) != 2:\n",
    "                raise ValueError(\"pad value should be set as (N, T_max)\")\n",
    "            data, label = apply_padding(df, *self.pad)\n",
    "            data = data[:,:,1:] ## Removing the frame column\n",
    "            label_2 = label[:, :, -1]\n",
    "            label_2[label_2[:, :] > 0] = label_2[label_2[:, :] > 0] \n",
    "            label = label[:, :, :-1]\n",
    "\n",
    "        # Normalize data if transform flag is True\n",
    "        if self.transform:\n",
    "            if self.pad is None:\n",
    "                normalize_df(data)\n",
    "                data = np.asarray(data)\n",
    "            else:\n",
    "                data = normalize_np(data)\n",
    "\n",
    "        if self.noise:\n",
    "            data = add_noise(data)\n",
    "        \n",
    "        # Normalize D between 0 and 1\n",
    "\n",
    "        # label[:,:,1][label[:,:,1] != 0] = np.log(label[:,:,1][label[:,:,1] != 0]) #- np.log(1e-6)) #/   (np.log(1e12) - np.log(1e-6))\n",
    "        # label = label[:,:,1]\n",
    "        label_regression = np.zeros((label.shape[0], 2))\n",
    "\n",
    "        # print(np.unique(label_2))\n",
    "        \n",
    "        for i in range(label.shape[0]):\n",
    "            alpha = np.unique(label[i,:,0][label[i,:,0] != 0])\n",
    "            if  len(alpha) == 2:\n",
    "                label_regression[i,:] = alpha\n",
    "\n",
    "            \n",
    "\n",
    "            elif len(alpha) == 1:\n",
    "                states = label_2[i,:]\n",
    "                if 1 in states:\n",
    "                    # print(np.unique(states))\n",
    "                    if states[0] == 1:\n",
    "                        label_regression[i,:] = [0, alpha[0]]\n",
    "                    else:\n",
    "                        label_regression[i,:] = [alpha[0], 0]\n",
    "                    \n",
    "                    # print(label_regression[i,:])\n",
    "\n",
    "                else:\n",
    "                    label_regression[i,:] = [alpha[0],alpha[0]] \n",
    "\n",
    "            else:\n",
    "                if  np.unique(label[i,:,1]) == 0:\n",
    "                    label_regression[i,:] = [0,0]\n",
    "                else :\n",
    "\n",
    "                    # print(np.unique(label[i,:,1]))\n",
    "\n",
    "                    # print(Ds)\n",
    "                    raise Exception(\"more than 2 diffusions\")\n",
    "\n",
    "        # Normaliza alpha between 0 and 1\n",
    "        # label[:,:,0] = label[:,:,0] / 2\n",
    "\n",
    "        #return only D\n",
    "        label = label[:,:,1]\n",
    "        # Return data and label\n",
    "\n",
    "\n",
    "        return torch.from_numpy(data.astype(np.float32)), torch.from_numpy(label_regression.astype(np.float32))\n",
    "            # torch.from_numpy(label_2.astype(np.float32)),\n",
    "        \n",
    "    \n",
    "def add_noise(data):\n",
    "    noise_amplitude = np.random.choice([0.01, 0.1,])\n",
    "    noise = np.random.normal(0, noise_amplitude, data[:,:,:].shape)\n",
    "    data[:,:,:][data[:,:,1:] != 0] = data[:,:,:][data[:,:,1:] != 0] + data[:,:,:][data[:,:,1:] != 0]*noise\n",
    "    return  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_set = list_directory_tree_with_pathlib(\n",
    "    r\"/home/m.lavaud/Documents/Zeus/I2/T_200_const_100_to_150/batch_T_Const_1\",)\n",
    "np.random.shuffle(all_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = Dataset_all_data(\n",
    "        all_data_set[:2000], transform=False, pad=(20, 200)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(training_dataset, shuffle=True, batch_size=10, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm import Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class alpha_regression(nn.Module):\n",
    "    def __init__(self, d_model,d_state,  d_conv, expand,dropout = 0.3, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.mamba = Mamba(d_model = d_model, d_state=d_state, d_conv=d_conv, expand=expand).to(device)\n",
    "        self.dropout = nn.Dropout(p=dropout).to(device)\n",
    "        self.fc = nn.Linear(in_features = 200*d_model, out_features=2).to(device)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        mamba_out = self.mamba(input)\n",
    "        mamba_out = rearrange(mamba_out, \"b l c -> b (l c)\")\n",
    "        mamba_out = self.dropout(mamba_out)\n",
    "        out = self.fc(mamba_out)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = alpha_regression(d_model=2, d_state=1, d_conv=4, expand=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_criterion = torch.nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/100 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "EinopsError",
     "evalue": " Error while processing rearrange-reduction pattern \"b l c -> b (l,c)\".\n Input tensor shape: torch.Size([200, 200, 2]). Additional info: {}.\n Unknown character ','",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/einops/einops.py:522\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    521\u001b[0m shape \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mshape(tensor)\n\u001b[0;32m--> 522\u001b[0m recipe \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_transformation_recipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _apply_recipe(\n\u001b[1;32m    524\u001b[0m     backend, recipe, cast(Tensor, tensor), reduction_type\u001b[38;5;241m=\u001b[39mreduction, axes_lengths\u001b[38;5;241m=\u001b[39mhashable_axes_lengths\n\u001b[1;32m    525\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/einops/einops.py:300\u001b[0m, in \u001b[0;36m_prepare_transformation_recipe\u001b[0;34m(pattern, operation, axes_names, ndim)\u001b[0m\n\u001b[1;32m    299\u001b[0m left \u001b[38;5;241m=\u001b[39m ParsedExpression(left_str)\n\u001b[0;32m--> 300\u001b[0m rght \u001b[38;5;241m=\u001b[39m \u001b[43mParsedExpression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrght_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# checking that axes are in agreement - new axes appear only in repeat, while disappear only in reduction\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/einops/parsing.py:105\u001b[0m, in \u001b[0;36mParsedExpression.__init__\u001b[0;34m(self, expression, allow_underscore, allow_duplicates)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown character \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(char))\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bracket_group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mEinopsError\u001b[0m: Unknown character ','",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m regression_targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(\n\u001b[1;32m     16\u001b[0m     regression_targets, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, end_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     17\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m regression_output  \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m regression_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(regression_output)\n\u001b[1;32m     25\u001b[0m regression_loss \u001b[38;5;241m=\u001b[39m regression_criterion(\n\u001b[1;32m     26\u001b[0m     regression_output, regression_targets\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[137], line 13\u001b[0m, in \u001b[0;36malpha_regression.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     12\u001b[0m     mamba_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmamba(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     mamba_out \u001b[38;5;241m=\u001b[39m \u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmamba_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb l c -> b (l,c)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     mamba_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(mamba_out)\n\u001b[1;32m     15\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(mamba_out)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/einops/einops.py:591\u001b[0m, in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrearrange\u001b[39m(tensor: Union[Tensor, List[Tensor]], pattern: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maxes_lengths) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    537\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03m    einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m    This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m \n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrearrange\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/einops/einops.py:533\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Input is list. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    532\u001b[0m message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdditional info: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(axes_lengths)\n\u001b[0;32m--> 533\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(message \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e))\n",
      "\u001b[0;31mEinopsError\u001b[0m:  Error while processing rearrange-reduction pattern \"b l c -> b (l,c)\".\n Input tensor shape: torch.Size([200, 200, 2]). Additional info: {}.\n Unknown character ','"
     ]
    }
   ],
   "source": [
    "max_epoch = 10\n",
    "total_running_loss = []\n",
    "for epoch in range(max_epoch):\n",
    "    running_regresssion_loss = []\n",
    "    with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "        model.train()\n",
    "\n",
    "        for inputs, regression_targets in tepoch:\n",
    "\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "            inputs = torch.flatten(inputs, start_dim=0, end_dim=1)\n",
    "\n",
    "            regression_targets = torch.flatten(\n",
    "                regression_targets, start_dim=0, end_dim=1,\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            regression_output  = model(inputs)\n",
    "\n",
    "            regression_output = torch.squeeze(regression_output)\n",
    "\n",
    "            regression_loss = regression_criterion(\n",
    "                regression_output, regression_targets\n",
    "            )\n",
    "            regression_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tepoch.set_postfix(\n",
    "                regression_loss=regression_loss.item(),\n",
    "            )\n",
    "\n",
    "            running_regresssion_loss.append(regression_loss.item())\n",
    "        \n",
    "        total_running_loss.append(np.mean(running_regresssion_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[7.2704e-01, 7.2704e-01],\n",
       "          [1.7604e-01, 1.7604e-01],\n",
       "          [4.6218e-02, 4.6218e-02],\n",
       "          [2.9364e-02, 2.9364e-02],\n",
       "          [1.1195e-01, 1.1195e-01],\n",
       "          [4.8451e-02, 4.8451e-02],\n",
       "          [5.1347e-02, 5.1347e-02],\n",
       "          [1.4964e-01, 1.4964e-01],\n",
       "          [7.9317e-02, 7.9317e-02],\n",
       "          [3.2106e-01, 3.2106e-01],\n",
       "          [3.1510e-01, 3.1510e-01],\n",
       "          [1.0635e-01, 1.0635e-01],\n",
       "          [2.4082e-01, 2.4082e-01],\n",
       "          [2.5281e-01, 2.5281e-01],\n",
       "          [3.8770e-01, 3.8770e-01],\n",
       "          [1.0982e-01, 1.0982e-01],\n",
       "          [2.3973e-01, 2.3973e-01],\n",
       "          [8.4001e-02, 8.4001e-02],\n",
       "          [1.7004e-01, 1.7004e-01],\n",
       "          [1.4964e-01, 1.4964e-01]],\n",
       " \n",
       "         [[5.3142e-01, 5.3142e-01],\n",
       "          [6.6122e-01, 6.6122e-01],\n",
       "          [1.1848e+00, 1.1848e+00],\n",
       "          [3.7267e-01, 3.7267e-01],\n",
       "          [5.8224e-01, 5.8224e-01],\n",
       "          [8.7493e-01, 8.7493e-01],\n",
       "          [1.8250e+00, 1.8250e+00],\n",
       "          [1.1676e+00, 1.1676e+00],\n",
       "          [1.3098e+00, 1.3098e+00],\n",
       "          [1.8988e+00, 1.8988e+00],\n",
       "          [5.3450e-01, 5.3450e-01],\n",
       "          [7.8589e-02, 7.8589e-02],\n",
       "          [6.1101e-01, 6.1101e-01],\n",
       "          [8.1012e-03, 8.1012e-03],\n",
       "          [6.1534e-02, 6.1534e-02],\n",
       "          [3.6697e-01, 3.6697e-01],\n",
       "          [3.5438e-03, 3.5438e-03],\n",
       "          [8.3017e-01, 8.3017e-01],\n",
       "          [2.2992e-01, 2.2992e-01],\n",
       "          [5.9430e-01, 5.9430e-01]],\n",
       " \n",
       "         [[9.1844e-01, 9.1844e-01],\n",
       "          [1.0661e+00, 1.0661e+00],\n",
       "          [1.0531e+00, 1.0531e+00],\n",
       "          [1.1156e+00, 1.1156e+00],\n",
       "          [1.0213e+00, 1.0213e+00],\n",
       "          [1.0632e+00, 1.0632e+00],\n",
       "          [9.9004e-01, 9.9004e-01],\n",
       "          [1.1532e+00, 1.1532e+00],\n",
       "          [1.0063e+00, 1.0063e+00],\n",
       "          [1.1315e+00, 1.1315e+00],\n",
       "          [1.2158e+00, 1.2158e+00],\n",
       "          [7.7545e-01, 7.7545e-01],\n",
       "          [1.0706e+00, 1.0706e+00],\n",
       "          [1.0061e+00, 1.0061e+00],\n",
       "          [9.6749e-01, 9.6749e-01],\n",
       "          [1.1085e+00, 1.1085e+00],\n",
       "          [9.5640e-01, 9.5640e-01],\n",
       "          [1.0969e+00, 1.0969e+00],\n",
       "          [1.0308e+00, 1.0308e+00],\n",
       "          [9.8986e-01, 9.8986e-01]],\n",
       " \n",
       "         [[1.1201e+00, 1.1201e+00],\n",
       "          [1.0094e+00, 1.0094e+00],\n",
       "          [7.0734e-01, 7.0734e-01],\n",
       "          [7.1291e-01, 7.1291e-01],\n",
       "          [3.5140e-01, 3.5140e-01],\n",
       "          [6.0921e-01, 6.0921e-01],\n",
       "          [6.0581e-01, 6.0581e-01],\n",
       "          [5.0257e-01, 5.0257e-01],\n",
       "          [8.0359e-01, 8.0359e-01],\n",
       "          [8.1740e-01, 8.1740e-01],\n",
       "          [5.6060e-01, 5.6060e-01],\n",
       "          [6.8060e-01, 6.8060e-01],\n",
       "          [5.7091e-01, 5.7091e-01],\n",
       "          [6.5313e-01, 6.5313e-01],\n",
       "          [5.7376e-01, 5.7376e-01],\n",
       "          [5.0805e-01, 5.0805e-01],\n",
       "          [8.2132e-01, 8.2132e-01],\n",
       "          [6.9706e-01, 6.9706e-01],\n",
       "          [8.0359e-01, 8.0359e-01],\n",
       "          [8.6915e-01, 8.6915e-01]],\n",
       " \n",
       "         [[4.0790e-02, 1.6985e-01],\n",
       "          [1.8921e-02, 3.2885e-01],\n",
       "          [2.2010e-02, 1.1884e-01],\n",
       "          [1.2132e-03, 5.8497e-02],\n",
       "          [4.8991e-02, 4.8991e-02],\n",
       "          [7.9701e-03, 2.4652e-02],\n",
       "          [1.4728e-01, 1.4728e-01],\n",
       "          [2.3070e-03, 2.0650e-01],\n",
       "          [5.9321e-02, 2.7097e-01],\n",
       "          [2.8047e-02, 2.5878e-01],\n",
       "          [3.7633e-01, 3.7633e-01],\n",
       "          [1.4045e-01, 1.4045e-01],\n",
       "          [5.3354e-03, 4.1965e-02],\n",
       "          [7.8542e-02, 1.2518e-01],\n",
       "          [5.9389e-02, 2.4488e-01],\n",
       "          [8.7472e-02, 8.7472e-02],\n",
       "          [1.3913e-01, 1.3913e-01],\n",
       "          [5.1064e-03, 4.7383e-02],\n",
       "          [1.4728e-01, 4.2536e-01],\n",
       "          [6.1679e-02, 1.0658e-01]],\n",
       " \n",
       "         [[2.5291e-01, 2.5291e-01],\n",
       "          [4.0806e-01, 4.0806e-01],\n",
       "          [5.0899e-01, 5.0899e-01],\n",
       "          [6.3634e-02, 6.3634e-02],\n",
       "          [3.6211e-02, 3.6211e-02],\n",
       "          [4.6491e-01, 4.6491e-01],\n",
       "          [2.5102e-01, 2.5102e-01],\n",
       "          [7.5056e-01, 7.5056e-01],\n",
       "          [1.1642e+00, 1.1642e+00],\n",
       "          [7.4052e-01, 7.4052e-01],\n",
       "          [7.9878e-01, 7.9878e-01],\n",
       "          [4.4352e-01, 4.4352e-01],\n",
       "          [5.5403e-01, 5.5403e-01],\n",
       "          [3.8531e-03, 3.8531e-03],\n",
       "          [7.1578e-01, 7.1578e-01],\n",
       "          [9.3950e-02, 9.3950e-02],\n",
       "          [3.0599e-01, 3.0599e-01],\n",
       "          [7.6692e-02, 7.6692e-02],\n",
       "          [3.4404e-01, 3.4404e-01],\n",
       "          [6.7533e-01, 6.7533e-01]],\n",
       " \n",
       "         [[9.2965e-01, 9.2965e-01],\n",
       "          [9.1070e-01, 9.1070e-01],\n",
       "          [7.4296e-01, 7.4296e-01],\n",
       "          [1.0756e+00, 1.0756e+00],\n",
       "          [6.3024e-01, 6.3024e-01],\n",
       "          [6.8248e-01, 6.8248e-01],\n",
       "          [8.3064e-01, 8.3064e-01],\n",
       "          [7.4368e-01, 7.4368e-01],\n",
       "          [9.9826e-01, 9.9826e-01],\n",
       "          [5.2072e-01, 5.2072e-01],\n",
       "          [8.0385e-01, 8.0385e-01],\n",
       "          [8.1140e-01, 8.1140e-01],\n",
       "          [7.6232e-01, 7.6232e-01],\n",
       "          [8.3650e-01, 8.3650e-01],\n",
       "          [7.2022e-01, 7.2022e-01],\n",
       "          [7.8136e-01, 7.8136e-01],\n",
       "          [8.5410e-01, 8.5410e-01],\n",
       "          [1.0105e+00, 1.0105e+00],\n",
       "          [9.8629e-01, 9.8629e-01],\n",
       "          [3.5720e-01, 3.5720e-01]],\n",
       " \n",
       "         [[5.5080e-02, 4.5316e-01],\n",
       "          [4.4939e-02, 4.4939e-02],\n",
       "          [2.4876e-02, 6.2698e-02],\n",
       "          [6.7948e-02, 8.4356e-02],\n",
       "          [1.2145e-01, 1.2145e-01],\n",
       "          [4.1292e-02, 4.1292e-02],\n",
       "          [1.2333e-01, 1.2333e-01],\n",
       "          [6.6737e-02, 6.6737e-02],\n",
       "          [8.0708e-02, 8.0708e-02],\n",
       "          [1.3547e-01, 1.3547e-01],\n",
       "          [2.8395e-01, 2.8395e-01],\n",
       "          [5.5083e-01, 5.5083e-01],\n",
       "          [1.8734e-01, 1.8734e-01],\n",
       "          [1.7846e-02, 1.7846e-02],\n",
       "          [4.6414e-01, 4.6414e-01],\n",
       "          [3.3887e-01, 3.3887e-01],\n",
       "          [2.8409e-02, 2.8409e-02],\n",
       "          [4.1448e-01, 4.1448e-01],\n",
       "          [1.0778e-01, 1.0778e-01],\n",
       "          [8.3010e-02, 8.3010e-02]],\n",
       " \n",
       "         [[2.2481e-01, 2.2481e-01],\n",
       "          [3.1757e-01, 3.1757e-01],\n",
       "          [6.7199e-02, 6.7199e-02],\n",
       "          [1.7987e-01, 1.7987e-01],\n",
       "          [9.2772e-02, 9.2772e-02],\n",
       "          [1.3221e-01, 1.3221e-01],\n",
       "          [1.4459e-01, 1.4459e-01],\n",
       "          [1.4245e-01, 1.4245e-01],\n",
       "          [9.2096e-02, 9.2096e-02],\n",
       "          [8.1194e-02, 8.1194e-02],\n",
       "          [1.1250e-01, 1.1250e-01],\n",
       "          [9.8077e-02, 9.8077e-02],\n",
       "          [6.9703e-02, 6.9703e-02],\n",
       "          [2.2558e-01, 2.2558e-01],\n",
       "          [7.4968e-02, 7.4968e-02],\n",
       "          [4.5055e-02, 4.5055e-02],\n",
       "          [2.4806e-03, 2.4806e-03],\n",
       "          [4.0608e-01, 4.0608e-01],\n",
       "          [4.0081e-01, 4.0081e-01],\n",
       "          [2.6131e-01, 2.6131e-01]],\n",
       " \n",
       "         [[8.3980e-01, 8.3980e-01],\n",
       "          [7.9022e-01, 7.9022e-01],\n",
       "          [7.6053e-01, 7.6053e-01],\n",
       "          [8.2318e-01, 8.2318e-01],\n",
       "          [9.2069e-01, 9.2069e-01],\n",
       "          [7.6396e-01, 7.6396e-01],\n",
       "          [8.0351e-01, 8.0351e-01],\n",
       "          [5.7346e-01, 5.7346e-01],\n",
       "          [8.1794e-01, 8.1794e-01],\n",
       "          [9.4672e-01, 9.4672e-01],\n",
       "          [8.7204e-01, 8.7204e-01],\n",
       "          [1.0296e+00, 1.0296e+00],\n",
       "          [1.0313e+00, 1.0313e+00],\n",
       "          [7.6674e-01, 7.6674e-01],\n",
       "          [7.9259e-01, 7.9259e-01],\n",
       "          [9.0902e-01, 9.0902e-01],\n",
       "          [8.9963e-01, 8.9963e-01],\n",
       "          [1.0014e+00, 1.0014e+00],\n",
       "          [7.7429e-01, 7.7429e-01],\n",
       "          [7.2678e-01, 7.2678e-01]]])]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
