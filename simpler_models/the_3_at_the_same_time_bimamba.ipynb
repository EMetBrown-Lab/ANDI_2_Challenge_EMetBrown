{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Function to pad an array to a specific shape\n",
    "def to_shape(a, shape):\n",
    "    # Unpack the target shape\n",
    "    y_, x_ = shape\n",
    "\n",
    "    # Get the current shape of the array\n",
    "    y, x = a.shape\n",
    "\n",
    "    # Calculate the padding needed in the y and x directions\n",
    "    y_pad = y_ - y\n",
    "    x_pad = x_ - x\n",
    "    output = np.zeros()\n",
    "    # Pad the array using numpy's pad function\n",
    "    return np.pad(\n",
    "        a,\n",
    "        [(0, 1), (0, 1)],\n",
    "        # Calculate the padding for each dimension\n",
    "        # ((y_pad // 2, y_pad // 2 + y_pad % 2), (x_pad // 2, x_pad // 2 + x_pad % 2)),\n",
    "        mode=\"constant\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Function to pad data and labels to a specific shape\n",
    "def apply_padding(data_df, N, T_max):\n",
    "    # Define the final shape of the data and labels\n",
    "    final_shape = (N, T_max, 3)\n",
    "\n",
    "    # Initialize the final data and labels with zeros\n",
    "    final_data = np.zeros(final_shape)\n",
    "    final_label = np.zeros((N, T_max, 3))\n",
    "\n",
    "    # Select a random subset of trajectory indices\n",
    "    if len(data_df[\"traj_idx\"].unique()) < N:\n",
    "        selected_ids = np.random.choice(\n",
    "            data_df[\"traj_idx\"].unique(), size=N, replace=True\n",
    "        )\n",
    "    else:\n",
    "        selected_ids = np.random.choice(\n",
    "            data_df[\"traj_idx\"].unique(), size=N, replace=False\n",
    "        )\n",
    "\n",
    "    # Iterate over the selected trajectory indices\n",
    "    for n, id in enumerate(selected_ids):\n",
    "        # Filter the data for the current trajectory index\n",
    "        exp = data_df[data_df[\"traj_idx\"] == id]\n",
    "\n",
    "        # Extract the data and labels for the current trajectory\n",
    "        data = exp[[\"frame\", \"x\", \"y\"]].to_numpy()\n",
    "        data[:, 0] = data[:, 0] - data[0, 0] + 1  # putting first frame rate to 1\n",
    "        data[:, 1] = data[:, 1] - data[0, 1]  # putting initial position to 0\n",
    "        data[:, 2] = (\n",
    "            data[:, 2] - data[0, 2]\n",
    "        )  # putting initital position to 0        # print(exp[\"frame\"])\n",
    "        label = exp[[\"alpha\", \"D\", \"state\"]].to_numpy()\n",
    "        ## adding one to the states\n",
    "        label[:, 2] = label[:, 2] + 1\n",
    "        # If the data is longer than T_max, truncate it\n",
    "        if data.shape[0] > T_max:\n",
    "            final_data[n, :, :] = data[:T_max, :]\n",
    "            final_label[n, :, :] = label[:T_max, :]\n",
    "\n",
    "        # Otherwise, pad the data to T_max\n",
    "        else:\n",
    "            # print((label.shape, T_max))\n",
    "            final_data[n, : data.shape[0], :] = data\n",
    "            final_label[n, : data.shape[0], :] = label\n",
    "\n",
    "    # Return the padded data and labels\n",
    "    return final_data, final_label\n",
    "\n",
    "\n",
    "# Define a function to normalize data\n",
    "def normalize_df(data):\n",
    "    # Calculate displacement in x and y directions\n",
    "    # Normalize by substring mean and dividing by variance.\n",
    "\n",
    "    displacement_x = []\n",
    "    displacement_y = []\n",
    "    for _, group in data.groupby(\"traj_idx\"):\n",
    "        x = np.asarray(group[\"x\"])\n",
    "        y = np.asarray(group[\"y\"])\n",
    "        d_x = x[1:] - x[:-1]\n",
    "        d_y = y[1:] - y[:-1]\n",
    "        displacement_x = displacement_x + list(d_x)\n",
    "        displacement_y = displacement_y + list(d_y)\n",
    "\n",
    "    # Calculate variance in x and y directions\n",
    "    variance_x = np.sqrt(np.std(displacement_x))\n",
    "    variance_y = np.sqrt(np.std(displacement_y))\n",
    "\n",
    "    # Normalize data\n",
    "    data.loc[:, \"x\"] = (data[\"x\"] - data[\"x\"].mean()) / variance_x\n",
    "    data.loc[:, \"y\"] = (data[\"y\"] - data[\"y\"].mean()) / variance_y\n",
    "\n",
    "\n",
    "def normalize_np(data):\n",
    "\n",
    "    displacement_x = []\n",
    "    displacement_y = []\n",
    "    for n in range(data.shape[0]):\n",
    "        x = data[n, :, 1]\n",
    "        y = data[n, :, 2]\n",
    "        d_x = x[1:] - x[:-1]\n",
    "        d_y = y[1:] - y[:-1]\n",
    "        displacement_x = displacement_x + list(d_x)\n",
    "        displacement_y = displacement_y + list(d_y)\n",
    "\n",
    "    # Calculate variance in x and y directions\n",
    "    variance_x = np.sqrt(np.std(displacement_x))\n",
    "    variance_y = np.sqrt(np.std(displacement_y))\n",
    "\n",
    "    # Normalize data\n",
    "\n",
    "    data[:, :, 1] = (data[:, :, 1] - np.mean(data[:, :, 1])) / variance_x\n",
    "    data[:, :, 2] = (data[:, :, 2] - np.mean(data[:, :, 2])) / variance_x\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Define a function to list directory tree with pathlib\n",
    "def list_directory_tree_with_pathlib(starting_directory):\n",
    "    path_object = Path(starting_directory)\n",
    "    folders = []\n",
    "    for file_path in path_object.rglob(\"*.csv\"):\n",
    "        folders.append(file_path)\n",
    "    return folders\n",
    "\n",
    "\n",
    "# Define a custom dataset class for all data\n",
    "@dataclass\n",
    "class Dataset_all_data(Dataset):\n",
    "    # Initialize filenames and transform flag\n",
    "    # Pad value should be a tuple such as (N, Tmax)\n",
    "    filenames: list\n",
    "    transform: bool = False\n",
    "    pad: None | tuple = None\n",
    "    noise: bool = False\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of files\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read csv file and extract data and label\n",
    "        df = pd.read_csv(self.filenames[idx])\n",
    "\n",
    "        if self.pad is None:\n",
    "            data = df[[\"traj_idx\", \"frame\", \"x\", \"y\"]]\n",
    "            label = np.asarray(df[[\"alpha\", \"D\"]])\n",
    "            label_2 = np.asarray(df[\"state\"])\n",
    "\n",
    "        else:\n",
    "            if len(self.pad) != 2:\n",
    "                raise ValueError(\"pad value should be set as (N, T_max)\")\n",
    "            data, label = apply_padding(df, *self.pad)\n",
    "            data = data[:, :, :]  ## Removing the frame column\n",
    "            label_2 = label[:, :, -1]\n",
    "            label_2[label_2[:, :] > 0] = label_2[label_2[:, :] > 0]\n",
    "            label = label[:, :, :-1]\n",
    "\n",
    "        # Normalize data if transform flag is True\n",
    "        if self.transform:\n",
    "            if self.pad is None:\n",
    "                normalize_df(data)\n",
    "                data = np.asarray(data)\n",
    "            else:\n",
    "                data = normalize_np(data)\n",
    "\n",
    "        if self.noise:\n",
    "            data = add_noise(data)\n",
    "\n",
    "        # Normalize D between 0 and 1\n",
    "\n",
    "        # label[:,:,1][label[:,:,1] != 0] = np.log(label[:,:,1][label[:,:,1] != 0]) #- np.log(1e-6)) #/   (np.log(1e12) - np.log(1e-6))\n",
    "        # label = label[:,:,1]\n",
    "        label_K = np.zeros((label.shape[0], 2))\n",
    "\n",
    "        # print(np.unique(label_2))\n",
    "\n",
    "        for i in range(label.shape[0]):\n",
    "            K = np.unique(label[i, :, 1][label[i, :, 1] != 0])\n",
    "            if len(K) == 2:\n",
    "                label_K[i, :] = K\n",
    "\n",
    "                if label[i, 0, 1] != label_K[i, 0]:\n",
    "                    label_K[i, :] = label_K[i, ::-1]\n",
    "\n",
    "            elif len(K) == 1:\n",
    "                states = label_2[i, :]\n",
    "                if 1 in states:\n",
    "                    # print(np.unique(states))\n",
    "                    if states[0] == 1:\n",
    "                        label_K[i, :] = [0, K[0]]\n",
    "                    else:\n",
    "                        label_K[i, :] = [K[0], 0]\n",
    "\n",
    "                    # print(label_regression[i,:])\n",
    "\n",
    "                else:\n",
    "                    label_K[i, :] = [K[0], K[0]]\n",
    "\n",
    "            else:\n",
    "                if np.unique(label[i, :, 1]) == 0:\n",
    "                    label_K[i, :] = [0, 0]\n",
    "                else:\n",
    "\n",
    "                    # print(np.unique(label[i,:,1]))\n",
    "\n",
    "                    # print(Ds)\n",
    "                    raise Exception(\"more than 2 diffusions\")\n",
    "        \n",
    "\n",
    "        # print(np.unique(label_2))\n",
    "        label_alpha = np.zeros((label.shape[0], 2))\n",
    "\n",
    "        for i in range(label.shape[0]):\n",
    "            alpha = np.unique(label[i,:,0][label[i,:,0] != 0])\n",
    "            if  len(alpha) == 2:\n",
    "                label_alpha[i,:] = alpha\n",
    "                if label[i,0,0] != label_alpha[i,0]:\n",
    "                    label_alpha[i,:] = label_alpha[i,::-1]\n",
    "            \n",
    "\n",
    "            elif len(alpha) == 1:\n",
    "                states = label_2[i,:]\n",
    "                if 1 in states:\n",
    "                    # print(np.unique(states))\n",
    "                    if states[0] == 1:\n",
    "                        label_alpha[i,:] = [0, alpha[0]]\n",
    "                    else:\n",
    "                        label_alpha[i,:] = [alpha[0], 0]\n",
    "                    \n",
    "                    # print(label_regression[i,:])\n",
    "\n",
    "                else:\n",
    "                    label_alpha[i,:] = [alpha[0],alpha[0]] \n",
    "\n",
    "            else:\n",
    "                if  np.unique(label[i,:,1]) == 0:\n",
    "                    label_alpha[i,:] = [0,0]\n",
    "                else :\n",
    "\n",
    "                    # print(np.unique(label[i,:,1]))\n",
    "\n",
    "                    # print(Ds)\n",
    "                    raise Exception(\"more than 2 diffusions\")\n",
    "\n",
    "        label_segmentation = np.zeros((label_2.shape[0], label_2.shape[1]))\n",
    "\n",
    "        for i in range(label.shape[0]):\n",
    "            if label_K[i, 0] == label_K[i, 1]:\n",
    "                position = label[i, :, 1] == label_K[i, 0]\n",
    "                label_segmentation[i, position] = 1\n",
    "            else:\n",
    "\n",
    "                position_1 = label[i, :, 1] == label_K[i, 0]\n",
    "                position_2 = label[i, :, 1] == label_K[i, 1]\n",
    "\n",
    "                label_segmentation[i, position_1] = 1\n",
    "                label_segmentation[i, position_2] = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(data.astype(np.float32)),\n",
    "            torch.from_numpy(label_segmentation.astype(np.float32)),\n",
    "            torch.from_numpy(label_K),\n",
    "            torch.from_numpy(label_alpha)\n",
    "\n",
    "        )\n",
    "        # torch.from_numpy(label_2.astype(np.float32)),\n",
    "\n",
    "\n",
    "def add_noise(data):\n",
    "    noise_amplitude = np.random.choice(\n",
    "        [\n",
    "            0.01,\n",
    "            0.1,\n",
    "        ]\n",
    "    )\n",
    "    noise = np.random.normal(0, noise_amplitude, data[:, :, :].shape)\n",
    "    data[:, :, :][data[:, :, 1:] != 0] = (\n",
    "        data[:, :, :][data[:, :, 1:] != 0] + data[:, :, :][data[:, :, 1:] != 0] * noise\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_set = list_directory_tree_with_pathlib(\n",
    "    r\"/home/m.lavaud/Documents/dataset\",\n",
    ")\n",
    "np.random.shuffle(all_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = Dataset_all_data(all_data_set[:10000], transform=False, pad=(20, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = iter(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(training_dataset, shuffle=True, batch_size=10, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m.lavaud/miniconda3/envs/torch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mamba_ssm import Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class segmentation_model(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand, dropout=0.2, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.mamba = Mamba(\n",
    "            d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand\n",
    "        ).to(device)\n",
    "\n",
    "        self.flipped_mamba = Mamba(\n",
    "            d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand\n",
    "        ).to(device)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout).to(device)\n",
    "        self.fc = nn.Linear(in_features=d_model, out_features=3).to(device)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        mamba_out = self.mamba(input)\n",
    "        mamba_flipped_out = self.flipped_mamba(torch.flip(input, dims=[1]))\n",
    "        mamba_out = mamba_out + mamba_flipped_out\n",
    "\n",
    "        mamba_out = self.dropout(mamba_out)\n",
    "        out = self.fc(mamba_out)\n",
    "\n",
    "        return out  # No activation here ! It is done by the cross entropy loss\n",
    "    \n",
    "class K_regression(nn.Module):\n",
    "    def __init__(self, d_model,d_state,  d_conv, expand,dropout = 0.3, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.mamba = Mamba(d_model = d_model, d_state=d_state, d_conv=d_conv, expand=expand).to(device)\n",
    "        self.flipped_mamba = Mamba(d_model = d_model, d_state=d_state, d_conv=d_conv, expand=expand).to(device)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout).to(device)\n",
    "        self.fc = nn.Linear(in_features = 200*d_model, out_features=2).to(device)\n",
    "\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "    def forward(self, input):\n",
    "\n",
    "        mamba_out = self.mamba(input)\n",
    "        mamba_flipped_out = self.flipped_mamba(torch.flip(input, dims=[1]))\n",
    "\n",
    "        mamba_out = mamba_out + mamba_flipped_out\n",
    "\n",
    "        mamba_out = rearrange(mamba_out, \"b l c -> b (l c)\")\n",
    "\n",
    "        mamba_out = self.dropout(mamba_out)\n",
    "        out = self.fc(mamba_out)\n",
    "        out = torch.clamp(out, min=0, max=1e12)\n",
    "        out[out < 1e-7] = 0\n",
    "        return out\n",
    "    \n",
    "\n",
    "class alpha_regression(nn.Module):\n",
    "    def __init__(self, d_model,d_state,  d_conv, expand,dropout = 0.3, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.mamba = Mamba(d_model = d_model, d_state=d_state, d_conv=d_conv, expand=expand).to(device)\n",
    "        self.flipped_mamba = Mamba(d_model = d_model, d_state=d_state, d_conv=d_conv, expand=expand).to(device)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout).to(device)\n",
    "        self.fc = nn.Linear(in_features = 200*d_model, out_features=2).to(device)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        mamba_out = self.mamba(input)\n",
    "        mamba_flipped_out = self.flipped_mamba(torch.flip(input, dims=[1]))\n",
    "\n",
    "        mamba_out = mamba_out + mamba_flipped_out\n",
    "\n",
    "        mamba_out = rearrange(mamba_out, \"b l c -> b (l c)\")\n",
    "\n",
    "        mamba_out = self.dropout(mamba_out)\n",
    "        out = self.fc(mamba_out)\n",
    "        return self.sigmoid(out) *2 \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class all_at_the_same_time(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand, dropout=0.2, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.model_K = K_regression(d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "        self.model_alpha = alpha_regression(d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "        self.segmentation = segmentation_model(d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        probas = self.segmentation(x)\n",
    "\n",
    "        classes = torch.argmax(torch.softmax(probas, dim = 2), dim=2)\n",
    "\n",
    "        classes[x[:,:,0] == 0] = 0\n",
    "\n",
    "        \n",
    "        classes[x[:,:,0] == 0] = 0\n",
    "        classes = classes.unsqueeze(-1) # adding a dimension for the next step\n",
    "        concat_entry = torch.cat((classes, x[:,:,1:]), dim=2)\n",
    "\n",
    "        alpha = self.model_alpha(concat_entry)\n",
    "\n",
    "        K = self.model_K(concat_entry)\n",
    "\n",
    "        return probas, alpha, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = all_at_the_same_time(d_model=2, d_state=1, d_conv=4, expand=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "alpha_criterion = torch.nn.L1Loss().to(\"cuda\")\n",
    "class MSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return (self.mse(torch.log(pred + 1), torch.log(actual + 1)))\n",
    "\n",
    "\n",
    "K_criterion = MSLELoss().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  52%|█████▏    | 418/800 [02:07<02:06,  3.02batch/s, loss_K=0.712, loss_a=0.36, loss_c=1.33]  "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_epoch = 50\n",
    "total_classification_loss = []\n",
    "total_K_loss = []\n",
    "total_alpha_loss = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    running_classification_loss = []\n",
    "    running_alpha_loss = []\n",
    "    running_K_loss = []\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "        model.train()\n",
    "\n",
    "        for inputs, classification_targets, K_targets, alpha_targets in tepoch:\n",
    "\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            inputs = inputs.to(\"cuda\", dtype=torch.float32)  # Ensuring float32 type\n",
    "            inputs = torch.flatten(inputs, start_dim=0, end_dim=1)\n",
    "\n",
    "            classification_targets = (\n",
    "                torch.flatten(\n",
    "                    classification_targets,\n",
    "                    start_dim=0,\n",
    "                    end_dim=1,\n",
    "                )\n",
    "                .type(torch.LongTensor)\n",
    "                .to(\"cuda\")\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            classification_output, alpha_output, K_output = model(inputs)\n",
    "            classification_output = torch.squeeze(classification_output)\n",
    "            \n",
    "            ## Computation of the weight of the classes\n",
    "\n",
    "            counts = torch.unique(classification_targets, return_counts=True)[1][1:]\n",
    "            weights = torch.sum(counts) / (2 * counts)\n",
    "            weights = weights.to(\"cpu\", dtype=torch.float32)  # Ensuring float32 type\n",
    "            weight = torch.zeros(3, dtype=torch.float32)  # Ensuring float32 type\n",
    "            weight[1:] = weights\n",
    "            ###\n",
    "\n",
    "            classification_criterion = nn.CrossEntropyLoss(\n",
    "                weight=weight, ignore_index=0\n",
    "            )\n",
    "\n",
    "            classification_loss = classification_criterion(\n",
    "                classification_output.view(-1, 3).to(\"cpu\", dtype=torch.float32),  # Ensuring float32 type\n",
    "                classification_targets.view(-1).to(\"cpu\")\n",
    "            ).to(\"cuda\")\n",
    "            \n",
    "            alpha_targets = torch.flatten(alpha_targets, start_dim=0, end_dim=1).to(\"cuda\", dtype=torch.float32)  # Ensuring float32 type\n",
    "            alpha_loss = alpha_criterion(\n",
    "                alpha_output, alpha_targets\n",
    "            ).to(\"cuda\", dtype=torch.float32)  # Ensuring float32 type\n",
    "\n",
    "            K_targets = torch.flatten(K_targets, start_dim=0, end_dim=1).to(\"cuda\", dtype=torch.float32)  # Ensuring float32 type\n",
    "            K_loss = K_criterion(\n",
    "                K_output, K_targets\n",
    "            ).to(\"cuda\", dtype=torch.float32)  # Ensuring float32 type\n",
    "\n",
    "            total_loss = alpha_loss + K_loss + classification_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tepoch.set_postfix(\n",
    "                loss_c=classification_loss.item(),\n",
    "                loss_a=alpha_loss.item(),\n",
    "                loss_K=K_loss.item()\n",
    "            )\n",
    "\n",
    "            running_classification_loss.append(classification_loss.item())\n",
    "            running_alpha_loss.append(alpha_loss.item())  # Corrected to alpha_loss.item()\n",
    "            running_K_loss.append(K_loss.item())  # Corrected to K_loss.item()\n",
    "\n",
    "        total_classification_loss.append(np.mean(running_classification_loss))\n",
    "        total_alpha_loss.append(np.mean(running_alpha_loss))\n",
    "        total_K_loss.append(np.mean(running_K_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.5916604101657867,\n",
       " 1.9421493983268738,\n",
       " 1.5447381779551506,\n",
       " 1.1942388623952866,\n",
       " 1.031971840262413,\n",
       " 0.9750270673632622,\n",
       " 0.906024914085865,\n",
       " 0.8663847279548645,\n",
       " 0.8263165420293808,\n",
       " 0.8028307405114173]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_classification_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4990538391470909,\n",
       " 0.3783333530277014,\n",
       " 0.34783628582954407,\n",
       " 0.3250553235411644,\n",
       " 0.3263745218515396,\n",
       " 0.31379026114940645,\n",
       " 0.30737220622599126,\n",
       " 0.30368047297000883,\n",
       " 0.30112284652888777,\n",
       " 0.29751449935138224]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_alpha_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7561281541083008,\n",
       " 0.36210534289479257,\n",
       " 0.19807127495761961,\n",
       " 0.13360511170700193,\n",
       " 0.11107545903185383,\n",
       " 0.10443825859110803,\n",
       " 0.09534662449732423,\n",
       " 0.09828063840977848,\n",
       " 0.09218638498801739,\n",
       " 0.09088338338304311]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_K_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_output.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
