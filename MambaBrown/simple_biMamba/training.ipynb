{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from simple_mamba import all_at_the_same_time\n",
    "from Dataloader import list_directory_tree, DatasetAllData\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = all_at_the_same_time(d_model=6, d_state=16, d_conv=2, expand=1, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, actual):\n",
    "        return self.mse(torch.log(pred + 1), torch.log(actual + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "# Define the loss functions\n",
    "alpha_criterion = torch.nn.L1Loss().to(\"cuda\")\n",
    "K_criterion = MSLELoss().to(\"cuda\")\n",
    "\n",
    "# Create a list of all data files\n",
    "all_data_set = list_directory_tree(r\"../../data/datasets\")\n",
    "np.random.shuffle(all_data_set)\n",
    "\n",
    "# Create training and test datasets\n",
    "training_dataset = DatasetAllData(all_data_set[:10000], transform=False, pad=(20, 200))\n",
    "test_dataset = DatasetAllData(all_data_set[-100:], transform=False, pad=(20, 200))\n",
    "\n",
    "# Create dataloaders for training and testing\n",
    "dataloader = DataLoader(training_dataset, shuffle=True, batch_size=10, num_workers=5)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=10, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_loss(model, test_dataloader, alpha_criterion, K_criterion):\n",
    "    model.eval()\n",
    "    test_classification_loss = []\n",
    "    test_alpha_loss = []\n",
    "    test_K_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, classification_targets, K_targets, alpha_targets in test_dataloader:\n",
    "            inputs = inputs.to(\"cuda\", dtype=torch.float32)\n",
    "            inputs = torch.flatten(inputs, start_dim=0, end_dim=1)\n",
    "\n",
    "            classification_targets = classification_targets.type(torch.LongTensor).to(\n",
    "                \"cuda\"\n",
    "            )\n",
    "            classification_targets = torch.flatten(\n",
    "                classification_targets, start_dim=0, end_dim=1\n",
    "            )\n",
    "\n",
    "            classification_output, alpha_output, K_output = model(inputs)\n",
    "\n",
    "            classification_output = torch.squeeze(classification_output)\n",
    "\n",
    "            counts = torch.unique(classification_targets, return_counts=True)[1][1:]\n",
    "            weights = torch.sum(counts) / (2 * counts)\n",
    "            weights = weights.to(\"cpu\", dtype=torch.float32)\n",
    "            weight = torch.zeros(3, dtype=torch.float32)\n",
    "            weight[1:] = weights\n",
    "\n",
    "            classification_criterion = nn.CrossEntropyLoss(\n",
    "                weight=weight, ignore_index=0\n",
    "            )\n",
    "\n",
    "            classification_loss = classification_criterion(\n",
    "                classification_output.view(-1, 3).to(\"cpu\", dtype=torch.float32),\n",
    "                classification_targets.view(-1).to(\"cpu\"),\n",
    "            )\n",
    "            alpha_targets = alpha_targets.to(\"cuda\", dtype=torch.float32)\n",
    "            alpha_targets = torch.flatten(alpha_targets, start_dim=0, end_dim=1)\n",
    "            alpha_loss = alpha_criterion(alpha_output, alpha_targets)\n",
    "            K_targets = K_targets.to(\"cuda\", dtype=torch.float32)\n",
    "            K_targets = torch.flatten(K_targets, start_dim=0, end_dim=1)\n",
    "            K_loss = K_criterion(K_output, K_targets)\n",
    "\n",
    "            test_classification_loss.append(classification_loss.item())\n",
    "            test_alpha_loss.append(alpha_loss.item())\n",
    "            test_K_loss.append(K_loss.item())\n",
    "\n",
    "    return (\n",
    "        np.mean(test_classification_loss),\n",
    "        np.mean(test_alpha_loss),\n",
    "        np.mean(test_K_loss),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_epoch = 50\n",
    "total_classification_loss = []\n",
    "total_K_loss = []\n",
    "total_alpha_loss = []\n",
    "test_classification_loss = []\n",
    "test_K_loss = []\n",
    "test_alpha_loss = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    running_classification_loss = []\n",
    "    running_alpha_loss = []\n",
    "    running_K_loss = []\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "        model.train()\n",
    "\n",
    "        for inputs, classification_targets, K_targets, alpha_targets in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            inputs = inputs.to(\"cuda\", dtype=torch.float32)\n",
    "            inputs = torch.flatten(inputs, start_dim=0, end_dim=1)\n",
    "\n",
    "            classification_targets = classification_targets.type(torch.LongTensor).to(\n",
    "                \"cuda\"\n",
    "            )\n",
    "            classification_targets = torch.flatten(\n",
    "                classification_targets, start_dim=0, end_dim=1\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            classification_output, alpha_output, K_output = model(inputs)\n",
    "            classification_output = torch.squeeze(classification_output)\n",
    "\n",
    "            counts = torch.unique(classification_targets, return_counts=True)[1][1:]\n",
    "            weights = torch.sum(counts) / (2 * counts)\n",
    "            weights = weights.to(\"cpu\", dtype=torch.float32)\n",
    "            weight = torch.zeros(3, dtype=torch.float32)\n",
    "            weight[1:] = weights\n",
    "\n",
    "            classification_criterion = nn.CrossEntropyLoss(\n",
    "                weight=weight, ignore_index=0\n",
    "            )\n",
    "\n",
    "            classification_loss = classification_criterion(\n",
    "                classification_output.view(-1, 3).to(\"cpu\", dtype=torch.float32),\n",
    "                classification_targets.view(-1).to(\"cpu\"),\n",
    "            )\n",
    "            alpha_targets = alpha_targets.to(\"cuda\", dtype=torch.float32)\n",
    "            alpha_targets = torch.flatten(alpha_targets, start_dim=0, end_dim=1)\n",
    "            alpha_loss = alpha_criterion(alpha_output, alpha_targets)\n",
    "            K_targets = K_targets.to(\"cuda\", dtype=torch.float32)\n",
    "            K_targets = torch.flatten(K_targets, start_dim=0, end_dim=1)\n",
    "            K_loss = K_criterion(K_output, K_targets)\n",
    "\n",
    "            total_loss = alpha_loss + K_loss + classification_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tepoch.set_postfix(\n",
    "                loss_c=classification_loss.item(),\n",
    "                loss_a=alpha_loss.item(),\n",
    "                loss_K=K_loss.item(),\n",
    "            )\n",
    "\n",
    "            running_classification_loss.append(classification_loss.item())\n",
    "            running_alpha_loss.append(alpha_loss.item())\n",
    "            running_K_loss.append(K_loss.item())\n",
    "\n",
    "        runnin_test_class_loss, runnin_test_alpha_loss, runnin_test_K_loss = (\n",
    "            compute_test_loss(model, test_dataloader, alpha_criterion, K_criterion)\n",
    "        )\n",
    "        total_classification_loss.append(np.mean(running_classification_loss))\n",
    "        total_alpha_loss.append(np.mean(running_alpha_loss))\n",
    "        total_K_loss.append(np.mean(running_K_loss))\n",
    "\n",
    "        test_classification_loss.append(runnin_test_class_loss)\n",
    "        test_K_loss.append(runnin_test_alpha_loss)\n",
    "        test_alpha_loss.append(runnin_test_K_loss)\n",
    "\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            f\"6_features_saved_state/100k_files_training_new_bimamba_epoch_{epoch}\",\n",
    "        )\n",
    "\n",
    "        plt.figure(dpi=300)\n",
    "        plt.semilogy(total_alpha_loss, label=\"alpha\")\n",
    "        plt.semilogy(total_classification_loss, label=\"classification\")\n",
    "        plt.semilogy(total_K_loss, label=\"k\")\n",
    "        plt.semilogy(test_alpha_loss, \"--\", label=\"test\")\n",
    "        plt.semilogy(test_classification_loss, \"--\", label=\"test\")\n",
    "        plt.semilogy(test_K_loss, \"--\", label=\"test\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "plt.semilogy(total_alpha_loss[90:], label=\"alpha\")\n",
    "plt.semilogy(total_classification_loss[90:], label=\"classification\")\n",
    "plt.semilogy(total_K_loss[90:], label=\"k\")\n",
    "\n",
    "plt.semilogy(test_alpha_loss[90:], \"--\", label=\"test\")\n",
    "plt.semilogy(test_classification_loss[90:], \"--\", label=\"test\")\n",
    "plt.semilogy(test_K_loss[90:], \"--\", label=\"test\")\n",
    "plt.ylim((1e-2, 1))\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"1000epoch18kfiles.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "plt.loglog(total_classification_loss, label=\"classification\")\n",
    "plt.loglog(test_classification_loss, \"--\", label=\"test\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"1000epoch18kfiles.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
