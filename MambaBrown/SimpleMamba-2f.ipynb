{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from mamba_ssm import Mamba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pad an array to a specific shape\n",
    "def to_shape(a, shape):\n",
    "    # Unpack the target shape\n",
    "    y_, x_ = shape\n",
    "\n",
    "    # Get the current shape of the array\n",
    "    y, x = a.shape\n",
    "\n",
    "    # Calculate the padding needed in the y and x directions\n",
    "    y_pad = y_ - y\n",
    "    x_pad = x_ - x\n",
    "    output = np.zeros()\n",
    "\n",
    "    # Pad the array using numpy's pad function\n",
    "    return np.pad(\n",
    "        a,\n",
    "        [(0, 1), (0, 1)],\n",
    "        mode=\"constant\",\n",
    "    )\n",
    "\n",
    "\n",
    "def angle_between(p1, p2):\n",
    "    ang1 = np.arctan2(*p1[::-1])\n",
    "    ang2 = np.arctan2(*p2[::-1])\n",
    "    return (ang1 - ang2) % (2 * np.pi)\n",
    "\n",
    "\n",
    "# Function to pad data and labels to a specific shape\n",
    "def apply_padding(data_df, N, T_max):\n",
    "    # Define the final shape of the data and labels\n",
    "    final_shape = (N, T_max - 1, 6)\n",
    "\n",
    "    # Initialize the final data and labels with zeros\n",
    "    final_data = np.zeros(final_shape)\n",
    "    final_label = np.zeros((N, T_max - 1, 3))\n",
    "\n",
    "    # Select a random subset of trajectory indices\n",
    "    if len(data_df[\"traj_idx\"].unique()) < N:\n",
    "        selected_ids = np.random.choice(\n",
    "            data_df[\"traj_idx\"].unique(), size=N, replace=True\n",
    "        )\n",
    "    else:\n",
    "        selected_ids = np.random.choice(\n",
    "            data_df[\"traj_idx\"].unique(), size=N, replace=False\n",
    "        )\n",
    "\n",
    "    # Iterate over the selected trajectory indices\n",
    "    for n, id in enumerate(selected_ids):\n",
    "        # Filter the data for the current trajectory index\n",
    "        exp = data_df[data_df[\"traj_idx\"] == id]\n",
    "\n",
    "        # Extract the data and labels for the current trajectory\n",
    "        data = exp[[\"frame\", \"x\", \"y\"]].to_numpy()\n",
    "        data[:, 0] = data[:, 0] - data[0, 0] + 1  # putting first frame rate to 1\n",
    "        data[:, 1] = data[:, 1] - data[0, 1]  # putting initial position to 0\n",
    "        data[:, 2] = data[:, 2] - data[0, 2]  # putting initital position to 0\n",
    "\n",
    "        # Displacement\n",
    "        Dx = data[1:, 1] - data[:-1, 1]\n",
    "        Dy = data[1:, 2] - data[:-1, 2]\n",
    "        MDx = np.zeros(len(Dx))\n",
    "        MDy = np.zeros(len(Dx))\n",
    "        angles = np.zeros(len(Dx))\n",
    "        distance_displacement = np.sqrt(np.power(Dx, 2) + np.power(Dy, 2))\n",
    "\n",
    "        # Displacement average\n",
    "        for i in range(1, len(Dx) + 1):\n",
    "            MDx[i - 1] = np.mean(data[i:, 1] - data[:-i, 1])\n",
    "            MDy[i - 1] = np.mean(data[i:, 2] - data[:-i, 2])\n",
    "\n",
    "            A = (data[(i - 1), 1], data[(i - 1), 2])\n",
    "            B = (data[i, 1], data[i, 2])\n",
    "\n",
    "            # Computation of angles\n",
    "            angles[i - 1] = angle_between(A, B)\n",
    "\n",
    "        label = exp[[\"alpha\", \"D\", \"state\"]].to_numpy()\n",
    "        # adding one to the states\n",
    "        label[:, 2] = label[:, 2] + 1\n",
    "        # If the data is longer than T_max, truncate it\n",
    "        if data.shape[0] > T_max:\n",
    "            # final_data[n, :, :] = data[:T_max, :]\n",
    "            final_data[n, :, 0] = Dx[: (T_max - 1)]\n",
    "            final_data[n, :, 1] = Dy[: (T_max - 1)]\n",
    "            final_data[n, :, 2] = MDx[: (T_max - 1)]\n",
    "            final_data[n, :, 3] = MDy[: (T_max - 1)]\n",
    "            final_data[n, :, 4] = distance_displacement[: (T_max - 1)]\n",
    "            final_data[n, :, 5] = angles[: (T_max - 1)]\n",
    "\n",
    "            final_label[n, :, :] = label[: T_max - 1, :]\n",
    "\n",
    "        # Otherwise, pad the data to T_max\n",
    "        else:\n",
    "            final_data[n, : (data.shape[0] - 1), 0] = Dx\n",
    "            final_data[n, : (data.shape[0] - 1), 1] = Dy\n",
    "            final_data[n, : (data.shape[0] - 1), 2] = MDx\n",
    "            final_data[n, : (data.shape[0] - 1), 3] = MDy\n",
    "            final_data[n, : (data.shape[0] - 1), 4] = distance_displacement\n",
    "            final_data[n, : (data.shape[0] - 1), 5] = angles\n",
    "            final_label[n, : data.shape[0] - 1, :] = label[:-1, :]\n",
    "\n",
    "    # Return the padded data and label\n",
    "    return final_data, final_label\n",
    "\n",
    "\n",
    "# Define a function to normalize data\n",
    "def normalize_df(data):\n",
    "    # Calculate displacement in x and y directions\n",
    "    # Normalize by substring mean and dividing by variance.\n",
    "\n",
    "    displacement_x = []\n",
    "    displacement_y = []\n",
    "    for _, group in data.groupby(\"traj_idx\"):\n",
    "        x = np.asarray(group[\"x\"])\n",
    "        y = np.asarray(group[\"y\"])\n",
    "        d_x = x[1:] - x[:-1]\n",
    "        d_y = y[1:] - y[:-1]\n",
    "        displacement_x = displacement_x + list(d_x)\n",
    "        displacement_y = displacement_y + list(d_y)\n",
    "\n",
    "    # Calculate variance in x and y directions\n",
    "    variance_x = np.sqrt(np.std(displacement_x))\n",
    "    variance_y = np.sqrt(np.std(displacement_y))\n",
    "\n",
    "    # Normalize data\n",
    "    data.loc[:, \"x\"] = (data[\"x\"] - data[\"x\"].mean()) / variance_x\n",
    "    data.loc[:, \"y\"] = (data[\"y\"] - data[\"y\"].mean()) / variance_y\n",
    "\n",
    "\n",
    "def normalize_np(data):\n",
    "\n",
    "    displacement_x = []\n",
    "    displacement_y = []\n",
    "    for n in range(data.shape[0]):\n",
    "        x = data[n, :, 1]\n",
    "        y = data[n, :, 2]\n",
    "        d_x = x[1:] - x[:-1]\n",
    "        d_y = y[1:] - y[:-1]\n",
    "        displacement_x = displacement_x + list(d_x)\n",
    "        displacement_y = displacement_y + list(d_y)\n",
    "\n",
    "    # Calculate variance in x and y directions\n",
    "    variance_x = np.sqrt(np.std(displacement_x))\n",
    "    variance_y = np.sqrt(np.std(displacement_y))\n",
    "\n",
    "    # Normalize data\n",
    "\n",
    "    data[:, :, 1] = (data[:, :, 1] - np.mean(data[:, :, 1])) / variance_x\n",
    "    data[:, :, 2] = (data[:, :, 2] - np.mean(data[:, :, 2])) / variance_x\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Define a function to list directory tree with pathlib\n",
    "def list_directory_tree_with_pathlib(starting_directory):\n",
    "    path_object = Path(starting_directory)\n",
    "    folders = []\n",
    "    for file_path in path_object.rglob(\"*.csv\"):\n",
    "        folders.append(file_path)\n",
    "    return folders\n",
    "\n",
    "\n",
    "# Define a custom dataset class for all data\n",
    "@dataclass\n",
    "class Dataset_all_data(Dataset):\n",
    "    # Initialize filenames and transform flag\n",
    "    # Pad value should be a tuple such as (N, Tmax)\n",
    "    filenames: list\n",
    "    transform: bool = False\n",
    "    pad: None | tuple = None\n",
    "    noise: bool = False\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of files\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read csv file and extract data and label\n",
    "        df = pd.read_csv(self.filenames[idx])\n",
    "\n",
    "        if self.pad is None:\n",
    "            data = df[[\"traj_idx\", \"frame\", \"x\", \"y\"]]\n",
    "            label = np.asarray(df[[\"alpha\", \"D\"]])\n",
    "            label_2 = np.asarray(df[\"state\"])\n",
    "\n",
    "        else:\n",
    "            if len(self.pad) != 2:\n",
    "                raise ValueError(\"pad value should be set as (N, T_max)\")\n",
    "            data, label = apply_padding(df, *self.pad)\n",
    "            data = data[:, :, :]  # Removing the frame column\n",
    "            label_2 = label[:, :, -1]\n",
    "            label_2[label_2[:, :] > 0] = label_2[label_2[:, :] > 0]\n",
    "            label = label[:, :, :-1]\n",
    "\n",
    "        # Normalize data if transform flag is True\n",
    "        if self.transform:\n",
    "            if self.pad is None:\n",
    "                normalize_df(data)\n",
    "                data = np.asarray(data)\n",
    "            else:\n",
    "                data = normalize_np(data)\n",
    "\n",
    "        if self.noise:\n",
    "            data = add_noise(data)\n",
    "\n",
    "        # Normalize D between 0 and 1\n",
    "        label_K = np.zeros((label.shape[0], 2))\n",
    "        for i in range(label.shape[0]):\n",
    "            K = np.unique(label[i, :, 1][label[i, :, 1] != 0])\n",
    "            if len(K) == 2:\n",
    "                label_K[i, :] = K\n",
    "\n",
    "                if label[i, 0, 1] != label_K[i, 0]:\n",
    "                    label_K[i, :] = label_K[i, ::-1]\n",
    "\n",
    "            elif len(K) == 1:\n",
    "                states = label_2[i, :]\n",
    "                if 1 in states:\n",
    "                    if states[0] == 1:\n",
    "                        label_K[i, :] = [0, K[0]]\n",
    "                    else:\n",
    "                        label_K[i, :] = [K[0], 0]\n",
    "                else:\n",
    "                    label_K[i, :] = [K[0], K[0]]\n",
    "\n",
    "            else:\n",
    "                if np.unique(label[i, :, 1]) == 0:\n",
    "                    label_K[i, :] = [0, 0]\n",
    "                else:\n",
    "                    raise Exception(\"more than 2 diffusions\")\n",
    "\n",
    "        label_alpha = np.zeros((label.shape[0], 2))\n",
    "\n",
    "        for i in range(label.shape[0]):\n",
    "            alpha = np.unique(label[i, :, 0][label[i, :, 0] != 0])\n",
    "            if len(alpha) == 2:\n",
    "                label_alpha[i, :] = alpha\n",
    "                if label[i, 0, 0] != label_alpha[i, 0]:\n",
    "                    label_alpha[i, :] = label_alpha[i, ::-1]\n",
    "\n",
    "            elif len(alpha) == 1:\n",
    "                states = label_2[i, :]\n",
    "                if 1 in states:\n",
    "                    if states[0] == 1:\n",
    "                        label_alpha[i, :] = [0, alpha[0]]\n",
    "                    else:\n",
    "                        label_alpha[i, :] = [alpha[0], 0]\n",
    "                else:\n",
    "                    label_alpha[i, :] = [alpha[0], alpha[0]]\n",
    "\n",
    "            else:\n",
    "                if np.unique(label[i, :, 1]) == 0:\n",
    "                    label_alpha[i, :] = [0, 0]\n",
    "                else:\n",
    "                    raise Exception(\"more than 2 diffusions\")\n",
    "\n",
    "        label_segmentation = np.zeros((label_2.shape[0], label_2.shape[1]))\n",
    "\n",
    "        for i in range(label.shape[0]):\n",
    "            if label_K[i, 0] == label_K[i, 1]:\n",
    "                position = label[i, :, 1] == label_K[i, 0]\n",
    "                label_segmentation[i, position] = 1\n",
    "            else:\n",
    "\n",
    "                position_1 = label[i, :, 1] == label_K[i, 0]\n",
    "                position_2 = label[i, :, 1] == label_K[i, 1]\n",
    "\n",
    "                label_segmentation[i, position_1] = 1\n",
    "                label_segmentation[i, position_2] = 2\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(data.astype(np.float32)),\n",
    "            torch.from_numpy(label_segmentation.astype(np.float32)),\n",
    "            torch.from_numpy(label_K),\n",
    "            torch.from_numpy(label_alpha),\n",
    "        )\n",
    "\n",
    "\n",
    "def add_noise(data):\n",
    "    noise_amplitude = np.random.choice(\n",
    "        [\n",
    "            0.01,\n",
    "            0.1,\n",
    "        ]\n",
    "    )\n",
    "    noise = np.random.normal(0, noise_amplitude, data[:, :, :].shape)\n",
    "    data[:, :, :][data[:, :, 1:] != 0] = (\n",
    "        data[:, :, :][data[:, :, 1:] != 0] + data[:, :, :][data[:, :, 1:] != 0] * noise\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class segmentation_model(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand, dropout=0.2, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.mamba = Mamba(\n",
    "            d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand\n",
    "        ).to(device)\n",
    "        self.flipped_mamba = Mamba(\n",
    "            d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand\n",
    "        ).to(device)\n",
    "        self.dropout = nn.Dropout(p=dropout).to(device)\n",
    "        self.fc = nn.Linear(in_features=d_model, out_features=3).to(device)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, input):\n",
    "        mamba_out = self.mamba(input)\n",
    "        mamba_flipped_out = self.flipped_mamba(torch.flip(input, dims=[1]))\n",
    "        mamba_out = mamba_out + mamba_flipped_out\n",
    "        mamba_out = self.dropout(mamba_out)\n",
    "        out = self.fc(mamba_out)\n",
    "        return out              # No activation here ! It is done by the cross entropy loss\n",
    "\n",
    "class alpha_regression(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand, dropout=0.3, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.mamba = Mamba(\n",
    "            d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand\n",
    "        ).to(device)\n",
    "        self.flipped_mamba = Mamba(\n",
    "            d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand\n",
    "        ).to(device)\n",
    "        self.dropout = nn.Dropout(p=dropout).to(device)\n",
    "        self.fc = nn.Linear(in_features=199 * d_model, out_features=2).to(device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        mamba_out = self.mamba(input)\n",
    "        mamba_flipped_out = self.flipped_mamba(torch.flip(input, dims=[1]))\n",
    "        mamba_out = mamba_out + mamba_flipped_out\n",
    "        mamba_out = rearrange(mamba_out, \"b l c -> b (l c)\")\n",
    "        mamba_out = self.dropout(mamba_out)\n",
    "        out = self.fc(mamba_out)\n",
    "        return self.sigmoid(out) * 2\n",
    "\n",
    "class K_regression(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand, dropout=0.3, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.mamba = Mamba(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand,\n",
    "        ).to(device)\n",
    "        self.flipped_mamba = Mamba(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand,\n",
    "        ).to(device)\n",
    "        self.dropout = nn.Dropout(p=dropout).to(device)\n",
    "        self.fc = nn.Linear(in_features=199 * d_model, out_features=2).to(device)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, input):\n",
    "        mamba_out = self.mamba(input)\n",
    "        mamba_flipped_out = self.flipped_mamba(torch.flip(input, dims=[1]))\n",
    "        mamba_out = mamba_out + mamba_flipped_out\n",
    "        mamba_out = rearrange(mamba_out, \"b l c -> b (l c)\")\n",
    "        mamba_out = self.dropout(mamba_out)\n",
    "        out = self.fc(mamba_out)\n",
    "        out = torch.clamp(out, min=0, max=1e12)\n",
    "        out[out < 1e-7] = 0\n",
    "        return out\n",
    "\n",
    "class MSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, actual):\n",
    "        return self.mse(torch.log(pred + 1), torch.log(actual + 1))\n",
    "    \n",
    "class all_at_the_same_time(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand, dropout=0.2, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model_K = K_regression(\n",
    "            d_model, d_state=d_state, d_conv=d_conv, expand=expand, dropout=dropout\n",
    "        )\n",
    "        self.model_alpha = alpha_regression(\n",
    "            d_model, d_state=d_state, d_conv=d_conv, expand=expand, dropout=dropout\n",
    "        )\n",
    "        self.segmentation = segmentation_model(\n",
    "            d_model, d_state=d_state, d_conv=d_conv, expand=expand, dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        probas = self.segmentation(x)\n",
    "        classes = torch.argmax(torch.softmax(probas, dim=2), dim=2)\n",
    "        classes[x[:, :, 0] == 0] = \n",
    "        classes[x[:, :, 0] == 0] = 0\n",
    "        classes = classes.unsqueeze(-1)  \n",
    "        concat_entry = torch.cat((classes, x[:,:,:-1]), dim=2)\n",
    "        alpha = self.model_alpha(concat_entry)\n",
    "        K = self.model_K(concat_entry)\n",
    "        return probas, alpha, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = all_at_the_same_time(d_model=6, d_state=4, d_conv=4, expand=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "alpha_criterion = torch.nn.L1Loss().to(\"cuda\")\n",
    "K_criterion = MSLELoss().to(\"cuda\")\n",
    "\n",
    "all_data_set = list_directory_tree_with_pathlib(\n",
    "    r\"/home/m.lavaud/ANDI_2_Challenge_EMetBrown/data/datasets\",\n",
    ")\n",
    "np.random.shuffle(all_data_set)\n",
    "\n",
    "training_dataset = Dataset_all_data(all_data_set[:10000], transform=False, pad=(20, 200))\n",
    "test_dataset = Dataset_all_data(all_data_set[-100:], transform=False, pad=(20,200))\n",
    "\n",
    "dataloader = DataLoader(training_dataset, shuffle=True, batch_size=10, num_workers=5)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=10, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_loss(model, test_dataloader, alpha_criterion, K_criterion):\n",
    "    model.eval()\n",
    "    test_classification_loss = []\n",
    "    test_alpha_loss = []\n",
    "    test_K_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, classification_targets, K_targets, alpha_targets in test_dataloader:\n",
    "            inputs = inputs.to(\"cuda\", dtype=torch.float32)  \n",
    "            inputs = torch.flatten(inputs, start_dim=0, end_dim=1)\n",
    "\n",
    "            classification_targets = (\n",
    "                torch.flatten(\n",
    "                    classification_targets,\n",
    "                    start_dim=0,\n",
    "                    end_dim=1,\n",
    "                )\n",
    "               .type(torch.LongTensor)\n",
    "               .to(\"cuda\")\n",
    "            )\n",
    "\n",
    "            classification_output, alpha_output, K_output= model(inputs)\n",
    "\n",
    "            classification_output = torch.squeeze(classification_output)\n",
    "\n",
    "            counts = torch.unique(classification_targets, return_counts=True)[1][1:]\n",
    "            weights = torch.sum(counts) / (2 * counts)\n",
    "            weights = weights.to(\"cpu\", dtype=torch.float32)  \n",
    "            weight = torch.zeros(3, dtype=torch.float32)  \n",
    "            weight[1:] = weights\n",
    "\n",
    "            classification_criterion = nn.CrossEntropyLoss(\n",
    "                weight=weight, ignore_index=0\n",
    "            )\n",
    "\n",
    "            classification_loss = classification_criterion(\n",
    "                classification_output.view(-1, 3).to(\n",
    "                    \"cpu\", dtype=torch.float32\n",
    "                ),  \n",
    "                classification_targets.view(-1).to(\"cpu\"),\n",
    "            ).to(\"cuda\")\n",
    "            alpha_targets = torch.flatten(alpha_targets, start_dim=0, end_dim=1).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )\n",
    "            alpha_loss = alpha_criterion(alpha_output, alpha_targets).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            ) \n",
    "            K_targets = torch.flatten(K_targets, start_dim=0, end_dim=1).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            ) \n",
    "            K_loss = K_criterion(K_output, K_targets).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  \n",
    "\n",
    "            test_classification_loss.append(classification_loss.item())\n",
    "            test_alpha_loss.append(alpha_loss.item())\n",
    "            test_K_loss.append(K_loss.item())\n",
    "\n",
    "    return np.mean(test_classification_loss), np.mean(test_alpha_loss), np.mean(test_K_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 793/793 [02:49<00:00,  4.67batch/s, loss_K=0.0287, loss_a=0.208, loss_c=0.733] \n",
      "Epoch 1:  27%|██▋       | 215/793 [00:46<01:32,  6.27batch/s, loss_K=0.0365, loss_a=0.324, loss_c=0.72]  "
     ]
    }
   ],
   "source": [
    "max_epoch = 50\n",
    "total_classification_loss = []\n",
    "total_K_loss = []\n",
    "total_alpha_loss = []\n",
    "test_classification_loss = []\n",
    "test_K_loss = []\n",
    "test_alpha_loss = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    running_classification_loss = []\n",
    "    running_alpha_loss = []\n",
    "    running_K_loss = []\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "        model.train()\n",
    "\n",
    "        for inputs, classification_targets, K_targets, alpha_targets in tepoch:\n",
    "\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            inputs = inputs.to(\"cuda\", dtype=torch.float32)  \n",
    "            inputs = torch.flatten(inputs, start_dim=0, end_dim=1)\n",
    "\n",
    "            classification_targets = (\n",
    "                torch.flatten(\n",
    "                    classification_targets,\n",
    "                    start_dim=0,\n",
    "                    end_dim=1,\n",
    "                )\n",
    "                .type(torch.LongTensor)\n",
    "                .to(\"cuda\")\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            classification_output, alpha_output, K_output = model(inputs)\n",
    "            classification_output = torch.squeeze(classification_output)\n",
    "\n",
    "            ## Computation of the weight of the classes\n",
    "\n",
    "            counts = torch.unique(classification_targets, return_counts=True)[1][1:]\n",
    "            weights = torch.sum(counts) / (2 * counts)\n",
    "            weights = weights.to(\"cpu\", dtype=torch.float32)  \n",
    "            weight = torch.zeros(3, dtype=torch.float32)  \n",
    "            weight[1:] = weights\n",
    "\n",
    "            classification_criterion = nn.CrossEntropyLoss(\n",
    "                weight=weight, ignore_index=0\n",
    "            )\n",
    "\n",
    "            classification_loss = classification_criterion(\n",
    "                classification_output.view(-1, 3).to(\n",
    "                    \"cpu\", dtype=torch.float32\n",
    "                ),  \n",
    "                classification_targets.view(-1).to(\"cpu\"),\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            alpha_targets = torch.flatten(alpha_targets, start_dim=0, end_dim=1).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  \n",
    "            alpha_loss = alpha_criterion(alpha_output, alpha_targets).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  \n",
    "\n",
    "            K_targets = torch.flatten(K_targets, start_dim=0, end_dim=1).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  \n",
    "            K_loss = K_criterion(K_output, K_targets).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  \n",
    "\n",
    "            total_loss = alpha_loss + K_loss + classification_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tepoch.set_postfix(\n",
    "                loss_c=classification_loss.item(),\n",
    "                loss_a=alpha_loss.item(),\n",
    "                loss_K=K_loss.item(),\n",
    "            )\n",
    "\n",
    "            running_classification_loss.append(classification_loss.item())\n",
    "            running_alpha_loss.append(\n",
    "                alpha_loss.item()\n",
    "            )  \n",
    "            running_K_loss.append(K_loss.item())  \n",
    "\n",
    "        runnin_test_class_loss, runnin_test_alpha_loss, runnin_test_K_loss = compute_test_loss(model, test_dataloader, alpha_criterion, K_criterion)\n",
    "        total_classification_loss.append(np.mean(running_classification_loss))\n",
    "        total_alpha_loss.append(np.mean(running_alpha_loss))\n",
    "        total_K_loss.append(np.mean(running_K_loss))\n",
    "\n",
    "\n",
    "        test_classification_loss.append(runnin_test_class_loss)\n",
    "        test_K_loss.append(runnin_test_alpha_loss)\n",
    "        test_alpha_loss.append(runnin_test_K_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3779325485229492,\n",
       " 1.026025584936142,\n",
       " 0.9299487566947937,\n",
       " 0.8623801159858704,\n",
       " 0.8259282028675079,\n",
       " 0.7968046057224274,\n",
       " 0.7707876229286194,\n",
       " 0.7510102748870849,\n",
       " 0.7388714849948883,\n",
       " 0.7302161526679992]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_classification_lossdata = {}\n",
    "data[\"total_alpha_loss\"] = total_alpha_loss\n",
    "data[\"total_classification_loss\"] = total_classification_loss\n",
    "data[\"total_K_loss\"] = total_K_loss\n",
    "\n",
    "data[\"test_alpha_loss\"] = test_alpha_loss\n",
    "data[\"test_classification_loss\"] = test_classification_loss\n",
    "data[\"test_K_loss\"] = test_K_loss\n",
    "import pickle\n",
    "with open(\"loss_2_xy_features_new_bimamba.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "torch.save(model, \"more_features_10epoch_5kfiles\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
