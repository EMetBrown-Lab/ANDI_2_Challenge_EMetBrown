{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Function to pad an array to a specific shape\n",
    "def to_shape(a, shape):\n",
    "    # Unpack the target shape\n",
    "    y_, x_ = shape\n",
    "\n",
    "    # Get the current shape of the array\n",
    "    y, x = a.shape\n",
    "\n",
    "    # Calculate the padding needed in the y and x directions\n",
    "    y_pad = y_ - y\n",
    "    x_pad = x_ - x\n",
    "    output = np.zeros()\n",
    "    # Pad the array using numpy's pad function\n",
    "    return np.pad(\n",
    "        a,\n",
    "        [(0, 1), (0, 1)],\n",
    "        # Calculate the padding for each dimension\n",
    "        # ((y_pad // 2, y_pad // 2 + y_pad % 2), (x_pad // 2, x_pad // 2 + x_pad % 2)),\n",
    "        mode=\"constant\",\n",
    "    )\n",
    "\n",
    "def angle_between(p1, p2):\n",
    "    ang1 = np.arctan2(*p1[::-1])\n",
    "    ang2 = np.arctan2(*p2[::-1])\n",
    "    return (ang1 - ang2) % (2 * np.pi)\n",
    "\n",
    "# Function to pad data and labels to a specific shape\n",
    "def apply_padding(data_df, N, T_max):\n",
    "    # Define the final shape of the data and labels\n",
    "    final_shape = (N, T_max-1, 6)\n",
    "\n",
    "    # Initialize the final data and labels with zeros\n",
    "    final_data = np.zeros(final_shape)\n",
    "    final_label = np.zeros((N, T_max-1, 3))\n",
    "\n",
    "    # Select a random subset of trajectory indices\n",
    "    if len(data_df[\"traj_idx\"].unique()) < N:\n",
    "        selected_ids = np.random.choice(\n",
    "            data_df[\"traj_idx\"].unique(), size=N, replace=True\n",
    "        )\n",
    "    else:\n",
    "        selected_ids = np.random.choice(\n",
    "            data_df[\"traj_idx\"].unique(), size=N, replace=False\n",
    "        )\n",
    "\n",
    "    # Iterate over the selected trajectory indices\n",
    "    for n, id in enumerate(selected_ids):\n",
    "        # Filter the data for the current trajectory index\n",
    "        exp = data_df[data_df[\"traj_idx\"] == id]\n",
    "\n",
    "        # Extract the data and labels for the current trajectory\n",
    "        data = exp[[\"frame\", \"x\", \"y\"]].to_numpy()  \n",
    "        data[:, 0] = data[:, 0] - data[0, 0] + 1  # putting first frame rate to 1\n",
    "        data[:, 1] = data[:, 1] - data[0, 1]  # putting initial position to 0\n",
    "        data[:, 2] = (\n",
    "            data[:, 2] - data[0, 2]\n",
    "        )  # putting initital position to 0        # print(exp[\"frame\"])\n",
    "        # Displacement\n",
    "        Dx = data[1:,1] - data[:-1,1]\n",
    "        Dy = data[1:,2] - data[:-1,2]\n",
    "        MDx = np.zeros(len(Dx))\n",
    "        MDy = np.zeros(len(Dx))\n",
    "        angles = np.zeros(len(Dx))\n",
    "        distance_displacement = np.sqrt(np.power(Dx,2) + np.power(Dy,2))\n",
    "        #Displacement average\n",
    "\n",
    "        for i in range(1, len(Dx)+1):\n",
    "            MDx[i-1] = np.mean(data[i:,1] - data[:-i,1])\n",
    "            MDy[i-1] = np.mean(data[i:,2] - data[:-i,2])\n",
    "\n",
    "            A = (data[(i-1),1], data[(i-1),2])\n",
    "            B = (data[i,1], data[i,2])\n",
    "        \n",
    "            # Computation of angles\n",
    "\n",
    "            angles[i-1] = angle_between(A,B)\n",
    "\n",
    "\n",
    "\n",
    "        label = exp[[\"alpha\", \"D\", \"state\"]].to_numpy()\n",
    "        ## adding one to the states\n",
    "        label[:, 2] = label[:, 2] + 1\n",
    "        # If the data is longer than T_max, truncate it\n",
    "        if data.shape[0] > T_max:\n",
    "            # final_data[n, :, :] = data[:T_max, :]\n",
    "            final_data[n,:,0] = Dx[:(T_max-1)]\n",
    "            final_data[n,:,1] = Dy[:(T_max-1)]\n",
    "            final_data[n,:,2] = MDx[:(T_max-1)]\n",
    "            final_data[n,:,3] = MDy[:(T_max-1)]\n",
    "            final_data[n,:,4] = distance_displacement[:(T_max-1)]\n",
    "            final_data[n,:,5] = angles[:(T_max-1)]\n",
    "\n",
    "            final_label[n, :, :] = label[:T_max-1, :]\n",
    "\n",
    "        # Otherwise, pad the data to T_max\n",
    "        else:\n",
    "            # print((label.shape, T_max))\n",
    "            final_data[n, : (data.shape[0] -1), 0] = Dx\n",
    "            final_data[n, : (data.shape[0] -1), 1] = Dy\n",
    "            final_data[n, : (data.shape[0] -1), 2] = MDx\n",
    "            final_data[n, : (data.shape[0] -1), 3] = MDy\n",
    "            final_data[n, : (data.shape[0] -1), 4] = distance_displacement\n",
    "            final_data[n, : (data.shape[0] -1), 5] = angles\n",
    "\n",
    "            final_label[n, : data.shape[0] -1, :] = label[:-1, :]\n",
    "\n",
    "    # Return the padded data and label\n",
    "    return final_data, final_label\n",
    "\n",
    "\n",
    "# Define a function to normalize data\n",
    "def normalize_df(data):\n",
    "    # Calculate displacement in x and y directions\n",
    "    # Normalize by substring mean and dividing by variance.\n",
    "\n",
    "    displacement_x = []\n",
    "    displacement_y = []\n",
    "    for _, group in data.groupby(\"traj_idx\"):\n",
    "        x = np.asarray(group[\"x\"])\n",
    "        y = np.asarray(group[\"y\"])\n",
    "        d_x = x[1:] - x[:-1]\n",
    "        d_y = y[1:] - y[:-1]\n",
    "        displacement_x = displacement_x + list(d_x)\n",
    "        displacement_y = displacement_y + list(d_y)\n",
    "\n",
    "    # Calculate variance in x and y directions\n",
    "    variance_x = np.sqrt(np.std(displacement_x))\n",
    "    variance_y = np.sqrt(np.std(displacement_y))\n",
    "\n",
    "    # Normalize data\n",
    "    data.loc[:, \"x\"] = (data[\"x\"] - data[\"x\"].mean()) / variance_x\n",
    "    data.loc[:, \"y\"] = (data[\"y\"] - data[\"y\"].mean()) / variance_y\n",
    "\n",
    "\n",
    "def normalize_np(data):\n",
    "\n",
    "    displacement_x = []\n",
    "    displacement_y = []\n",
    "    for n in range(data.shape[0]):\n",
    "        x = data[n, :, 1]\n",
    "        y = data[n, :, 2]\n",
    "        d_x = x[1:] - x[:-1]\n",
    "        d_y = y[1:] - y[:-1]\n",
    "        displacement_x = displacement_x + list(d_x)\n",
    "        displacement_y = displacement_y + list(d_y)\n",
    "\n",
    "    # Calculate variance in x and y directions\n",
    "    variance_x = np.sqrt(np.std(displacement_x))\n",
    "    variance_y = np.sqrt(np.std(displacement_y))\n",
    "\n",
    "    # Normalize data\n",
    "\n",
    "    data[:, :, 1] = (data[:, :, 1] - np.mean(data[:, :, 1])) / variance_x\n",
    "    data[:, :, 2] = (data[:, :, 2] - np.mean(data[:, :, 2])) / variance_x\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Define a function to list directory tree with pathlib\n",
    "def list_directory_tree_with_pathlib(starting_directory):\n",
    "    path_object = Path(starting_directory)\n",
    "    folders = []\n",
    "    for file_path in path_object.rglob(\"*.csv\"):\n",
    "        folders.append(file_path)\n",
    "    return folders\n",
    "\n",
    "\n",
    "# Define a custom dataset class for all data\n",
    "@dataclass\n",
    "class Dataset_all_data(Dataset):\n",
    "    # Initialize filenames and transform flag\n",
    "    # Pad value should be a tuple such as (N, Tmax)\n",
    "    filenames: list\n",
    "    transform: bool = False\n",
    "    pad: None | tuple = None\n",
    "    noise: bool = False\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of files\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read csv file and extract data and label\n",
    "        df = pd.read_csv(self.filenames[idx])\n",
    "\n",
    "        if self.pad is None:\n",
    "            data = df[[\"traj_idx\", \"frame\", \"x\", \"y\"]]\n",
    "            label = np.asarray(df[[\"alpha\", \"D\"]])\n",
    "            label_2 = np.asarray(df[\"state\"])\n",
    "\n",
    "        else:\n",
    "            if len(self.pad) != 2:\n",
    "                raise ValueError(\"pad value should be set as (N, T_max)\")\n",
    "            data, label = apply_padding(df, *self.pad)\n",
    "            data = data[:, :, :]  ## Removing the frame column\n",
    "            label_2 = label[:, :, -1]\n",
    "            label_2[label_2[:, :] > 0] = label_2[label_2[:, :] > 0]\n",
    "            label = label[:, :, :-1]\n",
    "\n",
    "        # Normalize data if transform flag is True\n",
    "        if self.transform:\n",
    "            if self.pad is None:\n",
    "                normalize_df(data)\n",
    "                data = np.asarray(data)\n",
    "            else:\n",
    "                data = normalize_np(data)\n",
    "\n",
    "        if self.noise:\n",
    "            data = add_noise(data)\n",
    "\n",
    "        # Normalize D between 0 and 1\n",
    "\n",
    "        # label[:,:,1][label[:,:,1] != 0] = np.log(label[:,:,1][label[:,:,1] != 0]) #- np.log(1e-6)) #/   (np.log(1e12) - np.log(1e-6))\n",
    "        # label = label[:,:,1]\n",
    "        label_K = np.zeros((label.shape[0], 2))\n",
    "\n",
    "        # print(np.unique(label_2))\n",
    "\n",
    "        for i in range(label.shape[0]):\n",
    "            K = np.unique(label[i, :, 1][label[i, :, 1] != 0])\n",
    "            if len(K) == 2:\n",
    "                label_K[i, :] = K\n",
    "\n",
    "                if label[i, 0, 1] != label_K[i, 0]:\n",
    "                    label_K[i, :] = label_K[i, ::-1]\n",
    "\n",
    "            elif len(K) == 1:\n",
    "                states = label_2[i, :]\n",
    "                if 1 in states:\n",
    "                    # print(np.unique(states))\n",
    "                    if states[0] == 1:\n",
    "                        label_K[i, :] = [0, K[0]]\n",
    "                    else:\n",
    "                        label_K[i, :] = [K[0], 0]\n",
    "\n",
    "                    # print(label_regression[i,:])\n",
    "\n",
    "                else:\n",
    "                    label_K[i, :] = [K[0], K[0]]\n",
    "\n",
    "            else:\n",
    "                if np.unique(label[i, :, 1]) == 0:\n",
    "                    label_K[i, :] = [0, 0]\n",
    "                else:\n",
    "\n",
    "                    # print(np.unique(label[i,:,1]))\n",
    "\n",
    "                    # print(Ds)\n",
    "                    raise Exception(\"more than 2 diffusions\")\n",
    "\n",
    "        # print(np.unique(label_2))\n",
    "        label_alpha = np.zeros((label.shape[0], 2))\n",
    "\n",
    "        for i in range(label.shape[0]):\n",
    "            alpha = np.unique(label[i, :, 0][label[i, :, 0] != 0])\n",
    "            if len(alpha) == 2:\n",
    "                label_alpha[i, :] = alpha\n",
    "                if label[i, 0, 0] != label_alpha[i, 0]:\n",
    "                    label_alpha[i, :] = label_alpha[i, ::-1]\n",
    "\n",
    "            elif len(alpha) == 1:\n",
    "                states = label_2[i, :]\n",
    "                if 1 in states:\n",
    "                    # print(np.unique(states))\n",
    "                    if states[0] == 1:\n",
    "                        label_alpha[i, :] = [0, alpha[0]]\n",
    "                    else:\n",
    "                        label_alpha[i, :] = [alpha[0], 0]\n",
    "\n",
    "                    # print(label_regression[i,:])\n",
    "\n",
    "                else:\n",
    "                    label_alpha[i, :] = [alpha[0], alpha[0]]\n",
    "\n",
    "            else:\n",
    "                if np.unique(label[i, :, 1]) == 0:\n",
    "                    label_alpha[i, :] = [0, 0]\n",
    "                else:\n",
    "\n",
    "                    # print(np.unique(label[i,:,1]))\n",
    "\n",
    "                    # print(Ds)\n",
    "                    raise Exception(\"more than 2 diffusions\")\n",
    "\n",
    "        label_segmentation = np.zeros((label_2.shape[0], label_2.shape[1]))\n",
    "\n",
    "        for i in range(label.shape[0]):\n",
    "            if label_K[i, 0] == label_K[i, 1]:\n",
    "                position = label[i, :, 1] == label_K[i, 0]\n",
    "                label_segmentation[i, position] = 1\n",
    "            else:\n",
    "\n",
    "                position_1 = label[i, :, 1] == label_K[i, 0]\n",
    "                position_2 = label[i, :, 1] == label_K[i, 1]\n",
    "\n",
    "                label_segmentation[i, position_1] = 1\n",
    "                label_segmentation[i, position_2] = 2\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(data.astype(np.float32)),\n",
    "            torch.from_numpy(label_segmentation.astype(np.float32)),\n",
    "            torch.from_numpy(label_K),\n",
    "            torch.from_numpy(label_alpha),\n",
    "        )\n",
    "        # torch.from_numpy(label_2.astype(np.float32)),\n",
    "\n",
    "\n",
    "def add_noise(data):\n",
    "    noise_amplitude = np.random.choice(\n",
    "        [\n",
    "            0.01,\n",
    "            0.1,\n",
    "        ]\n",
    "    )\n",
    "    noise = np.random.normal(0, noise_amplitude, data[:, :, :].shape)\n",
    "    data[:, :, :][data[:, :, 1:] != 0] = (\n",
    "        data[:, :, :][data[:, :, 1:] != 0] + data[:, :, :][data[:, :, 1:] != 0] * noise\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_set = list_directory_tree_with_pathlib(\n",
    "    r\"/home/m.lavaud/ANDI_2_Challenge_EMetBrown/data/datasets\",\n",
    ")\n",
    "np.random.shuffle(all_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7925"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = Dataset_all_data(all_data_set[:10000], transform=False, pad=(20, 200))\n",
    "test_dataset = Dataset_all_data(all_data_set[-100:], transform=False, pad=(20,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = iter(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 5.1690e-01,  3.5175e-01,  5.3626e-03,  4.0330e-03,  6.2523e-01,\n",
       "            5.6857e+00],\n",
       "          [ 3.1081e-02, -1.0704e+00,  1.0216e-02,  3.5594e-03,  1.0708e+00,\n",
       "            1.5168e+00],\n",
       "          [ 3.4703e-01,  1.2125e+00,  1.1217e-02,  8.6874e-03,  1.2612e+00,\n",
       "            4.8597e+00],\n",
       "          ...,\n",
       "          [ 1.8398e-02, -3.1197e-01,  7.3660e-01,  5.7048e-01,  3.1251e-01,\n",
       "            3.2360e-01],\n",
       "          [ 7.3770e-01, -3.4088e-02,  1.0114e+00,  3.5238e-01,  7.3849e-01,\n",
       "            2.0277e-01],\n",
       "          [-4.0545e-01,  5.4861e-01,  1.0672e+00,  8.0256e-01,  6.8217e-01,\n",
       "            5.8091e+00]],\n",
       " \n",
       "         [[ 1.6965e+00, -1.1654e-01,  6.7169e-02, -1.6952e-02,  1.7005e+00,\n",
       "            6.8585e-02],\n",
       "          [-9.8620e-01, -1.2946e+00,  9.6924e-02, -9.1540e-02,  1.6274e+00,\n",
       "            1.0358e+00],\n",
       "          [-1.1606e-02,  1.6607e-01,  1.1608e-01,  2.2533e-03,  1.6648e-01,\n",
       "            6.2381e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00]],\n",
       " \n",
       "         [[ 2.7327e+00,  1.9640e+00,  1.5243e-02, -7.0543e-03,  3.3652e+00,\n",
       "            5.6600e+00],\n",
       "          [-2.4388e+00, -4.3126e-02,  2.0058e-02, -1.4084e-02,  2.4391e+00,\n",
       "            5.4874e+00],\n",
       "          [ 9.8164e-03,  1.0375e-02,  4.8936e-02, -1.8444e-02,  1.4283e-02,\n",
       "            4.1607e-03],\n",
       "          ...,\n",
       "          [ 1.4152e+00, -5.7497e-01,  3.2135e+00, -1.2111e+00,  1.5275e+00,\n",
       "            1.6971e-01],\n",
       "          [-2.2921e+00, -4.9701e-01,  1.9857e+00, -1.3943e+00,  2.3454e+00,\n",
       "            2.2075e-02],\n",
       "          [-6.3741e-01, -1.9829e+00,  3.0334e+00, -1.4038e+00,  2.0829e+00,\n",
       "            5.8991e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1.1610e+00, -2.2625e-01,  9.6639e-03,  2.5011e-03,  1.1829e+00,\n",
       "            1.9246e-01],\n",
       "          [-3.1314e-01,  9.1360e-01,  1.5498e-02, -2.9483e-03,  9.6578e-01,\n",
       "            5.4095e+00],\n",
       "          [-3.8790e-01, -1.2464e-01,  1.8399e-02, -3.3863e-03,  4.0744e-01,\n",
       "            6.0789e+00],\n",
       "          ...,\n",
       "          [-3.9654e-01,  9.3525e-02,  1.2082e+00, -2.2237e-01,  4.0742e-01,\n",
       "            6.1532e+00],\n",
       "          [ 9.0252e-01, -1.9117e+00,  1.5343e+00, -2.9189e-01,  2.1141e+00,\n",
       "            9.2208e-01],\n",
       "          [-3.8335e-01,  1.8054e+00,  1.9231e+00,  4.9771e-01,  1.8457e+00,\n",
       "            5.5141e+00]],\n",
       " \n",
       "         [[ 5.8861e-01,  1.1889e+00,  7.7608e-03,  3.1418e-03,  1.3266e+00,\n",
       "            5.1721e+00],\n",
       "          [-3.1683e-01, -8.1361e-01,  8.9676e-03,  1.1490e-04,  8.7312e-01,\n",
       "            1.6705e-01],\n",
       "          [ 5.2395e-01, -4.7809e-01,  1.2512e-02,  1.1813e-03,  7.0929e-01,\n",
       "            1.0725e+00],\n",
       "          ...,\n",
       "          [-3.5018e-01, -3.3724e-01,  8.2163e-01,  7.7572e-02,  4.8617e-01,\n",
       "            6.6100e-02],\n",
       "          [-1.4129e-01,  1.1692e-03,  8.8780e-01,  1.1375e-02,  1.4129e-01,\n",
       "            6.2092e+00],\n",
       "          [ 7.2460e-01,  3.8781e-02,  1.5444e+00,  6.2522e-01,  7.2564e-01,\n",
       "            2.3629e-01]],\n",
       " \n",
       "         [[-2.5365e-01, -1.0974e-01,  3.0125e-04,  1.6912e-04,  2.7637e-01,\n",
       "            2.7333e+00],\n",
       "          [ 4.7344e-01, -2.2428e-01,  3.6792e-03,  3.7669e-04,  5.2388e-01,\n",
       "            4.5387e+00],\n",
       "          [-3.6135e-01,  1.0635e-01,  5.0848e-03,  2.9789e-03,  3.7668e-01,\n",
       "            1.1383e+00],\n",
       "          ...,\n",
       "          [ 4.2273e-01,  3.0046e-01,  3.3390e-01,  1.9561e-01,  5.1863e-01,\n",
       "            4.8873e+00],\n",
       "          [-7.8127e-02, -2.4705e-01,  3.6424e-01,  3.7292e-02,  2.5911e-01,\n",
       "            5.1126e-01],\n",
       "          [-3.5493e-01,  1.0247e-01,  5.9949e-02,  3.3655e-02,  3.6943e-01,\n",
       "            5.6073e+00]]]),\n",
       " tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]),\n",
       " tensor([[0.1460, 0.1460],\n",
       "         [0.5638, 0.5638],\n",
       "         [1.3225, 1.3225],\n",
       "         [0.1395, 0.1395],\n",
       "         [0.0594, 0.0758],\n",
       "         [0.2394, 0.2394],\n",
       "         [0.8455, 0.8455],\n",
       "         [0.0789, 0.0789],\n",
       "         [0.2257, 0.2257],\n",
       "         [0.1672, 0.1672],\n",
       "         [0.2648, 0.2648],\n",
       "         [0.5365, 0.5365],\n",
       "         [0.1419, 0.1419],\n",
       "         [0.0814, 0.0814],\n",
       "         [0.2257, 0.2257],\n",
       "         [0.2825, 0.2825],\n",
       "         [1.1523, 1.1523],\n",
       "         [0.2833, 0.2833],\n",
       "         [0.1441, 0.1441],\n",
       "         [0.0142, 0.0142]], dtype=torch.float64),\n",
       " tensor([[0.0355, 0.0355],\n",
       "         [0.0219, 0.0219],\n",
       "         [0.0480, 0.0480],\n",
       "         [0.0940, 0.0940],\n",
       "         [0.0524, 0.0051],\n",
       "         [0.0070, 0.0070],\n",
       "         [0.0384, 0.0384],\n",
       "         [0.1233, 0.1233],\n",
       "         [0.0264, 0.0264],\n",
       "         [0.1146, 0.1146],\n",
       "         [0.0362, 0.0362],\n",
       "         [0.0988, 0.0988],\n",
       "         [0.0253, 0.0253],\n",
       "         [0.1588, 0.1588],\n",
       "         [0.0264, 0.0264],\n",
       "         [0.0249, 0.0249],\n",
       "         [0.0663, 0.0663],\n",
       "         [0.1128, 0.1128],\n",
       "         [0.0883, 0.0883],\n",
       "         [0.0075, 0.0075]], dtype=torch.float64))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(training_dataset, shuffle=True, batch_size=10, num_workers=5)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=10, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm import  Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class segmentation_model(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand, dropout=0.2, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.mamba = Mamba(\n",
    "            d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand\n",
    "        ).to(device)\n",
    "\n",
    "        self.flipped_mamba = Mamba(\n",
    "            d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand\n",
    "        ).to(device)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout).to(device)\n",
    "        self.fc = nn.Linear(in_features=d_model, out_features=3).to(device)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "\n",
    "        mamba_out = self.mamba(input)\n",
    "        mamba_flipped_out = self.flipped_mamba(torch.flip(input, dims=[1]))\n",
    "        mamba_out = mamba_out + mamba_flipped_out\n",
    "\n",
    "        mamba_out = self.dropout(mamba_out)\n",
    "        out = self.fc(mamba_out)\n",
    "\n",
    "        return out  # No activation here ! It is done by the cross entropy loss\n",
    "\n",
    "\n",
    "class K_regression(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand, dropout=0.3, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.mamba = Mamba(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand,\n",
    "        ).to(device)\n",
    "        self.flipped_mamba = Mamba(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand,\n",
    "        ).to(device)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout).to(device)\n",
    "        self.fc = nn.Linear(in_features=199 * d_model, out_features=2).to(device)\n",
    "\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        mamba_out = self.mamba(input)\n",
    "        mamba_flipped_out = self.flipped_mamba(torch.flip(input, dims=[1]))\n",
    "\n",
    "        mamba_out = mamba_out + mamba_flipped_out\n",
    "\n",
    "        mamba_out = rearrange(mamba_out, \"b l c -> b (l c)\")\n",
    "\n",
    "        mamba_out = self.dropout(mamba_out)\n",
    "        out = self.fc(mamba_out)\n",
    "        out = torch.clamp(out, min=0, max=1e12)\n",
    "        out[out < 1e-7] = 0\n",
    "        return out\n",
    "\n",
    "\n",
    "class alpha_regression(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand, dropout=0.3, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.mamba = Mamba(\n",
    "            d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand\n",
    "        ).to(device)\n",
    "        self.flipped_mamba = Mamba(\n",
    "            d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand\n",
    "        ).to(device)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout).to(device)\n",
    "        self.fc = nn.Linear(in_features=199 * d_model, out_features=2).to(device)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # self.relu = nn.ReLU()\n",
    "    def forward(self, input):\n",
    "\n",
    "        mamba_out = self.mamba(input)\n",
    "        mamba_flipped_out = self.flipped_mamba(torch.flip(input, dims=[1]))\n",
    "\n",
    "        mamba_out = mamba_out + mamba_flipped_out\n",
    "\n",
    "        mamba_out = rearrange(mamba_out, \"b l c -> b (l c)\")\n",
    "\n",
    "        mamba_out = self.dropout(mamba_out)\n",
    "        out = self.fc(mamba_out)\n",
    "\n",
    "        \n",
    "        return self.sigmoid(out) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class all_at_the_same_time(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand, dropout=0.2, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.model_K = K_regression(\n",
    "            d_model, d_state=d_state, d_conv=d_conv, expand=expand, dropout=dropout\n",
    "        )\n",
    "        self.model_alpha = alpha_regression(\n",
    "            d_model, d_state=d_state, d_conv=d_conv, expand=expand, dropout=dropout\n",
    "        )\n",
    "        self.segmentation = segmentation_model(\n",
    "            d_model, d_state=d_state, d_conv=d_conv, expand=expand, dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        probas = self.segmentation(x)\n",
    "\n",
    "        classes = torch.argmax(torch.softmax(probas, dim=2), dim=2)\n",
    "\n",
    "        classes[x[:, :, 0] == 0] = 0\n",
    "\n",
    "        classes[x[:, :, 0] == 0] = 0\n",
    "        classes = classes.unsqueeze(-1)  # adding a dimension for the next step\n",
    "        concat_entry = torch.cat((classes, x[:,:,:-1]), dim=2)\n",
    "\n",
    "        alpha = self.model_alpha(concat_entry)\n",
    "\n",
    "        K = self.model_K(concat_entry)\n",
    "\n",
    "        return probas, alpha, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = all_at_the_same_time(d_model=6, d_state=4, d_conv=4, expand=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "alpha_criterion = torch.nn.L1Loss().to(\"cuda\")\n",
    "\n",
    "\n",
    "class MSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, actual):\n",
    "        return self.mse(torch.log(pred + 1), torch.log(actual + 1))\n",
    "\n",
    "\n",
    "K_criterion = MSLELoss().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_loss(model, test_dataloader, alpha_criterion, K_criterion):\n",
    "    model.eval()\n",
    "    test_classification_loss = []\n",
    "    test_alpha_loss = []\n",
    "    test_K_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, classification_targets, K_targets, alpha_targets in test_dataloader:\n",
    "            inputs = inputs.to(\"cuda\", dtype=torch.float32)  \n",
    "            inputs = torch.flatten(inputs, start_dim=0, end_dim=1)\n",
    "\n",
    "            classification_targets = (\n",
    "                torch.flatten(\n",
    "                    classification_targets,\n",
    "                    start_dim=0,\n",
    "                    end_dim=1,\n",
    "                )\n",
    "               .type(torch.LongTensor)\n",
    "               .to(\"cuda\")\n",
    "            )\n",
    "\n",
    "            classification_output, alpha_output, K_output= model(inputs)\n",
    "\n",
    "            classification_output = torch.squeeze(classification_output)\n",
    "\n",
    "            counts = torch.unique(classification_targets, return_counts=True)[1][1:]\n",
    "            weights = torch.sum(counts) / (2 * counts)\n",
    "            weights = weights.to(\"cpu\", dtype=torch.float32)  \n",
    "            weight = torch.zeros(3, dtype=torch.float32)  \n",
    "            weight[1:] = weights\n",
    "\n",
    "            classification_criterion = nn.CrossEntropyLoss(\n",
    "                weight=weight, ignore_index=0\n",
    "            )\n",
    "\n",
    "            classification_loss = classification_criterion(\n",
    "                classification_output.view(-1, 3).to(\n",
    "                    \"cpu\", dtype=torch.float32\n",
    "                ),  \n",
    "                classification_targets.view(-1).to(\"cpu\"),\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            alpha_targets = torch.flatten(alpha_targets, start_dim=0, end_dim=1).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  # Ensuring float32 type\n",
    "            alpha_loss = alpha_criterion(alpha_output, alpha_targets).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  # Ensuring float32 type\n",
    "\n",
    "            K_targets = torch.flatten(K_targets, start_dim=0, end_dim=1).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  # Ensuring float32 type\n",
    "            K_loss = K_criterion(K_output, K_targets).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  # Ensuring float32 type\n",
    "\n",
    "            test_classification_loss.append(classification_loss.item())\n",
    "            test_alpha_loss.append(alpha_loss.item())\n",
    "            test_K_loss.append(K_loss.item())\n",
    "\n",
    "    return np.mean(test_classification_loss), np.mean(test_alpha_loss), np.mean(test_K_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 793/793 [02:49<00:00,  4.67batch/s, loss_K=0.0287, loss_a=0.208, loss_c=0.733] \n",
      "Epoch 1:  27%|██▋       | 215/793 [00:46<01:32,  6.27batch/s, loss_K=0.0365, loss_a=0.324, loss_c=0.72]  "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_epoch = 50\n",
    "total_classification_loss = []\n",
    "total_K_loss = []\n",
    "total_alpha_loss = []\n",
    "test_classification_loss = []\n",
    "test_K_loss = []\n",
    "test_alpha_loss = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    running_classification_loss = []\n",
    "    running_alpha_loss = []\n",
    "    running_K_loss = []\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "        model.train()\n",
    "\n",
    "        for inputs, classification_targets, K_targets, alpha_targets in tepoch:\n",
    "\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            inputs = inputs.to(\"cuda\", dtype=torch.float32)  # Ensuring float32 type\n",
    "            inputs = torch.flatten(inputs, start_dim=0, end_dim=1)\n",
    "\n",
    "            classification_targets = (\n",
    "                torch.flatten(\n",
    "                    classification_targets,\n",
    "                    start_dim=0,\n",
    "                    end_dim=1,\n",
    "                )\n",
    "                .type(torch.LongTensor)\n",
    "                .to(\"cuda\")\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            classification_output, alpha_output, K_output = model(inputs)\n",
    "            classification_output = torch.squeeze(classification_output)\n",
    "\n",
    "            ## Computation of the weight of the classes\n",
    "\n",
    "            counts = torch.unique(classification_targets, return_counts=True)[1][1:]\n",
    "            weights = torch.sum(counts) / (2 * counts)\n",
    "            weights = weights.to(\"cpu\", dtype=torch.float32)  # Ensuring float32 type\n",
    "            weight = torch.zeros(3, dtype=torch.float32)  # Ensuring float32 type\n",
    "            weight[1:] = weights\n",
    "            ###\n",
    "\n",
    "            classification_criterion = nn.CrossEntropyLoss(\n",
    "                weight=weight, ignore_index=0\n",
    "            )\n",
    "\n",
    "            classification_loss = classification_criterion(\n",
    "                classification_output.view(-1, 3).to(\n",
    "                    \"cpu\", dtype=torch.float32\n",
    "                ),  # Ensuring float32 type\n",
    "                classification_targets.view(-1).to(\"cpu\"),\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            alpha_targets = torch.flatten(alpha_targets, start_dim=0, end_dim=1).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  # Ensuring float32 type\n",
    "            alpha_loss = alpha_criterion(alpha_output, alpha_targets).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  # Ensuring float32 type\n",
    "\n",
    "            K_targets = torch.flatten(K_targets, start_dim=0, end_dim=1).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  # Ensuring float32 type\n",
    "            K_loss = K_criterion(K_output, K_targets).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  # Ensuring float32 type\n",
    "\n",
    "            total_loss = alpha_loss + K_loss + classification_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tepoch.set_postfix(\n",
    "                loss_c=classification_loss.item(),\n",
    "                loss_a=alpha_loss.item(),\n",
    "                loss_K=K_loss.item(),\n",
    "            )\n",
    "\n",
    "            running_classification_loss.append(classification_loss.item())\n",
    "            running_alpha_loss.append(\n",
    "                alpha_loss.item()\n",
    "            )  # Corrected to alpha_loss.item()\n",
    "            running_K_loss.append(K_loss.item())  # Corrected to K_loss.item()\n",
    "\n",
    "        runnin_test_class_loss, runnin_test_alpha_loss, runnin_test_K_loss = compute_test_loss(model, test_dataloader, alpha_criterion, K_criterion)\n",
    "        total_classification_loss.append(np.mean(running_classification_loss))\n",
    "        total_alpha_loss.append(np.mean(running_alpha_loss))\n",
    "        total_K_loss.append(np.mean(running_K_loss))\n",
    "\n",
    "\n",
    "        test_classification_loss.append(runnin_test_class_loss)\n",
    "        test_K_loss.append(runnin_test_alpha_loss)\n",
    "        test_alpha_loss.append(runnin_test_K_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3779325485229492,\n",
       " 1.026025584936142,\n",
       " 0.9299487566947937,\n",
       " 0.8623801159858704,\n",
       " 0.8259282028675079,\n",
       " 0.7968046057224274,\n",
       " 0.7707876229286194,\n",
       " 0.7510102748870849,\n",
       " 0.7388714849948883,\n",
       " 0.7302161526679992]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_classification_lossdata = {}\n",
    "data[\"total_alpha_loss\"] = total_alpha_loss\n",
    "data[\"total_classification_loss\"] = total_classification_loss\n",
    "data[\"total_K_loss\"] = total_K_loss\n",
    "\n",
    "data[\"test_alpha_loss\"] = test_alpha_loss\n",
    "data[\"test_classification_loss\"] = test_classification_loss\n",
    "data[\"test_K_loss\"] = test_K_loss\n",
    "import pickle\n",
    "with open(\"loss_2_xy_features_new_bimamba.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.449656223654747,\n",
       " 0.31982260704040527,\n",
       " 0.3008570569753647,\n",
       " 0.2918631410598755,\n",
       " 0.28590414583683016,\n",
       " 0.2801697298884392,\n",
       " 0.27513948678970335,\n",
       " 0.2706154495477676,\n",
       " 0.2675237622857094,\n",
       " 0.2672152155637741]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_alpha_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.40755308389663697,\n",
       " 0.21188426211476327,\n",
       " 0.15605537578463555,\n",
       " 0.12599711053073406,\n",
       " 0.1182988779246807,\n",
       " 0.09666158117353917,\n",
       " 0.08997823104262352,\n",
       " 0.08180542565882205,\n",
       " 0.07598686948418618,\n",
       " 0.0709807500243187]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_K_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"more_features_10epoch_5kfiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
