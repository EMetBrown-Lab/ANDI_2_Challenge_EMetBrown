{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Function to pad an array to a specific shape\n",
    "def to_shape(a, shape):\n",
    "    # Unpack the target shape\n",
    "    y_, x_ = shape\n",
    "\n",
    "    # Get the current shape of the array\n",
    "    y, x = a.shape\n",
    "\n",
    "    # Calculate the padding needed in the y and x directions\n",
    "    y_pad = y_ - y\n",
    "    x_pad = x_ - x\n",
    "    output = np.zeros()\n",
    "    # Pad the array using numpy's pad function\n",
    "    return np.pad(\n",
    "        a,\n",
    "        [(0, 1), (0, 1)],\n",
    "        # Calculate the padding for each dimension\n",
    "        # ((y_pad // 2, y_pad // 2 + y_pad % 2), (x_pad // 2, x_pad // 2 + x_pad % 2)),\n",
    "        mode=\"constant\",\n",
    "    )\n",
    "\n",
    "def angle_between(p1, p2):\n",
    "    ang1 = np.arctan2(*p1[::-1])\n",
    "    ang2 = np.arctan2(*p2[::-1])\n",
    "    return (ang1 - ang2) % (2 * np.pi)\n",
    "\n",
    "# Function to pad data and labels to a specific shape\n",
    "def apply_padding(data_df, N, T_max):\n",
    "    # Define the final shape of the data and labels\n",
    "    final_shape = (N, T_max-1, 6)\n",
    "\n",
    "    # Initialize the final data and labels with zeros\n",
    "    final_data = np.zeros(final_shape)\n",
    "    final_label = np.zeros((N, T_max-1, 3))\n",
    "\n",
    "    # Select a random subset of trajectory indices\n",
    "    if len(data_df[\"traj_idx\"].unique()) < N:\n",
    "        selected_ids = np.random.choice(\n",
    "            data_df[\"traj_idx\"].unique(), size=N, replace=True\n",
    "        )\n",
    "    else:\n",
    "        selected_ids = np.random.choice(\n",
    "            data_df[\"traj_idx\"].unique(), size=N, replace=False\n",
    "        )\n",
    "\n",
    "    # Iterate over the selected trajectory indices\n",
    "    for n, id in enumerate(selected_ids):\n",
    "        # Filter the data for the current trajectory index\n",
    "        exp = data_df[data_df[\"traj_idx\"] == id]\n",
    "\n",
    "        # Extract the data and labels for the current trajectory\n",
    "        data = exp[[\"frame\", \"x\", \"y\"]].to_numpy()  \n",
    "        data[:, 0] = data[:, 0] - data[0, 0] + 1  # putting first frame rate to 1\n",
    "        data[:, 1] = data[:, 1] - data[0, 1]  # putting initial position to 0\n",
    "        data[:, 2] = (\n",
    "            data[:, 2] - data[0, 2]\n",
    "        )  # putting initital position to 0        # print(exp[\"frame\"])\n",
    "        # Displacement\n",
    "        Dx = data[1:,1] - data[:-1,1]\n",
    "        Dy = data[1:,2] - data[:-1,2]\n",
    "        MDx = np.zeros(len(Dx))\n",
    "        MDy = np.zeros(len(Dx))\n",
    "        angles = np.zeros(len(Dx))\n",
    "        distance_displacement = np.sqrt(np.power(Dx,2) + np.power(Dy,2))\n",
    "        #Displacement average\n",
    "\n",
    "        for i in range(1, len(Dx)+1):\n",
    "            MDx[i-1] = np.mean(data[i:,1] - data[:-i,1])\n",
    "            MDy[i-1] = np.mean(data[i:,2] - data[:-i,2])\n",
    "\n",
    "            A = (data[(i-1),1], data[(i-1),2])\n",
    "            B = (data[i,1], data[i,2])\n",
    "        \n",
    "            # Computation of angles\n",
    "\n",
    "            angles[i-1] = angle_between(A,B)\n",
    "\n",
    "\n",
    "\n",
    "        label = exp[[\"alpha\", \"D\", \"state\"]].to_numpy()\n",
    "        ## adding one to the states\n",
    "        label[:, 2] = label[:, 2] + 1\n",
    "        # If the data is longer than T_max, truncate it\n",
    "        if data.shape[0] > T_max:\n",
    "            # final_data[n, :, :] = data[:T_max, :]\n",
    "            final_data[n,:,0] = Dx[:(T_max-1)]\n",
    "            final_data[n,:,1] = Dy[:(T_max-1)]\n",
    "            final_data[n,:,2] = MDx[:(T_max-1)]\n",
    "            final_data[n,:,3] = MDy[:(T_max-1)]\n",
    "            final_data[n,:,4] = distance_displacement[:(T_max-1)]\n",
    "            final_data[n,:,5] = angles[:(T_max-1)]\n",
    "\n",
    "            final_label[n, :, :] = label[:T_max-1, :]\n",
    "\n",
    "        # Otherwise, pad the data to T_max\n",
    "        else:\n",
    "            # print((label.shape, T_max))\n",
    "            final_data[n, : (data.shape[0] -1), 0] = Dx\n",
    "            final_data[n, : (data.shape[0] -1), 1] = Dy\n",
    "            final_data[n, : (data.shape[0] -1), 2] = MDx\n",
    "            final_data[n, : (data.shape[0] -1), 3] = MDy\n",
    "            final_data[n, : (data.shape[0] -1), 4] = distance_displacement\n",
    "            final_data[n, : (data.shape[0] -1), 5] = angles\n",
    "\n",
    "            final_label[n, : data.shape[0] -1, :] = label[:-1, :]\n",
    "\n",
    "    # Return the padded data and label\n",
    "    return final_data, final_label\n",
    "\n",
    "\n",
    "# Define a function to normalize data\n",
    "def normalize_df(data):\n",
    "    # Calculate displacement in x and y directions\n",
    "    # Normalize by substring mean and dividing by variance.\n",
    "\n",
    "    displacement_x = []\n",
    "    displacement_y = []\n",
    "    for _, group in data.groupby(\"traj_idx\"):\n",
    "        x = np.asarray(group[\"x\"])\n",
    "        y = np.asarray(group[\"y\"])\n",
    "        d_x = x[1:] - x[:-1]\n",
    "        d_y = y[1:] - y[:-1]\n",
    "        displacement_x = displacement_x + list(d_x)\n",
    "        displacement_y = displacement_y + list(d_y)\n",
    "\n",
    "    # Calculate variance in x and y directions\n",
    "    variance_x = np.sqrt(np.std(displacement_x))\n",
    "    variance_y = np.sqrt(np.std(displacement_y))\n",
    "\n",
    "    # Normalize data\n",
    "    data.loc[:, \"x\"] = (data[\"x\"] - data[\"x\"].mean()) / variance_x\n",
    "    data.loc[:, \"y\"] = (data[\"y\"] - data[\"y\"].mean()) / variance_y\n",
    "\n",
    "\n",
    "def normalize_np(data):\n",
    "\n",
    "    displacement_x = []\n",
    "    displacement_y = []\n",
    "    for n in range(data.shape[0]):\n",
    "        x = data[n, :, 1]\n",
    "        y = data[n, :, 2]\n",
    "        d_x = x[1:] - x[:-1]\n",
    "        d_y = y[1:] - y[:-1]\n",
    "        displacement_x = displacement_x + list(d_x)\n",
    "        displacement_y = displacement_y + list(d_y)\n",
    "\n",
    "    # Calculate variance in x and y directions\n",
    "    variance_x = np.sqrt(np.std(displacement_x))\n",
    "    variance_y = np.sqrt(np.std(displacement_y))\n",
    "\n",
    "    # Normalize data\n",
    "\n",
    "    data[:, :, 1] = (data[:, :, 1] - np.mean(data[:, :, 1])) / variance_x\n",
    "    data[:, :, 2] = (data[:, :, 2] - np.mean(data[:, :, 2])) / variance_x\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Define a function to list directory tree with pathlib\n",
    "def list_directory_tree_with_pathlib(starting_directory):\n",
    "    path_object = Path(starting_directory)\n",
    "    folders = []\n",
    "    for file_path in path_object.rglob(\"*.csv\"):\n",
    "        folders.append(file_path)\n",
    "    return folders\n",
    "\n",
    "\n",
    "# Define a custom dataset class for all data\n",
    "@dataclass\n",
    "class Dataset_all_data(Dataset):\n",
    "    # Initialize filenames and transform flag\n",
    "    # Pad value should be a tuple such as (N, Tmax)\n",
    "    filenames: list\n",
    "    transform: bool = False\n",
    "    pad: None | tuple = None\n",
    "    noise: bool = False\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of files\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read csv file and extract data and label\n",
    "        df = pd.read_csv(self.filenames[idx])\n",
    "\n",
    "        if self.pad is None:\n",
    "            data = df[[\"traj_idx\", \"frame\", \"x\", \"y\"]]\n",
    "            label = np.asarray(df[[\"alpha\", \"D\"]])\n",
    "            label_2 = np.asarray(df[\"state\"])\n",
    "\n",
    "        else:\n",
    "            if len(self.pad) != 2:\n",
    "                raise ValueError(\"pad value should be set as (N, T_max)\")\n",
    "            data, label = apply_padding(df, *self.pad)\n",
    "            data = data[:, :, :]  ## Removing the frame column\n",
    "            label_2 = label[:, :, -1]\n",
    "            label_2[label_2[:, :] > 0] = label_2[label_2[:, :] > 0]\n",
    "            label = label[:, :, :-1]\n",
    "\n",
    "        # Normalize data if transform flag is True\n",
    "        if self.transform:\n",
    "            if self.pad is None:\n",
    "                normalize_df(data)\n",
    "                data = np.asarray(data)\n",
    "            else:\n",
    "                data = normalize_np(data)\n",
    "\n",
    "        if self.noise:\n",
    "            data = add_noise(data)\n",
    "\n",
    "        # Normalize D between 0 and 1\n",
    "\n",
    "        # label[:,:,1][label[:,:,1] != 0] = np.log(label[:,:,1][label[:,:,1] != 0]) #- np.log(1e-6)) #/   (np.log(1e12) - np.log(1e-6))\n",
    "        # label = label[:,:,1]\n",
    "        label_K = np.zeros((label.shape[0], 2))\n",
    "\n",
    "        # print(np.unique(label_2))\n",
    "\n",
    "        for i in range(label.shape[0]):\n",
    "            K = np.unique(label[i, :, 1][label[i, :, 1] != 0])\n",
    "            if len(K) == 2:\n",
    "                label_K[i, :] = K\n",
    "\n",
    "                if label[i, 0, 1] != label_K[i, 0]:\n",
    "                    label_K[i, :] = label_K[i, ::-1]\n",
    "\n",
    "            elif len(K) == 1:\n",
    "                states = label_2[i, :]\n",
    "                if 1 in states:\n",
    "                    # print(np.unique(states))\n",
    "                    if states[0] == 1:\n",
    "                        label_K[i, :] = [0, K[0]]\n",
    "                    else:\n",
    "                        label_K[i, :] = [K[0], 0]\n",
    "\n",
    "                    # print(label_regression[i,:])\n",
    "\n",
    "                else:\n",
    "                    label_K[i, :] = [K[0], K[0]]\n",
    "\n",
    "            else:\n",
    "                if np.unique(label[i, :, 1]) == 0:\n",
    "                    label_K[i, :] = [0, 0]\n",
    "                else:\n",
    "\n",
    "                    # print(np.unique(label[i,:,1]))\n",
    "\n",
    "                    # print(Ds)\n",
    "                    raise Exception(\"more than 2 diffusions\")\n",
    "\n",
    "        # print(np.unique(label_2))\n",
    "        label_alpha = np.zeros((label.shape[0], 2))\n",
    "\n",
    "        for i in range(label.shape[0]):\n",
    "            alpha = np.unique(label[i, :, 0][label[i, :, 0] != 0])\n",
    "            if len(alpha) == 2:\n",
    "                label_alpha[i, :] = alpha\n",
    "                if label[i, 0, 0] != label_alpha[i, 0]:\n",
    "                    label_alpha[i, :] = label_alpha[i, ::-1]\n",
    "\n",
    "            elif len(alpha) == 1:\n",
    "                states = label_2[i, :]\n",
    "                if 1 in states:\n",
    "                    # print(np.unique(states))\n",
    "                    if states[0] == 1:\n",
    "                        label_alpha[i, :] = [0, alpha[0]]\n",
    "                    else:\n",
    "                        label_alpha[i, :] = [alpha[0], 0]\n",
    "\n",
    "                    # print(label_regression[i,:])\n",
    "\n",
    "                else:\n",
    "                    label_alpha[i, :] = [alpha[0], alpha[0]]\n",
    "\n",
    "            else:\n",
    "                if np.unique(label[i, :, 1]) == 0:\n",
    "                    label_alpha[i, :] = [0, 0]\n",
    "                else:\n",
    "\n",
    "                    # print(np.unique(label[i,:,1]))\n",
    "\n",
    "                    # print(Ds)\n",
    "                    raise Exception(\"more than 2 diffusions\")\n",
    "\n",
    "        label_segmentation = np.zeros((label_2.shape[0], label_2.shape[1]))\n",
    "\n",
    "        for i in range(label.shape[0]):\n",
    "            if label_K[i, 0] == label_K[i, 1]:\n",
    "                position = label[i, :, 1] == label_K[i, 0]\n",
    "                label_segmentation[i, position] = 1\n",
    "            else:\n",
    "\n",
    "                position_1 = label[i, :, 1] == label_K[i, 0]\n",
    "                position_2 = label[i, :, 1] == label_K[i, 1]\n",
    "\n",
    "                label_segmentation[i, position_1] = 1\n",
    "                label_segmentation[i, position_2] = 2\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(data.astype(np.float32)),\n",
    "            torch.from_numpy(label_segmentation.astype(np.float32)),\n",
    "            torch.from_numpy(label_K),\n",
    "            torch.from_numpy(label_alpha),\n",
    "        )\n",
    "        # torch.from_numpy(label_2.astype(np.float32)),\n",
    "\n",
    "\n",
    "def add_noise(data):\n",
    "    noise_amplitude = np.random.choice(\n",
    "        [\n",
    "            0.01,\n",
    "            0.1,\n",
    "        ]\n",
    "    )\n",
    "    noise = np.random.normal(0, noise_amplitude, data[:, :, :].shape)\n",
    "    data[:, :, :][data[:, :, 1:] != 0] = (\n",
    "        data[:, :, :][data[:, :, 1:] != 0] + data[:, :, :][data[:, :, 1:] != 0] * noise\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_set = list_directory_tree_with_pathlib(\n",
    "    r\"/home/m.lavaud/ANDI_2_Challenge_EMetBrown/data/datasets\",\n",
    ")\n",
    "np.random.shuffle(all_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149609"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = Dataset_all_data(all_data_set[:5000], transform=False, pad=(20, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = iter(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 3.9383e-01, -2.0168e-01,  7.5242e-04, -1.2836e-03,  4.4247e-01,\n",
       "            4.7328e-01],\n",
       "          [ 1.0254e-01, -1.3914e-01,  1.3307e-03, -1.6087e-03,  1.7284e-01,\n",
       "            1.2841e-01],\n",
       "          [-1.4367e-01, -6.0227e-02,  1.5436e-03, -1.4207e-03,  1.5579e-01,\n",
       "            2.4776e-01],\n",
       "          ...,\n",
       "          [-4.5819e-01,  4.6314e-02,  2.9206e-02, -1.4707e-04,  4.6053e-01,\n",
       "            8.3580e-01],\n",
       "          [ 6.7617e-01,  2.6268e-01,  3.1799e-02, -1.4323e-03,  7.2540e-01,\n",
       "            4.8504e+00],\n",
       "          [-3.5616e-01,  1.8629e-01,  3.1283e-02, -3.0600e-03,  4.0194e-01,\n",
       "            5.9147e+00]],\n",
       " \n",
       "         [[-4.4894e-01, -1.3851e-01, -5.1563e-03,  4.1681e-03,  4.6982e-01,\n",
       "            2.8423e+00],\n",
       "          [-1.0245e-01,  1.3332e+00, -1.0002e-02,  8.1562e-03,  1.3371e+00,\n",
       "            1.4376e+00],\n",
       "          [ 2.3169e-01, -5.2059e-01, -1.4714e-02,  8.4910e-03,  5.6982e-01,\n",
       "            6.2727e+00],\n",
       "          ...,\n",
       "          [-7.1183e-01, -3.3851e-01, -9.0045e-01, -9.4858e-02,  7.8822e-01,\n",
       "            6.1851e+00],\n",
       "          [ 1.8129e+00, -3.6296e-01, -8.9877e-01, -9.1726e-02,  1.8489e+00,\n",
       "            6.0019e+00],\n",
       "          [-6.0102e-01,  6.4520e-01, -9.0285e-01, -8.5627e-02,  8.8177e-01,\n",
       "            3.5811e-01]],\n",
       " \n",
       "         [[-2.4257e-01,  2.2471e-01,  1.1045e-02, -1.7280e-02,  3.3066e-01,\n",
       "            3.8888e+00],\n",
       "          [ 1.8492e-01, -5.3172e-01,  2.2445e-02, -3.4884e-02,  5.6296e-01,\n",
       "            4.1508e+00],\n",
       "          [ 4.5830e-01,  5.1824e-01,  3.5346e-02, -5.0837e-02,  6.9182e-01,\n",
       "            4.0416e+00],\n",
       "          ...,\n",
       "          [-2.2810e-01,  6.3697e-01,  2.1982e+00, -2.6285e+00,  6.7658e-01,\n",
       "            6.2025e+00],\n",
       "          [ 3.5494e-01, -5.3620e-01,  2.2124e+00, -2.6387e+00,  6.4304e-01,\n",
       "            5.5808e-02],\n",
       "          [ 2.7281e-01, -4.2217e-01,  2.2261e+00, -2.6463e+00,  5.0264e-01,\n",
       "            3.7631e-02]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1.7628e-01,  2.7472e-03,  6.2364e-04, -2.0382e-04,  1.7630e-01,\n",
       "            6.2676e+00],\n",
       "          [ 1.0795e-01,  4.7925e-01,  9.6307e-04, -3.9057e-04,  4.9126e-01,\n",
       "            5.2608e+00],\n",
       "          [-3.8457e-01, -5.1146e-01,  1.0359e-03, -9.4444e-04,  6.3991e-01,\n",
       "            3.8939e+00],\n",
       "          ...,\n",
       "          [ 3.3282e-01,  1.4792e-02,  4.3520e-02, -5.7625e-02,  3.3315e-01,\n",
       "            8.3022e-01],\n",
       "          [-2.0789e-01,  2.0528e-01,  4.3843e-02, -5.8723e-02,  2.9216e-01,\n",
       "            6.1025e+00],\n",
       "          [ 2.4753e-01, -2.7075e-01,  4.4903e-02, -5.9762e-02,  3.6685e-01,\n",
       "            1.7978e-01]],\n",
       " \n",
       "         [[ 4.8485e-01,  3.5489e-01,  8.2971e-03,  2.1113e-03,  6.0085e-01,\n",
       "            5.6513e+00],\n",
       "          [-1.1282e-01, -4.1465e-01,  1.5609e-02,  1.5170e-03,  4.2972e-01,\n",
       "            7.9112e-01],\n",
       "          [-3.2885e-01,  8.9216e-02,  2.3669e-02,  1.3735e-03,  3.4074e-01,\n",
       "            5.5253e+00],\n",
       "          ...,\n",
       "          [-8.3851e-01,  1.2234e-01,  1.4239e+00,  7.1705e-02,  8.4739e-01,\n",
       "            1.9491e-01],\n",
       "          [ 2.3248e-01, -3.3265e-01,  1.4343e+00,  7.4853e-02,  4.0584e-01,\n",
       "            6.2826e+00],\n",
       "          [-2.1270e-01,  4.5219e-01,  1.4431e+00,  7.9357e-02,  4.9972e-01,\n",
       "            6.2496e+00]],\n",
       " \n",
       "         [[ 4.0897e-01,  2.7449e-01, -3.0154e-04,  1.3143e-03,  4.9255e-01,\n",
       "            5.6921e+00],\n",
       "          [-7.8809e-01, -3.1458e-01, -1.0015e-03,  2.2153e-03,  8.4856e-01,\n",
       "            3.6273e+00],\n",
       "          [ 3.6668e-01, -4.4050e-01,  2.0398e-04,  3.7994e-03,  5.7314e-01,\n",
       "            4.8436e+00],\n",
       "          ...,\n",
       "          [ 3.0442e-02,  7.3071e-01,  1.2891e-01,  2.7482e-01,  7.3134e-01,\n",
       "            8.9754e-01],\n",
       "          [-2.3692e-02, -1.8493e-01,  1.3034e-01,  2.7266e-01,  1.8644e-01,\n",
       "            6.1634e+00],\n",
       "          [ 2.5806e-01, -7.0422e-01,  1.3124e-01,  2.7164e-01,  7.5002e-01,\n",
       "            4.7367e+00]]]),\n",
       " tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]),\n",
       " tensor([[0.0378, 0.0378],\n",
       "         [0.1937, 0.1937],\n",
       "         [0.0990, 0.0990],\n",
       "         [0.0617, 0.0617],\n",
       "         [0.0947, 0.0947],\n",
       "         [0.1541, 0.1541],\n",
       "         [0.0078, 0.0078],\n",
       "         [0.0877, 0.0877],\n",
       "         [0.0548, 0.0548],\n",
       "         [0.0963, 0.0963],\n",
       "         [0.3508, 0.1896],\n",
       "         [0.0259, 0.0259],\n",
       "         [0.1474, 0.1474],\n",
       "         [0.0309, 0.0309],\n",
       "         [0.0966, 0.0966],\n",
       "         [0.0547, 0.0547],\n",
       "         [0.0396, 0.0396],\n",
       "         [0.0271, 0.0271],\n",
       "         [0.0594, 0.0594],\n",
       "         [0.0513, 0.0513]], dtype=torch.float64),\n",
       " tensor([[4.9966e-04, 4.9966e-04],\n",
       "         [4.0041e-01, 4.0041e-01],\n",
       "         [6.9172e-01, 6.9172e-01],\n",
       "         [4.8985e-01, 4.8985e-01],\n",
       "         [6.3487e-01, 6.3487e-01],\n",
       "         [3.3224e-01, 3.3224e-01],\n",
       "         [2.1116e-01, 2.1116e-01],\n",
       "         [6.5277e-01, 6.5277e-01],\n",
       "         [9.3882e-01, 9.3882e-01],\n",
       "         [4.4715e-01, 4.4715e-01],\n",
       "         [5.4815e-01, 2.8482e-01],\n",
       "         [4.2663e-01, 4.2663e-01],\n",
       "         [1.6405e-01, 1.6405e-01],\n",
       "         [6.8160e-01, 6.8160e-01],\n",
       "         [1.9318e-01, 1.9318e-01],\n",
       "         [7.7694e-01, 7.7694e-01],\n",
       "         [3.3686e-01, 3.3686e-01],\n",
       "         [4.3010e-02, 4.3010e-02],\n",
       "         [6.2796e-01, 6.2796e-01],\n",
       "         [9.9700e-02, 9.9700e-02]], dtype=torch.float64))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(training_dataset, shuffle=True, batch_size=100, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm import  Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class segmentation_model(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand, dropout=0.2, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    ":4000]a(\n",
    "            d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand\n",
    "        ).to(device)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout).to(device)\n",
    "        self.fc = nn.Linear(in_features=d_model, out_features=3).to(device)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "\n",
    "        mamba_out = self.mamba(input)\n",
    "        mamba_flipped_out = self.flipped_mamba(torch.flip(input, dims=[1]))\n",
    "        mamba_out = mamba_out + mamba_flipped_out\n",
    "\n",
    "        mamba_out = self.dropout(mamba_out)\n",
    "        out = self.fc(mamba_out)\n",
    "\n",
    "        return out  # No activation here ! It is done by the cross entropy loss\n",
    "\n",
    "\n",
    "class K_regression(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand, dropout=0.3, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.mamba = Mamba(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand,\n",
    "        ).to(device)\n",
    "        self.flipped_mamba = Mamba(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand,\n",
    "        ).to(device)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout).to(device)\n",
    "        self.fc = nn.Linear(in_features=199 * d_model, out_features=2).to(device)\n",
    "\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        mamba_out = self.mamba(input)\n",
    "        mamba_flipped_out = self.flipped_mamba(torch.flip(input, dims=[1]))\n",
    "\n",
    "        mamba_out = mamba_out + mamba_flipped_out\n",
    "\n",
    "        mamba_out = rearrange(mamba_out, \"b l c -> b (l c)\")\n",
    "\n",
    "        mamba_out = self.dropout(mamba_out)\n",
    "        out = self.fc(mamba_out)\n",
    "        out = torch.clamp(out, min=0, max=1e12)\n",
    "        out[out < 1e-7] = 0\n",
    "        return out\n",
    "\n",
    "\n",
    "class alpha_regression(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand, dropout=0.3, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.mamba = Mamba(\n",
    "            d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand\n",
    "        ).to(device)\n",
    "        self.flipped_mamba = Mamba(\n",
    "            d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand\n",
    "        ).to(device)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout).to(device)\n",
    "        self.fc = nn.Linear(in_features=199 * d_model, out_features=2).to(device)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # self.relu = nn.ReLU()\n",
    "    def forward(self, input):\n",
    "\n",
    "        mamba_out = self.mamba(input)\n",
    "        mamba_flipped_out = self.flipped_mamba(torch.flip(input, dims=[1]))\n",
    "\n",
    "        mamba_out = mamba_out + mamba_flipped_out\n",
    "\n",
    "        mamba_out = rearrange(mamba_out, \"b l c -> b (l c)\")\n",
    "\n",
    "        mamba_out = self.dropout(mamba_out)\n",
    "        out = self.fc(mamba_out)\n",
    "\n",
    "        \n",
    "        return self.sigmoid(out) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class all_at_the_same_time(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand, dropout=0.2, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.model_K = K_regression(\n",
    "            d_model, d_state=d_state, d_conv=d_conv, expand=expand, dropout=dropout\n",
    "        )\n",
    "        self.model_alpha = alpha_regression(\n",
    "            d_model, d_state=d_state, d_conv=d_conv, expand=expand, dropout=dropout\n",
    "        )\n",
    "        self.segmentation = segmentation_model(\n",
    "            d_model, d_state=d_state, d_conv=d_conv, expand=expand, dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        probas = self.segmentation(x)\n",
    "\n",
    "        classes = torch.argmax(torch.softmax(probas, dim=2), dim=2)\n",
    "\n",
    "        classes[x[:, :, 0] == 0] = 0\n",
    "\n",
    "        classes[x[:, :, 0] == 0] = 0\n",
    "        classes = classes.unsqueeze(-1)  # adding a dimension for the next step\n",
    "        concat_entry = torch.cat((classes, x[:,:,:-1]), dim=2)\n",
    "\n",
    "        alpha = self.model_alpha(concat_entry)\n",
    "\n",
    "        K = self.model_K(concat_entry)\n",
    "\n",
    "        return probas, alpha, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = all_at_the_same_time(d_model=6, d_state=4, d_conv=4, expand=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "alpha_criterion = torch.nn.L1Loss().to(\"cuda\")\n",
    "\n",
    "\n",
    "class MSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, actual):\n",
    "        return self.mse(torch.log(pred + 1), torch.log(actual + 1))\n",
    "\n",
    "\n",
    "K_criterion = MSLELoss().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  48%|████▊     | 24/50 [01:21<01:31,  3.54s/batch, loss_K=0.345, loss_a=0.447, loss_c=1.82]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_epoch = 10\n",
    "total_classification_loss = []\n",
    "total_K_loss = []\n",
    "total_alpha_loss = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    running_classification_loss = []\n",
    "    running_alpha_loss = []\n",
    "    running_K_loss = []\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "        model.train()\n",
    "\n",
    "        for inputs, classification_targets, K_targets, alpha_targets in tepoch:\n",
    "\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            inputs = inputs.to(\"cuda\", dtype=torch.float32)  # Ensuring float32 type\n",
    "            inputs = torch.flatten(inputs, start_dim=0, end_dim=1)\n",
    "\n",
    "            classification_targets = (\n",
    "                torch.flatten(\n",
    "                    classification_targets,\n",
    "                    start_dim=0,\n",
    "                    end_dim=1,\n",
    "                )\n",
    "                .type(torch.LongTensor)\n",
    "                .to(\"cuda\")\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            classification_output, alpha_output, K_output = model(inputs)\n",
    "            classification_output = torch.squeeze(classification_output)\n",
    "\n",
    "            ## Computation of the weight of the classes\n",
    "\n",
    "            counts = torch.unique(classification_targets, return_counts=True)[1][1:]\n",
    "            weights = torch.sum(counts) / (2 * counts)\n",
    "            weights = weights.to(\"cpu\", dtype=torch.float32)  # Ensuring float32 type\n",
    "            weight = torch.zeros(3, dtype=torch.float32)  # Ensuring float32 type\n",
    "            weight[1:] = weights\n",
    "            ###\n",
    "\n",
    "            classification_criterion = nn.CrossEntropyLoss(\n",
    "                weight=weight, ignore_index=0\n",
    "            )\n",
    "\n",
    "            classification_loss = classification_criterion(\n",
    "                classification_output.view(-1, 3).to(\n",
    "                    \"cpu\", dtype=torch.float32\n",
    "                ),  # Ensuring float32 type\n",
    "                classification_targets.view(-1).to(\"cpu\"),\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            alpha_targets = torch.flatten(alpha_targets, start_dim=0, end_dim=1).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  # Ensuring float32 type\n",
    "            alpha_loss = alpha_criterion(alpha_output, alpha_targets).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  # Ensuring float32 type\n",
    "\n",
    "            K_targets = torch.flatten(K_targets, start_dim=0, end_dim=1).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  # Ensuring float32 type\n",
    "            K_loss = K_criterion(K_output, K_targets).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  # Ensuring float32 type\n",
    "\n",
    "            total_loss = alpha_loss + K_loss + classification_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tepoch.set_postfix(\n",
    "                loss_c=classification_loss.item(),\n",
    "                loss_a=alpha_loss.item(),\n",
    "                loss_K=K_loss.item(),\n",
    "            )\n",
    "\n",
    "            running_classification_loss.append(classification_loss.item())\n",
    "            running_alpha_loss.append(\n",
    "                alpha_loss.item()\n",
    "            )  # Corrected to alpha_loss.item()\n",
    "            running_K_loss.append(K_loss.item())  # Corrected to K_loss.item()\n",
    "\n",
    "        total_classification_loss.append(np.mean(running_classification_loss))\n",
    "        total_alpha_loss.append(np.mean(running_alpha_loss))\n",
    "        total_K_loss.append(np.mean(running_K_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8223144904167237,\n",
       " 0.6807549311147981,\n",
       " 0.5944482200370284,\n",
       " 0.567254135747233,\n",
       " 0.55501983393887,\n",
       " 0.5478509053159891]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_classification_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.23307907021913674,\n",
       " 0.18144199543781256,\n",
       " 0.16547784602435014,\n",
       " 0.15760873224270208,\n",
       " 0.15217636415141378,\n",
       " 0.14726961901785934]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_alpha_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08634741686262146,\n",
       " 0.043210548692138376,\n",
       " 0.04164033198172977,\n",
       " 0.039643912645817084,\n",
       " 0.03600935581337434,\n",
       " 0.03182164137236122]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_K_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"more_features_5epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
