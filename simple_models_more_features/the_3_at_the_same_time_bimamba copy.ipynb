{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def angle_between(p1, p2):\n",
    "    ang1 = np.arctan2(*p1[::-1])\n",
    "    ang2 = np.arctan2(*p2[::-1])\n",
    "    return (ang1 - ang2) % (2 * np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Function to pad an array to a specific shape\n",
    "def to_shape(a, shape):\n",
    "    # Unpack the target shape\n",
    "    y_, x_ = shape\n",
    "\n",
    "    # Get the current shape of the array\n",
    "    y, x = a.shape\n",
    "\n",
    "    # Calculate the padding needed in the y and x directions\n",
    "    y_pad = y_ - y\n",
    "    x_pad = x_ - x\n",
    "    output = np.zeros()\n",
    "    # Pad the array using numpy's pad function\n",
    "    return np.pad(\n",
    "        a,\n",
    "        [(0, 1), (0, 1)],\n",
    "        # Calculate the padding for each dimension\n",
    "        # ((y_pad // 2, y_pad // 2 + y_pad % 2), (x_pad // 2, x_pad // 2 + x_pad % 2)),\n",
    "        mode=\"constant\",\n",
    "    )\n",
    "\n",
    "def angle_between(p1, p2):\n",
    "    ang1 = np.arctan2(*p1[::-1])\n",
    "    ang2 = np.arctan2(*p2[::-1])\n",
    "    return (ang1 - ang2) % (2 * np.pi)\n",
    "\n",
    "# Function to pad data and labels to a specific shape\n",
    "def apply_padding(data_df, N, T_max):\n",
    "    # Define the final shape of the data and labels\n",
    "    final_shape = (N, T_max-1, 6)\n",
    "\n",
    "    # Initialize the final data and labels with zeros\n",
    "    final_data = np.zeros(final_shape)\n",
    "    final_label = np.zeros((N, T_max-1, 3))\n",
    "\n",
    "    # Select a random subset of trajectory indices\n",
    "    if len(data_df[\"traj_idx\"].unique()) < N:\n",
    "        selected_ids = np.random.choice(\n",
    "            data_df[\"traj_idx\"].unique(), size=N, replace=True\n",
    "        )\n",
    "    else:\n",
    "        selected_ids = np.random.choice(\n",
    "            data_df[\"traj_idx\"].unique(), size=N, replace=False\n",
    "        )\n",
    "\n",
    "    # Iterate over the selected trajectory indices\n",
    "    for n, id in enumerate(selected_ids):\n",
    "        # Filter the data for the current trajectory index\n",
    "        exp = data_df[data_df[\"traj_idx\"] == id]\n",
    "\n",
    "        # Extract the data and labels for the current trajectory\n",
    "        data = exp[[\"frame\", \"x\", \"y\"]].to_numpy()  \n",
    "        data[:, 0] = data[:, 0] - data[0, 0] + 1  # putting first frame rate to 1\n",
    "        data[:, 1] = data[:, 1] - data[0, 1]  # putting initial position to 0\n",
    "        data[:, 2] = (\n",
    "            data[:, 2] - data[0, 2]\n",
    "        )  # putting initital position to 0        # print(exp[\"frame\"])\n",
    "        # Displacement\n",
    "        Dx = data[1:,1] - data[:-1,1]\n",
    "        Dy = data[1:,2] - data[:-1,2]\n",
    "        MDx = np.zeros(len(Dx))\n",
    "        MDy = np.zeros(len(Dx))\n",
    "        angles = np.zeros(len(Dx))\n",
    "        distance_displacement = np.sqrt(np.power(Dx,2) + np.power(Dy,2))\n",
    "        #Displacement average\n",
    "\n",
    "        for i in range(1, len(Dx)+1):\n",
    "            MDx[i-1] = np.mean(data[i:,1] - data[:-i,1])\n",
    "            MDy[i-1] = np.mean(data[i:,2] - data[:-i,2])\n",
    "\n",
    "            A = (data[(i-1),1], data[(i-1),2])\n",
    "            B = (data[i,1], data[i,2])\n",
    "        \n",
    "            # Computation of angles\n",
    "\n",
    "            angles[i-1] = angle_between(A,B)\n",
    "\n",
    "\n",
    "\n",
    "        label = exp[[\"alpha\", \"D\", \"state\"]].to_numpy()\n",
    "        ## adding one to the states\n",
    "        label[:, 2] = label[:, 2] + 1\n",
    "        # If the data is longer than T_max, truncate it\n",
    "        if data.shape[0] > T_max:\n",
    "            # final_data[n, :, :] = data[:T_max, :]\n",
    "            final_data[n,:,0] = Dx[:(T_max-1)]\n",
    "            final_data[n,:,1] = Dy[:(T_max-1)]\n",
    "            final_data[n,:,2] = MDx[:(T_max-1)]\n",
    "            final_data[n,:,3] = MDy[:(T_max-1)]\n",
    "            final_data[n,:,4] = distance_displacement[:(T_max-1)]\n",
    "            final_data[n,:,5] = angles[:(T_max-1)]\n",
    "\n",
    "            final_label[n, :, :] = label[:T_max-1, :]\n",
    "\n",
    "        # Otherwise, pad the data to T_max\n",
    "        else:\n",
    "            # print((label.shape, T_max))\n",
    "            final_data[n, : (data.shape[0] -1), 0] = Dx\n",
    "            final_data[n, : (data.shape[0] -1), 1] = Dy\n",
    "            final_data[n, : (data.shape[0] -1), 2] = MDx\n",
    "            final_data[n, : (data.shape[0] -1), 3] = MDy\n",
    "            final_data[n, : (data.shape[0] -1), 4] = distance_displacement\n",
    "            final_data[n, : (data.shape[0] -1), 5] = angles\n",
    "\n",
    "            final_label[n, : data.shape[0] -1, :] = label[:-1, :]\n",
    "\n",
    "    # Return the padded data and label\n",
    "    return final_data, final_label\n",
    "\n",
    "\n",
    "# Define a function to normalize data\n",
    "def normalize_df(data):\n",
    "    # Calculate displacement in x and y directions\n",
    "    # Normalize by substring mean and dividing by variance.\n",
    "\n",
    "    displacement_x = []\n",
    "    displacement_y = []\n",
    "    for _, group in data.groupby(\"traj_idx\"):\n",
    "        x = np.asarray(group[\"x\"])\n",
    "        y = np.asarray(group[\"y\"])\n",
    "        d_x = x[1:] - x[:-1]\n",
    "        d_y = y[1:] - y[:-1]\n",
    "        displacement_x = displacement_x + list(d_x)\n",
    "        displacement_y = displacement_y + list(d_y)\n",
    "\n",
    "    # Calculate variance in x and y directions\n",
    "    variance_x = np.sqrt(np.std(displacement_x))\n",
    "    variance_y = np.sqrt(np.std(displacement_y))\n",
    "\n",
    "    # Normalize data\n",
    "    data.loc[:, \"x\"] = (data[\"x\"] - data[\"x\"].mean()) / variance_x\n",
    "    data.loc[:, \"y\"] = (data[\"y\"] - data[\"y\"].mean()) / variance_y\n",
    "\n",
    "\n",
    "def normalize_np(data):\n",
    "\n",
    "    displacement_x = []\n",
    "    displacement_y = []\n",
    "    for n in range(data.shape[0]):\n",
    "        x = data[n, :, 1]\n",
    "        y = data[n, :, 2]\n",
    "        d_x = x[1:] - x[:-1]\n",
    "        d_y = y[1:] - y[:-1]\n",
    "        displacement_x = displacement_x + list(d_x)\n",
    "        displacement_y = displacement_y + list(d_y)\n",
    "\n",
    "    # Calculate variance in x and y directions\n",
    "    variance_x = np.sqrt(np.std(displacement_x))\n",
    "    variance_y = np.sqrt(np.std(displacement_y))\n",
    "\n",
    "    # Normalize data\n",
    "\n",
    "    data[:, :, 1] = (data[:, :, 1] - np.mean(data[:, :, 1])) / variance_x\n",
    "    data[:, :, 2] = (data[:, :, 2] - np.mean(data[:, :, 2])) / variance_x\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Define a function to list directory tree with pathlib\n",
    "def list_directory_tree_with_pathlib(starting_directory):\n",
    "    path_object = Path(starting_directory)\n",
    "    folders = []\n",
    "    for file_path in path_object.rglob(\"*.csv\"):\n",
    "        folders.append(file_path)\n",
    "    return folders\n",
    "\n",
    "\n",
    "# Define a custom dataset class for all data\n",
    "@dataclass\n",
    "class Dataset_all_data(Dataset):\n",
    "    # Initialize filenames and transform flag\n",
    "    # Pad value should be a tuple such as (N, Tmax)\n",
    "    filenames: list\n",
    "    transform: bool = False\n",
    "    pad: None | tuple = None\n",
    "    noise: bool = False\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of files\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read csv file and extract data and label\n",
    "        df = pd.read_csv(self.filenames[idx])\n",
    "\n",
    "        if self.pad is None:\n",
    "            data = df[[\"traj_idx\", \"frame\", \"x\", \"y\"]]\n",
    "            label = np.asarray(df[[\"alpha\", \"D\"]])\n",
    "            label_2 = np.asarray(df[\"state\"])\n",
    "\n",
    "        else:\n",
    "            if len(self.pad) != 2:\n",
    "                raise ValueError(\"pad value should be set as (N, T_max)\")\n",
    "            data, label = apply_padding(df, *self.pad)\n",
    "            data = data[:, :, :]  ## Removing the frame column\n",
    "            label_2 = label[:, :, -1]\n",
    "            label_2[label_2[:, :] > 0] = label_2[label_2[:, :] > 0]\n",
    "            label = label[:, :, :-1]\n",
    "\n",
    "        # Normalize data if transform flag is True\n",
    "        if self.transform:\n",
    "            if self.pad is None:\n",
    "                normalize_df(data)\n",
    "                data = np.asarray(data)\n",
    "            else:\n",
    "                data = normalize_np(data)\n",
    "\n",
    "        if self.noise:\n",
    "            data = add_noise(data)\n",
    "\n",
    "        # Normalize D between 0 and 1\n",
    "\n",
    "        # label[:,:,1][label[:,:,1] != 0] = np.log(label[:,:,1][label[:,:,1] != 0]) #- np.log(1e-6)) #/   (np.log(1e12) - np.log(1e-6))\n",
    "        # label = label[:,:,1]\n",
    "        label_K = np.zeros((label.shape[0], 2))\n",
    "\n",
    "        # print(np.unique(label_2))\n",
    "\n",
    "        for i in range(label.shape[0]):\n",
    "            K = np.unique(label[i, :, 1][label[i, :, 1] != 0])\n",
    "            if len(K) == 2:\n",
    "                label_K[i, :] = K\n",
    "\n",
    "                if label[i, 0, 1] != label_K[i, 0]:\n",
    "                    label_K[i, :] = label_K[i, ::-1]\n",
    "\n",
    "            elif len(K) == 1:\n",
    "                states = label_2[i, :]\n",
    "                if 1 in states:\n",
    "                    # print(np.unique(states))\n",
    "                    if states[0] == 1:\n",
    "                        label_K[i, :] = [0, K[0]]\n",
    "                    else:\n",
    "                        label_K[i, :] = [K[0], 0]\n",
    "\n",
    "                    # print(label_regression[i,:])\n",
    "\n",
    "                else:\n",
    "                    label_K[i, :] = [K[0], K[0]]\n",
    "\n",
    "            else:\n",
    "                if np.unique(label[i, :, 1]) == 0:\n",
    "                    label_K[i, :] = [0, 0]\n",
    "                else:\n",
    "\n",
    "                    # print(np.unique(label[i,:,1]))\n",
    "\n",
    "                    # print(Ds)\n",
    "                    raise Exception(\"more than 2 diffusions\")\n",
    "\n",
    "        # print(np.unique(label_2))\n",
    "        label_alpha = np.zeros((label.shape[0], 2))\n",
    "\n",
    "        for i in range(label.shape[0]):\n",
    "            alpha = np.unique(label[i, :, 0][label[i, :, 0] != 0])\n",
    "            if len(alpha) == 2:\n",
    "                label_alpha[i, :] = alpha\n",
    "                if label[i, 0, 0] != label_alpha[i, 0]:\n",
    "                    label_alpha[i, :] = label_alpha[i, ::-1]\n",
    "\n",
    "            elif len(alpha) == 1:\n",
    "                states = label_2[i, :]\n",
    "                if 1 in states:\n",
    "                    # print(np.unique(states))\n",
    "                    if states[0] == 1:\n",
    "                        label_alpha[i, :] = [0, alpha[0]]\n",
    "                    else:\n",
    "                        label_alpha[i, :] = [alpha[0], 0]\n",
    "\n",
    "                    # print(label_regression[i,:])\n",
    "\n",
    "                else:\n",
    "                    label_alpha[i, :] = [alpha[0], alpha[0]]\n",
    "\n",
    "            else:\n",
    "                if np.unique(label[i, :, 1]) == 0:\n",
    "                    label_alpha[i, :] = [0, 0]\n",
    "                else:\n",
    "\n",
    "                    # print(np.unique(label[i,:,1]))\n",
    "\n",
    "                    # print(Ds)\n",
    "                    raise Exception(\"more than 2 diffusions\")\n",
    "\n",
    "        label_segmentation = np.zeros((label_2.shape[0], label_2.shape[1]))\n",
    "\n",
    "        for i in range(label.shape[0]):\n",
    "            if label_K[i, 0] == label_K[i, 1]:\n",
    "                position = label[i, :, 1] == label_K[i, 0]\n",
    "                label_segmentation[i, position] = 1\n",
    "            else:\n",
    "\n",
    "                position_1 = label[i, :, 1] == label_K[i, 0]\n",
    "                position_2 = label[i, :, 1] == label_K[i, 1]\n",
    "\n",
    "                label_segmentation[i, position_1] = 1\n",
    "                label_segmentation[i, position_2] = 2\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(data.astype(np.float32)),\n",
    "            torch.from_numpy(label_segmentation.astype(np.float32)),\n",
    "            torch.from_numpy(label_K),\n",
    "            torch.from_numpy(label_alpha),\n",
    "        )\n",
    "        # torch.from_numpy(label_2.astype(np.float32)),\n",
    "\n",
    "\n",
    "def add_noise(data):\n",
    "    noise_amplitude = np.random.choice(\n",
    "        [\n",
    "            0.01,\n",
    "            0.1,\n",
    "        ]\n",
    "    )\n",
    "    noise = np.random.normal(0, noise_amplitude, data[:, :, :].shape)\n",
    "    data[:, :, :][data[:, :, 1:] != 0] = (\n",
    "        data[:, :, :][data[:, :, 1:] != 0] + data[:, :, :][data[:, :, 1:] != 0] * noise\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_set = list_directory_tree_with_pathlib(\n",
    "    r\"/home/m.lavaud/ANDI_2_Challenge_EMetBrown/data/datasets\",\n",
    ")\n",
    "np.random.shuffle(all_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149609"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = Dataset_all_data(all_data_set, transform=False, pad=(20, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = iter(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.1671e-01,  3.0546e-01, -1.5678e-01,  1.1744e-01,  3.7452e-01,\n",
       "            4.0953e+00],\n",
       "          [-2.5307e-01,  3.8916e-01, -3.0977e-01,  2.2672e-01,  4.6420e-01,\n",
       "            2.2397e-02],\n",
       "          [-1.3720e-01, -9.8354e-02, -4.5953e-01,  3.3352e-01,  1.6881e-01,\n",
       "            6.0835e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00]],\n",
       " \n",
       "         [[ 2.3851e-01, -7.6293e-01, -2.4391e-03, -1.7491e-01,  7.9934e-01,\n",
       "            1.2678e+00],\n",
       "          [ 7.3752e-01, -8.8956e-02, -1.9328e-03, -3.2447e-01,  7.4287e-01,\n",
       "            5.7330e+00],\n",
       "          [ 5.8969e-01, -2.4742e-01, -1.5035e-02, -4.8211e-01,  6.3949e-01,\n",
       "            6.1777e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00]],\n",
       " \n",
       "         [[-5.2841e-01,  1.3900e-01, -9.1624e-01,  9.9304e-03,  5.4639e-01,\n",
       "            3.3988e+00],\n",
       "          [-1.6772e+00, -1.0496e+00, -1.8441e+00,  1.2415e-03,  1.9785e+00,\n",
       "            5.6344e+00],\n",
       "          [-1.2621e+00, -9.0021e-01, -2.7499e+00,  1.6222e-02,  1.5502e+00,\n",
       "            6.1935e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-6.6253e-01,  1.1527e+00, -2.0420e-02,  1.1419e-01,  1.3295e+00,\n",
       "            4.1907e+00],\n",
       "          [-1.5675e-01,  1.2332e+00, -4.2040e-02,  2.1998e-01,  1.2431e+00,\n",
       "            1.9088e-01],\n",
       "          [ 3.5343e-01,  1.2264e+00, -6.5484e-02,  3.1694e-01,  1.2763e+00,\n",
       "            2.0251e-01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00]],\n",
       " \n",
       "         [[ 8.5831e-01,  9.2900e-01,  1.4093e-01,  2.5697e-02,  1.2648e+00,\n",
       "            5.4583e+00],\n",
       "          [ 3.9310e-01,  2.6737e-01,  2.7298e-01,  3.8286e-02,  4.7541e-01,\n",
       "            6.2011e-02],\n",
       "          [ 3.9129e-01,  8.2503e-02,  3.9124e-01,  5.2492e-02,  3.9989e-01,\n",
       "            1.0141e-01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00]],\n",
       " \n",
       "         [[ 5.6731e-01,  2.7734e-01,  1.5971e-03,  1.0976e-01,  6.3147e-01,\n",
       "            5.8285e+00],\n",
       "          [-2.1734e-01, -1.5971e-01, -1.4433e-03,  2.2398e-01,  2.6971e-01,\n",
       "            1.3045e-01],\n",
       "          [ 8.2013e-02,  1.3565e-01, -1.8106e-03,  3.4103e-01,  1.5851e-01,\n",
       "            6.0771e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00]]]),\n",
       " tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.]]),\n",
       " tensor([[0.0776, 0.0776],\n",
       "         [0.2637, 0.2637],\n",
       "         [0.2254, 0.2254],\n",
       "         [0.1699, 0.2388],\n",
       "         [0.0355, 0.0355],\n",
       "         [0.2075, 0.2075],\n",
       "         [0.2637, 0.2637],\n",
       "         [0.0195, 0.7256],\n",
       "         [0.1392, 0.3489],\n",
       "         [0.0338, 0.5487],\n",
       "         [0.3259, 1.0445],\n",
       "         [0.1438, 0.9174],\n",
       "         [0.1776, 0.1776],\n",
       "         [0.2309, 0.2309],\n",
       "         [0.1671, 0.1671],\n",
       "         [0.1998, 0.5027],\n",
       "         [0.2016, 0.2016],\n",
       "         [0.2736, 0.5199],\n",
       "         [0.2633, 0.2633],\n",
       "         [0.0627, 0.0627]], dtype=torch.float64),\n",
       " tensor([[1.7198, 1.7198],\n",
       "         [1.8899, 1.8899],\n",
       "         [1.7980, 1.7980],\n",
       "         [1.7816, 1.4094],\n",
       "         [1.9299, 1.9299],\n",
       "         [1.9619, 1.9619],\n",
       "         [1.8899, 1.8899],\n",
       "         [1.6436, 1.4077],\n",
       "         [1.8687, 1.6224],\n",
       "         [1.8805, 0.8217],\n",
       "         [1.5877, 1.2644],\n",
       "         [1.6610, 1.1121],\n",
       "         [1.6574, 1.6574],\n",
       "         [1.8354, 1.8354],\n",
       "         [1.6862, 1.6862],\n",
       "         [1.7286, 0.8980],\n",
       "         [1.7626, 1.7626],\n",
       "         [1.5949, 1.3358],\n",
       "         [1.8013, 1.8013],\n",
       "         [1.5877, 1.5877]], dtype=torch.float64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(training_dataset, shuffle=True, batch_size=100, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m.lavaud/miniconda3/envs/torch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mamba_ssm import  Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class segmentation_model(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand, dropout=0.2, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.mamba = Mamba(\n",
    "            d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand\n",
    "        ).to(device)\n",
    "\n",
    "        self.flipped_mamba = Mamba(\n",
    "            d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand\n",
    "        ).to(device)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout).to(device)\n",
    "        self.fc = nn.Linear(in_features=d_model, out_features=3).to(device)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "\n",
    "        mamba_out = self.mamba(input)\n",
    "        mamba_flipped_out = self.flipped_mamba(torch.flip(input, dims=[1]))\n",
    "        mamba_out = mamba_out + mamba_flipped_out\n",
    "\n",
    "        mamba_out = self.dropout(mamba_out)\n",
    "        out = self.fc(mamba_out)\n",
    "\n",
    "        return out  # No activation here ! It is done by the cross entropy loss\n",
    "\n",
    "\n",
    "class K_regression(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand, dropout=0.3, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.mamba = Mamba(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand,\n",
    "        ).to(device)\n",
    "        self.flipped_mamba = Mamba(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand,\n",
    "        ).to(device)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout).to(device)\n",
    "        self.fc = nn.Linear(in_features=199 * d_model, out_features=2).to(device)\n",
    "\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        mamba_out = self.mamba(input)\n",
    "        mamba_flipped_out = self.flipped_mamba(torch.flip(input, dims=[1]))\n",
    "\n",
    "        mamba_out = mamba_out + mamba_flipped_out\n",
    "\n",
    "        mamba_out = rearrange(mamba_out, \"b l c -> b (l c)\")\n",
    "\n",
    "        mamba_out = self.dropout(mamba_out)\n",
    "        out = self.fc(mamba_out)\n",
    "        out = torch.clamp(out, min=0, max=1e12)\n",
    "        out[out < 1e-7] = 0\n",
    "        return out\n",
    "\n",
    "\n",
    "class alpha_regression(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand, dropout=0.3, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.mamba = Mamba(\n",
    "            d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand\n",
    "        ).to(device)\n",
    "        self.flipped_mamba = Mamba(\n",
    "            d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand\n",
    "        ).to(device)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout).to(device)\n",
    "        self.fc = nn.Linear(in_features=199 * d_model, out_features=2).to(device)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # self.relu = nn.ReLU()\n",
    "    def forward(self, input):\n",
    "\n",
    "        mamba_out = self.mamba(input)\n",
    "        mamba_flipped_out = self.flipped_mamba(torch.flip(input, dims=[1]))\n",
    "\n",
    "        mamba_out = mamba_out + mamba_flipped_out\n",
    "\n",
    "        mamba_out = rearrange(mamba_out, \"b l c -> b (l c)\")\n",
    "\n",
    "        mamba_out = self.dropout(mamba_out)\n",
    "        out = self.fc(mamba_out)\n",
    "\n",
    "        \n",
    "        return self.sigmoid(out) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class all_at_the_same_time(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand, dropout=0.2, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.model_K = K_regression(\n",
    "            d_model + 1, d_state=d_state, d_conv=d_conv, expand=expand, dropout=dropout\n",
    "        )\n",
    "        self.model_alpha = alpha_regression(\n",
    "            d_model +1, d_state=d_state, d_conv=d_conv, expand=expand, dropout=dropout\n",
    "        )\n",
    "        self.segmentation = segmentation_model(\n",
    "            d_model, d_state=d_state, d_conv=d_conv, expand=expand, dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        probas = self.segmentation(x)\n",
    "\n",
    "        classes = torch.argmax(torch.softmax(probas, dim=2), dim=2)\n",
    "\n",
    "        classes[x[:, :, 0] == 0] = 0\n",
    "\n",
    "        classes[x[:, :, 0] == 0] = 0\n",
    "        classes = classes.unsqueeze(-1)  # adding a dimension for the next step\n",
    "        concat_entry = torch.cat((classes, x), dim=2)\n",
    "\n",
    "        alpha = self.model_alpha(concat_entry)\n",
    "\n",
    "        K = self.model_K(concat_entry)\n",
    "\n",
    "        return probas, alpha, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = all_at_the_same_time(d_model=6, d_state=4, d_conv=4, expand=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "alpha_criterion = torch.nn.L1Loss().to(\"cuda\")\n",
    "\n",
    "\n",
    "class MSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, actual):\n",
    "        return self.mse(torch.log(pred + 1), torch.log(actual + 1))\n",
    "\n",
    "\n",
    "K_criterion = MSLELoss().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1497 [00:12<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1497 [00:19<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'alpha_regression' object has no attribute 'sigmoid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 38\u001b[0m\n\u001b[1;32m     26\u001b[0m classification_targets \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     27\u001b[0m     torch\u001b[38;5;241m.\u001b[39mflatten(\n\u001b[1;32m     28\u001b[0m         classification_targets,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 38\u001b[0m classification_output, alpha_output, K_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m classification_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(classification_output)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m## Computation of the weight of the classes\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[40], line 27\u001b[0m, in \u001b[0;36mall_at_the_same_time.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m classes \u001b[38;5;241m=\u001b[39m classes\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# adding a dimension for the next step\u001b[39;00m\n\u001b[1;32m     25\u001b[0m concat_entry \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((classes, x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_alpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcat_entry\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_K(concat_entry)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probas, alpha, K\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 101\u001b[0m, in \u001b[0;36malpha_regression.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(mamba_out)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# out = \u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m(out) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'alpha_regression' object has no attribute 'sigmoid'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_epoch = 10\n",
    "total_classification_loss = []\n",
    "total_K_loss = []\n",
    "total_alpha_loss = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    running_classification_loss = []\n",
    "    running_alpha_loss = []\n",
    "    running_K_loss = []\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "        model.train()\n",
    "\n",
    "        for inputs, classification_targets, K_targets, alpha_targets in tepoch:\n",
    "\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            inputs = inputs.to(\"cuda\", dtype=torch.float32)  # Ensuring float32 type\n",
    "            inputs = torch.flatten(inputs, start_dim=0, end_dim=1)\n",
    "\n",
    "            classification_targets = (\n",
    "                torch.flatten(\n",
    "                    classification_targets,\n",
    "                    start_dim=0,\n",
    "                    end_dim=1,\n",
    "                )\n",
    "                .type(torch.LongTensor)\n",
    "                .to(\"cuda\")\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            classification_output, alpha_output, K_output = model(inputs)\n",
    "            classification_output = torch.squeeze(classification_output)\n",
    "\n",
    "            ## Computation of the weight of the classes\n",
    "\n",
    "            counts = torch.unique(classification_targets, return_counts=True)[1][1:]\n",
    "            weights = torch.sum(counts) / (2 * counts)\n",
    "            weights = weights.to(\"cpu\", dtype=torch.float32)  # Ensuring float32 type\n",
    "            weight = torch.zeros(3, dtype=torch.float32)  # Ensuring float32 type\n",
    "            weight[1:] = weights\n",
    "            ###\n",
    "\n",
    "            classification_criterion = nn.CrossEntropyLoss(\n",
    "                weight=weight, ignore_index=0\n",
    "            )\n",
    "\n",
    "            classification_loss = classification_criterion(\n",
    "                classification_output.view(-1, 3).to(\n",
    "                    \"cpu\", dtype=torch.float32\n",
    "                ),  # Ensuring float32 type\n",
    "                classification_targets.view(-1).to(\"cpu\"),\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            alpha_targets = torch.flatten(alpha_targets, start_dim=0, end_dim=1).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  # Ensuring float32 type\n",
    "            alpha_loss = alpha_criterion(alpha_output, alpha_targets).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  # Ensuring float32 type\n",
    "\n",
    "            K_targets = torch.flatten(K_targets, start_dim=0, end_dim=1).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  # Ensuring float32 type\n",
    "            K_loss = K_criterion(K_output, K_targets).to(\n",
    "                \"cuda\", dtype=torch.float32\n",
    "            )  # Ensuring float32 type\n",
    "\n",
    "            total_loss = alpha_loss + K_loss + classification_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tepoch.set_postfix(\n",
    "                loss_c=classification_loss.item(),\n",
    "                loss_a=alpha_loss.item(),\n",
    "                loss_K=K_loss.item(),\n",
    "            )\n",
    "\n",
    "            running_classification_loss.append(classification_loss.item())\n",
    "            running_alpha_loss.append(\n",
    "                alpha_loss.item()\n",
    "            )  # Corrected to alpha_loss.item()\n",
    "            running_K_loss.append(K_loss.item())  # Corrected to K_loss.item()\n",
    "\n",
    "        total_classification_loss.append(np.mean(running_classification_loss))\n",
    "        total_alpha_loss.append(np.mean(running_alpha_loss))\n",
    "        total_K_loss.append(np.mean(running_K_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
