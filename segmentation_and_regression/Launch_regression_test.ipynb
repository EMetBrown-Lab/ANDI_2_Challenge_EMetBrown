{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a4c625-1262-40f8-8bf6-10af6a1f0509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_fn import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e29ef47-b60d-48c2-ac8a-8cfac322103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train((1,0.05,1e-3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a047659-d3b7-4f0f-82cd-87efffa28f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_mamba_stacks = [1,2,3,4]\n",
    "dropouts = [0.05,0.1]\n",
    "learning_rate = [1e-3,1e-4]\n",
    "n_layer = [4,8,16,32]\n",
    "\n",
    "import itertools\n",
    "all_tests = list(itertools.product(bi_mamba_stacks, dropouts, learning_rate, n_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864cab03-d441-4d0d-813f-2cccc36e8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "from torch.multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7bd9a83-f5ae-43c4-bb28-ba822f61bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3405345-e421-4781-a29f-eb610b86a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b9dd9e9-dd5a-4862-a5a7-9978d842a85d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n",
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:08<00:00,  6.20batch/s, loss=0.0241]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0313]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0319]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0341]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0293] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:08<00:00,  6.22batch/s, loss=0.0227]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0339]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0278]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0194] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0198]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:08<00:00,  6.21batch/s, loss=0.0259]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0195]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:09<00:00,  6.20batch/s, loss=0.0393] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0109]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0276]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0296]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0267]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0233]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0233]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0191]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0288]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0385]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0367] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0193]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:08<00:00,  6.23batch/s, loss=0.0226]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:09<00:00,  6.20batch/s, loss=0.0303]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0184]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0261]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:09<00:00,  6.20batch/s, loss=0.0267] \n",
      "Epoch 14: 100%|██████████| 800/800 [02:09<00:00,  6.20batch/s, loss=0.0172]\n",
      "  0%|          | 0/800 [00:00<?, ?batch/s]  | 1/64 [40:12<42:13:25, 2412.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 1/800 [00:00<02:09,  6.16batch/s, loss=0.338]5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:08<00:00,  6.22batch/s, loss=0.0561] \n",
      "Epoch 0: 100%|██████████| 800/800 [02:09<00:00,  6.20batch/s, loss=0.0293]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.049] \n",
      "Epoch 1: 100%|██████████| 800/800 [02:09<00:00,  6.20batch/s, loss=0.0546]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0189] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0334]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0269] \n",
      "Epoch 3: 100%|██████████| 800/800 [02:08<00:00,  6.20batch/s, loss=0.0185]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0576]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0203]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0247] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0205] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:09<00:00,  6.15batch/s, loss=0.04]  \n",
      "Epoch 6: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0143]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0163] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:09<00:00,  6.20batch/s, loss=0.0205]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.033]  \n",
      "Epoch 8: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0331]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0289]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.029] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0326] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0241]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0293]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:10<00:00,  6.15batch/s, loss=0.0237]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0349]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0245]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0405] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:10<00:00,  6.15batch/s, loss=0.0314]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0253] \n",
      "Epoch 14: 100%|██████████| 800/800 [02:09<00:00,  6.20batch/s, loss=0.0229]\n",
      "  0%|          | 0/800 [00:00<?, ?batch/s]| 3/64 [1:20:27<27:49:36, 1642.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 25/800 [00:04<02:02,  6.33batch/s, loss=0.3]  .45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:08<00:00,  6.20batch/s, loss=0.118] \n",
      "Epoch 0: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0922]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:09<00:00,  6.20batch/s, loss=0.0523]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0616]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:08<00:00,  6.21batch/s, loss=0.052] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:09<00:00,  6.20batch/s, loss=0.0613]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:08<00:00,  6.25batch/s, loss=0.0351]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0892]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:09<00:00,  6.20batch/s, loss=0.0391]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0515]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0293]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0802]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0282]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0274]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:10<00:00,  6.14batch/s, loss=0.0347]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0481]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0192]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0598]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:08<00:00,  6.21batch/s, loss=0.0456]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.067] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:09<00:00,  6.20batch/s, loss=0.0301]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0554]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0326]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:09<00:00,  6.20batch/s, loss=0.0718]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:08<00:00,  6.21batch/s, loss=0.0418]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0492]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:08<00:00,  6.21batch/s, loss=0.0211]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0548]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0457] \n",
      "Epoch 14: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0797]\n",
      "  0%|          | 0/800 [00:00<?, ?batch/s]| 5/64 [2:00:34<24:37:54, 1502.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 1/800 [00:00<02:11,  6.09batch/s, loss=0.378]].04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:11<00:00,  6.07batch/s, loss=0.0401]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.093] \n",
      "Epoch 1: 100%|██████████| 800/800 [02:11<00:00,  6.08batch/s, loss=0.0327]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0825]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:11<00:00,  6.09batch/s, loss=0.0303]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.06]  \n",
      "Epoch 3: 100%|██████████| 800/800 [02:10<00:00,  6.11batch/s, loss=0.0398]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0783]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:11<00:00,  6.09batch/s, loss=0.0201]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0576]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:10<00:00,  6.12batch/s, loss=0.0434]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0834]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:10<00:00,  6.15batch/s, loss=0.0404]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0617]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0722]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:11<00:00,  6.08batch/s, loss=0.0161]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.083] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:10<00:00,  6.12batch/s, loss=0.018] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:10<00:00,  6.15batch/s, loss=0.0465]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:11<00:00,  6.08batch/s, loss=0.0236]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0255]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:11<00:00,  6.07batch/s, loss=0.0311]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.021] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:11<00:00,  6.10batch/s, loss=0.0277]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:09<00:00,  6.15batch/s, loss=0.0411] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:11<00:00,  6.11batch/s, loss=0.0225]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0156] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:11<00:00,  6.09batch/s, loss=0.0382]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0229] \n",
      "Epoch 14: 100%|██████████| 800/800 [02:11<00:00,  6.10batch/s, loss=0.0284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   8%|▊         | 67/800 [00:10<02:01,  6.03batch/s, loss=0.122].86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:10<00:00,  6.14batch/s, loss=0.0429]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0835]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:10<00:00,  6.12batch/s, loss=0.04]  \n",
      "Epoch 1: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.072] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:10<00:00,  6.11batch/s, loss=0.027] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0212]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:11<00:00,  6.10batch/s, loss=0.0351]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0228]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:10<00:00,  6.13batch/s, loss=0.0317] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:10<00:00,  6.14batch/s, loss=0.032] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:10<00:00,  6.11batch/s, loss=0.0263] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0414]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:10<00:00,  6.14batch/s, loss=0.0266]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0268]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:10<00:00,  6.13batch/s, loss=0.0207]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0378]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0218] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:10<00:00,  6.12batch/s, loss=0.0201]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0266]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:10<00:00,  6.11batch/s, loss=0.0311]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0332] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:10<00:00,  6.15batch/s, loss=0.0401]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:08<00:00,  6.20batch/s, loss=0.0466] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:10<00:00,  6.14batch/s, loss=0.0353]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.023]  \n",
      "Epoch 12: 100%|██████████| 800/800 [02:10<00:00,  6.11batch/s, loss=0.0467]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.027]  \n",
      "Epoch 13: 100%|██████████| 800/800 [02:10<00:00,  6.13batch/s, loss=0.0217]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0289]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:10<00:00,  6.12batch/s, loss=0.0186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▏                               | 9/64 [3:21:45<20:31:51, 1343.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0709]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.05]  \n",
      "Epoch 1: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0794]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0183]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0314]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0201]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0462]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0262]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0277] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:10<00:00,  6.15batch/s, loss=0.0374]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:10<00:00,  6.15batch/s, loss=0.0196]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0291]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0216] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0232]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0202] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:08<00:00,  6.22batch/s, loss=0.0466]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:08<00:00,  6.21batch/s, loss=0.0267]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0312]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.023] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0163]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0415] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0264]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:08<00:00,  6.21batch/s, loss=0.0383]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:10<00:00,  6.15batch/s, loss=0.0353]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:09<00:00,  6.20batch/s, loss=0.0239]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0312]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.039]  \n",
      "Epoch 13: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0641]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0224]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0293]\n",
      "  0%|          | 0/800 [00:00<?, ?batch/s] 11/64 [4:01:48<18:55:42, 1285.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  12%|█▏        | 97/800 [00:15<01:58,  5.95batch/s, loss=0.0628]00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:12<00:00,  6.04batch/s, loss=0.0617]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:10<00:00,  6.13batch/s, loss=0.0924]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:12<00:00,  6.05batch/s, loss=0.0465]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:10<00:00,  6.13batch/s, loss=0.0913]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:11<00:00,  6.06batch/s, loss=0.0353]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:10<00:00,  6.13batch/s, loss=0.0745]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:11<00:00,  6.07batch/s, loss=0.0351]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:10<00:00,  6.14batch/s, loss=0.0986]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:12<00:00,  6.06batch/s, loss=0.03]  \n",
      "Epoch 4: 100%|██████████| 800/800 [02:10<00:00,  6.14batch/s, loss=0.0793]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:12<00:00,  6.05batch/s, loss=0.025] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0941]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:11<00:00,  6.07batch/s, loss=0.0271]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:10<00:00,  6.11batch/s, loss=0.116] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:12<00:00,  6.04batch/s, loss=0.0286]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:10<00:00,  6.13batch/s, loss=0.0879]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:12<00:00,  6.03batch/s, loss=0.028] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:10<00:00,  6.13batch/s, loss=0.0901]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:12<00:00,  6.05batch/s, loss=0.0489]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:10<00:00,  6.15batch/s, loss=0.0687]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:12<00:00,  6.05batch/s, loss=0.0211]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:10<00:00,  6.11batch/s, loss=0.0903]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:11<00:00,  6.08batch/s, loss=0.033] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:11<00:00,  6.11batch/s, loss=0.0669]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:10<00:00,  6.14batch/s, loss=0.055] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:12<00:00,  6.05batch/s, loss=0.0312]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:10<00:00,  6.12batch/s, loss=0.0816]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:12<00:00,  6.05batch/s, loss=0.0536]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:10<00:00,  6.12batch/s, loss=0.0582]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:11<00:00,  6.07batch/s, loss=0.0349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 13/800 [00:02<02:06,  6.23batch/s, loss=0.189].65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0349]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:12<00:00,  6.05batch/s, loss=0.0698]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0622]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:12<00:00,  6.04batch/s, loss=0.0471]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.017] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:13<00:00,  6.01batch/s, loss=0.07]  \n",
      "Epoch 3: 100%|██████████| 800/800 [02:10<00:00,  6.15batch/s, loss=0.0271]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:12<00:00,  6.03batch/s, loss=0.0329]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0315]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:12<00:00,  6.03batch/s, loss=0.0308]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:10<00:00,  6.14batch/s, loss=0.0217]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:12<00:00,  6.04batch/s, loss=0.025] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:09<00:00,  6.19batch/s, loss=0.0224]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:11<00:00,  6.08batch/s, loss=0.0274]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:09<00:00,  6.18batch/s, loss=0.0302] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:12<00:00,  6.04batch/s, loss=0.0193]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0327]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:11<00:00,  6.07batch/s, loss=0.0256]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:10<00:00,  6.14batch/s, loss=0.0309]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:13<00:00,  6.01batch/s, loss=0.0441]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:09<00:00,  6.17batch/s, loss=0.0411] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:12<00:00,  6.02batch/s, loss=0.0222]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:10<00:00,  6.13batch/s, loss=0.04]  \n",
      "Epoch 11: 100%|██████████| 800/800 [02:12<00:00,  6.03batch/s, loss=0.0217]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0239]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:13<00:00,  6.01batch/s, loss=0.0215] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:09<00:00,  6.15batch/s, loss=0.0218]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:11<00:00,  6.08batch/s, loss=0.0322]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:09<00:00,  6.16batch/s, loss=0.0409]\n",
      "Epoch 14:  94%|█████████▍| 755/800 [02:04<00:07,  6.19batch/s, loss=0.0283]s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 800/800 [02:12<00:00,  6.04batch/s, loss=0.0261]\n",
      "Epoch 0:  29%|██▉       | 235/800 [00:39<01:34,  5.95batch/s, loss=0.0296]7s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0619]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:16<00:00,  5.85batch/s, loss=0.0244]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:14<00:00,  5.97batch/s, loss=0.0237] \n",
      "Epoch 1: 100%|██████████| 800/800 [02:17<00:00,  5.82batch/s, loss=0.0216]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0352]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:16<00:00,  5.87batch/s, loss=0.0269]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0274] \n",
      "Epoch 3: 100%|██████████| 800/800 [02:17<00:00,  5.82batch/s, loss=0.039]]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:13<00:00,  5.97batch/s, loss=0.0417]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:16<00:00,  5.84batch/s, loss=0.0239]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:13<00:00,  6.00batch/s, loss=0.0193]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:16<00:00,  5.87batch/s, loss=0.033] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0238] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:16<00:00,  5.86batch/s, loss=0.0342]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.017] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:16<00:00,  5.85batch/s, loss=0.017] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0358] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:16<00:00,  5.84batch/s, loss=0.0162] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0205] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:17<00:00,  5.82batch/s, loss=0.0264]]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0274] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:16<00:00,  5.87batch/s, loss=0.032]  \n",
      "Epoch 11: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0136] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:17<00:00,  5.84batch/s, loss=0.0157]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0171] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:17<00:00,  5.82batch/s, loss=0.0158]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0296] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:17<00:00,  5.83batch/s, loss=0.0185] \n",
      "Epoch 14: 100%|██████████| 800/800 [02:13<00:00,  6.00batch/s, loss=0.0143] \n",
      "Epoch 0:   0%|          | 0/800 [00:00<?, ?batch/s].83batch/s, loss=0.0223]s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 800/800 [02:16<00:00,  5.85batch/s, loss=0.0179]\n",
      "Epoch 0:  58%|█████▊    | 462/800 [01:17<00:56,  5.96batch/s, loss=0.0273]2s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0303]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:16<00:00,  5.88batch/s, loss=0.0234]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:13<00:00,  6.00batch/s, loss=0.0215]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0239]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0232]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:16<00:00,  5.88batch/s, loss=0.0241] \n",
      "Epoch 3: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0391]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:16<00:00,  5.87batch/s, loss=0.0217] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0317] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:15<00:00,  5.88batch/s, loss=0.0148]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0219] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:16<00:00,  5.86batch/s, loss=0.0298] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:14<00:00,  5.97batch/s, loss=0.0246] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:16<00:00,  5.85batch/s, loss=0.0186] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:13<00:00,  6.00batch/s, loss=0.0232] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0145] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:14<00:00,  5.97batch/s, loss=0.0234] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:16<00:00,  5.88batch/s, loss=0.0299] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0152] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0268]] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0381]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0294] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0252] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0186] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0221] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0224] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0171] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0307] \n",
      "Epoch 14: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0266] \n",
      "Epoch 14:  45%|████▌     | 363/800 [01:01<01:14,  5.89batch/s, loss=0.00883]/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0264] \n",
      "Epoch 0:  80%|███████▉  | 637/800 [01:46<00:27,  5.92batch/s, loss=0.0642]3s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0626]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:14<00:00,  5.97batch/s, loss=0.0318]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.065] \n",
      "Epoch 1: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0391]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:13<00:00,  6.00batch/s, loss=0.0621]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0239] \n",
      "Epoch 3: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0483] \n",
      "Epoch 3: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0182] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0259]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0252] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:13<00:00,  6.00batch/s, loss=0.0813] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0223] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:12<00:00,  6.02batch/s, loss=0.108] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.031] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0614]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.024]  \n",
      "Epoch 8: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0329] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0369] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:14<00:00,  5.97batch/s, loss=0.101] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0344]]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0847]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0356] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:13<00:00,  5.97batch/s, loss=0.0558]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0286] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0622]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0385] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0585]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:14<00:00,  5.97batch/s, loss=0.0344]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0512]\n",
      " 33%|███████████▊                        | 21/64 [7:27:42<16:59:57, 1423.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0181]\n",
      "Epoch 0:   0%|          | 0/800 [00:00<?, ?batch/s]03batch/s, loss=0.0621]8s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0815]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0347]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:14<00:00,  5.97batch/s, loss=0.0417]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:15<00:00,  5.93batch/s, loss=0.0363]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:12<00:00,  6.02batch/s, loss=0.0522]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0218]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0962]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0405]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:13<00:00,  6.01batch/s, loss=0.0218]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0286] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:13<00:00,  6.00batch/s, loss=0.0569]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:14<00:00,  5.97batch/s, loss=0.0156] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:12<00:00,  6.03batch/s, loss=0.0788]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0369] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0355]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0124]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:12<00:00,  6.03batch/s, loss=0.0612] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0362] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:13<00:00,  6.00batch/s, loss=0.0736]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0152]] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0435] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0183] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.1]   \n",
      "Epoch 11: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0195]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:13<00:00,  6.00batch/s, loss=0.0375]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0175] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0825]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0206]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0607]\n",
      "Epoch 14:  16%|█▋        | 131/800 [00:22<01:51,  6.00batch/s, loss=0.0355]s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0163]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0259]\n",
      "  0%|          | 0/800 [00:00<?, ?batch/s] 24/64 [8:11:30<11:41:24, 1052.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0331] \n",
      "Epoch 1: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0637]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0196] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0617]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0159]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0317]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0288]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:14<00:00,  5.97batch/s, loss=0.0669] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0242] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0554]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0314] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0661]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0176] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:13<00:00,  5.97batch/s, loss=0.0546] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0175]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0433]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:13<00:00,  5.97batch/s, loss=0.0404] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0555] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.04]  ]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0246]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0371] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:13<00:00,  5.97batch/s, loss=0.0466]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.015] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0287]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0274] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:14<00:00,  5.97batch/s, loss=0.0834]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0163]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0777]\n",
      "Epoch 0:   0%|          | 1/800 [00:00<02:27,  5.41batch/s, loss=0.0944]8]6s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.044] \n",
      "Epoch 0: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0368]\n",
      "  0%|          | 0/800 [00:00<?, ?batch/s] 26/64 [8:53:18<11:09:19, 1056.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0313]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0316]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0245] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0193]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.028]] \n",
      "Epoch 3: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.024] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:14<00:00,  5.97batch/s, loss=0.0278] \n",
      "Epoch 3: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0148]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.023]  \n",
      "Epoch 4: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0149]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0241] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0278]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0199] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0202]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0155]]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.038]  \n",
      "Epoch 9: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0148] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0232] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0213]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0139]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:13<00:00,  6.00batch/s, loss=0.0359]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0244]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0241] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0327]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:13<00:00,  5.98batch/s, loss=0.0393] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0195]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0136] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0139] \n",
      "  0%|          | 0/800 [00:00<?, ?batch/s] 27/64 [9:32:07<14:45:26, 1435.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:14<00:00,  5.97batch/s, loss=0.0479]] \n",
      "Epoch 14: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0158]\n",
      "Epoch 1:  14%|█▍        | 116/800 [00:19<01:53,  6.03batch/s, loss=0.0277]7s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0279]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:13<00:00,  5.97batch/s, loss=0.0261]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0325]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0436]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0338] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0289]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:13<00:00,  5.97batch/s, loss=0.0177]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0207]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:13<00:00,  6.01batch/s, loss=0.017] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0245]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:13<00:00,  5.97batch/s, loss=0.0204] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.024] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0417] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0229]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:13<00:00,  6.00batch/s, loss=0.0262]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0306]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.02]   \n",
      "Epoch 8: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0429]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0363] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0206]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0399]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0298]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0325]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0235]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0303] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0167]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0158] \n",
      "Epoch 13:  95%|█████████▍| 758/800 [02:08<00:07,  6.00batch/s, loss=0.0263]s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0253]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:13<00:00,  5.97batch/s, loss=0.0333]] \n",
      "Epoch 14: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.014] \n",
      "Epoch 1:  30%|███       | 242/800 [00:40<01:39,  5.60batch/s, loss=0.0327]1s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0401]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.022] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0285]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0184]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:14<00:00,  5.97batch/s, loss=0.037] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0448]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:14<00:00,  5.97batch/s, loss=0.0357]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0277]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0334]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.019] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0358]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0232]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:14<00:00,  5.97batch/s, loss=0.0413]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.042] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0696] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:15<00:00,  5.93batch/s, loss=0.0416]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0369]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0505]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:14<00:00,  5.96batch/s, loss=0.0257]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0204]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0308]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0433]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0329]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0504]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0184]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0486]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0288]\n",
      " 48%|████████████████▉                  | 31/64 [10:55:25<13:10:04, 1436.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0192]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:18<00:00,  5.79batch/s, loss=0.069] ]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.038] \n",
      "Epoch 1:  27%|██▋       | 217/800 [00:37<01:39,  5.84batch/s, loss=0.0657]5s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 800/800 [02:18<00:00,  5.76batch/s, loss=0.0575]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:19<00:00,  5.75batch/s, loss=0.0262]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:19<00:00,  5.75batch/s, loss=0.0672]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:19<00:00,  5.74batch/s, loss=0.0289]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:19<00:00,  5.74batch/s, loss=0.0597] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:19<00:00,  5.75batch/s, loss=0.0147] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:18<00:00,  5.77batch/s, loss=0.063]  \n",
      "Epoch 3: 100%|██████████| 800/800 [02:18<00:00,  5.76batch/s, loss=0.0247]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:18<00:00,  5.77batch/s, loss=0.0588] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:19<00:00,  5.74batch/s, loss=0.0264] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:18<00:00,  5.79batch/s, loss=0.0615] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:19<00:00,  5.73batch/s, loss=0.0499] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:18<00:00,  5.76batch/s, loss=0.0493] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:19<00:00,  5.74batch/s, loss=0.0201] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:18<00:00,  5.78batch/s, loss=0.0526]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:19<00:00,  5.76batch/s, loss=0.0282] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:18<00:00,  5.80batch/s, loss=0.0555] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:19<00:00,  5.76batch/s, loss=0.0216] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:18<00:00,  5.78batch/s, loss=0.0491]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.0256] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:18<00:00,  5.76batch/s, loss=0.064]  \n",
      "Epoch 10: 100%|██████████| 800/800 [02:19<00:00,  5.74batch/s, loss=0.0176]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:18<00:00,  5.76batch/s, loss=0.0532] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.0242]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:19<00:00,  5.75batch/s, loss=0.0642] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:19<00:00,  5.74batch/s, loss=0.0242]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:18<00:00,  5.77batch/s, loss=0.0542] \n",
      "Epoch 13:  97%|█████████▋| 774/800 [02:14<00:04,  5.61batch/s, loss=0.0133]s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 800/800 [02:18<00:00,  5.77batch/s, loss=0.0164]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:18<00:00,  5.78batch/s, loss=0.0207]] \n",
      "Epoch 14: 100%|██████████| 800/800 [02:19<00:00,  5.74batch/s, loss=0.0145] \n",
      "Epoch 1:  27%|██▋       | 219/800 [00:37<01:40,  5.81batch/s, loss=0.0241]2s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 800/800 [02:18<00:00,  5.76batch/s, loss=0.0365] \n",
      "Epoch 0: 100%|██████████| 800/800 [02:19<00:00,  5.75batch/s, loss=0.1]   \n",
      "Epoch 2: 100%|██████████| 800/800 [02:18<00:00,  5.77batch/s, loss=0.0184] \n",
      "Epoch 1: 100%|██████████| 800/800 [02:19<00:00,  5.75batch/s, loss=0.0691]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:19<00:00,  5.74batch/s, loss=0.0213] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:19<00:00,  5.73batch/s, loss=0.0871]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:19<00:00,  5.74batch/s, loss=0.0101] \n",
      "Epoch 3: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.0731]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:18<00:00,  5.76batch/s, loss=0.0211] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:20<00:00,  5.70batch/s, loss=0.0986]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:18<00:00,  5.77batch/s, loss=0.0337] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.123] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:18<00:00,  5.76batch/s, loss=0.0233]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:19<00:00,  5.73batch/s, loss=0.0739]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:18<00:00,  5.76batch/s, loss=0.0156] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:19<00:00,  5.73batch/s, loss=0.0704]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:18<00:00,  5.77batch/s, loss=0.021]  \n",
      "Epoch 8: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.123] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:19<00:00,  5.75batch/s, loss=0.0256] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.0802]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.0148] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:20<00:00,  5.70batch/s, loss=0.0684]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.0166] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.105] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.0287] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:20<00:00,  5.70batch/s, loss=0.0738]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.0216] \n",
      "Epoch 13:  95%|█████████▌| 760/800 [02:13<00:07,  5.63batch/s, loss=0.137] s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.0745]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:19<00:00,  5.75batch/s, loss=0.0432]]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:20<00:00,  5.68batch/s, loss=0.0823]\n",
      "Epoch 1:  30%|██▉       | 238/800 [00:41<01:35,  5.87batch/s, loss=0.0526]9s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 800/800 [02:19<00:00,  5.75batch/s, loss=0.0703]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.0449]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.115] \n",
      "Epoch 1: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.0377]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.0417] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:21<00:00,  5.66batch/s, loss=0.0343]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:19<00:00,  5.74batch/s, loss=0.0322]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:20<00:00,  5.68batch/s, loss=0.037] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.0684] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:21<00:00,  5.67batch/s, loss=0.0413]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.0358]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.0404]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:19<00:00,  5.73batch/s, loss=0.0461]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.0287]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.0688] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:20<00:00,  5.70batch/s, loss=0.0229]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:19<00:00,  5.73batch/s, loss=0.051]] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.0345]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.0311]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:20<00:00,  5.70batch/s, loss=0.029]  \n",
      "Epoch 11: 100%|██████████| 800/800 [02:18<00:00,  5.76batch/s, loss=0.0612]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.0262]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:18<00:00,  5.77batch/s, loss=0.0348]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:20<00:00,  5.70batch/s, loss=0.0303]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:18<00:00,  5.76batch/s, loss=0.0375] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.0257]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:19<00:00,  5.75batch/s, loss=0.0694] \n",
      "Epoch 13:  85%|████████▍ | 677/800 [01:59<00:21,  5.70batch/s, loss=0.0235]s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 800/800 [02:20<00:00,  5.69batch/s, loss=0.0433]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:19<00:00,  5.74batch/s, loss=0.0636]]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:20<00:00,  5.69batch/s, loss=0.0387] \n",
      "Epoch 1:  40%|███▉      | 319/800 [00:55<01:25,  5.66batch/s, loss=0.0785]2s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 800/800 [02:18<00:00,  5.77batch/s, loss=0.0755]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.0708]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:19<00:00,  5.73batch/s, loss=0.0505]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.102]]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:19<00:00,  5.74batch/s, loss=0.0681]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:19<00:00,  5.74batch/s, loss=0.0842]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:21<00:00,  5.64batch/s, loss=0.0789]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:22<00:00,  5.62batch/s, loss=0.0862]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:20<00:00,  5.69batch/s, loss=0.0481]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.105] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:19<00:00,  5.73batch/s, loss=0.0587]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:20<00:00,  5.70batch/s, loss=0.0622]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:19<00:00,  5.73batch/s, loss=0.0504]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:21<00:00,  5.66batch/s, loss=0.08] ]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:21<00:00,  5.65batch/s, loss=0.0606]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:21<00:00,  5.67batch/s, loss=0.132]]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:20<00:00,  5.69batch/s, loss=0.0643]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.103]8]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.0625]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:21<00:00,  5.66batch/s, loss=0.0802]]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:19<00:00,  5.75batch/s, loss=0.0529]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:20<00:00,  5.69batch/s, loss=0.0903]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:19<00:00,  5.73batch/s, loss=0.065] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:19<00:00,  5.73batch/s, loss=0.0718]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:19<00:00,  5.73batch/s, loss=0.0511]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.0911]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:19<00:00,  5.74batch/s, loss=0.0633]\n",
      "Epoch 13:  87%|████████▋ | 695/800 [02:01<00:18,  5.73batch/s, loss=0.0946]s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.0853]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:19<00:00,  5.74batch/s, loss=0.0231] \n",
      "Epoch 14: 100%|██████████| 800/800 [02:20<00:00,  5.68batch/s, loss=0.0707]\n",
      "Epoch 1:  38%|███▊      | 301/800 [00:51<01:26,  5.75batch/s, loss=0.0362]3s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 800/800 [02:18<00:00,  5.77batch/s, loss=0.0151] \n",
      "Epoch 0: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.0503]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:19<00:00,  5.74batch/s, loss=0.0184]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:20<00:00,  5.70batch/s, loss=0.0186]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:20<00:00,  5.69batch/s, loss=0.0292] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:21<00:00,  5.66batch/s, loss=0.0218]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:21<00:00,  5.65batch/s, loss=0.0187] \n",
      "Epoch 3: 100%|██████████| 800/800 [02:22<00:00,  5.60batch/s, loss=0.0299] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:21<00:00,  5.66batch/s, loss=0.0229] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:21<00:00,  5.66batch/s, loss=0.02]   \n",
      "Epoch 6: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.0357] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:21<00:00,  5.67batch/s, loss=0.0246]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:21<00:00,  5.67batch/s, loss=0.023] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:22<00:00,  5.63batch/s, loss=0.0147]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:21<00:00,  5.66batch/s, loss=0.00998]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:21<00:00,  5.67batch/s, loss=0.0272]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:21<00:00,  5.67batch/s, loss=0.0327] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:22<00:00,  5.60batch/s, loss=0.0268]]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:21<00:00,  5.64batch/s, loss=0.0201]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:23<00:00,  5.56batch/s, loss=0.0184]] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:22<00:00,  5.63batch/s, loss=0.0126] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:23<00:00,  5.59batch/s, loss=0.0332] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:21<00:00,  5.64batch/s, loss=0.0175] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:24<00:00,  5.55batch/s, loss=0.043] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:22<00:00,  5.60batch/s, loss=0.0246] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:22<00:00,  5.61batch/s, loss=0.0188] \n",
      "Epoch 14: 100%|██████████| 800/800 [02:21<00:00,  5.66batch/s, loss=0.0354] \n",
      "Epoch 13:  82%|████████▏ | 659/800 [01:58<00:25,  5.60batch/s, loss=0.0146]s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 800/800 [02:23<00:00,  5.59batch/s, loss=0.0145]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:20<00:00,  5.68batch/s, loss=0.0754]] \n",
      "Epoch 14: 100%|██████████| 800/800 [02:22<00:00,  5.63batch/s, loss=0.0306] \n",
      "Epoch 1:  42%|████▏     | 339/800 [01:00<01:20,  5.71batch/s, loss=0.0249]2s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 800/800 [02:22<00:00,  5.60batch/s, loss=0.0272]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:23<00:00,  5.58batch/s, loss=0.0507]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:23<00:00,  5.59batch/s, loss=0.0284]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:24<00:00,  5.55batch/s, loss=0.0611]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:21<00:00,  5.67batch/s, loss=0.0163]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:22<00:00,  5.63batch/s, loss=0.0516]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:20<00:00,  5.68batch/s, loss=0.0254]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:22<00:00,  5.61batch/s, loss=0.0514]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:21<00:00,  5.65batch/s, loss=0.0206] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:23<00:00,  5.57batch/s, loss=0.0531]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:21<00:00,  5.67batch/s, loss=0.0131]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:21<00:00,  5.66batch/s, loss=0.0681]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:19<00:00,  5.73batch/s, loss=0.0201]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:21<00:00,  5.67batch/s, loss=0.0574]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:21<00:00,  5.67batch/s, loss=0.0205]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:22<00:00,  5.63batch/s, loss=0.0711] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:21<00:00,  5.64batch/s, loss=0.0127]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:22<00:00,  5.63batch/s, loss=0.0606]]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:21<00:00,  5.65batch/s, loss=0.0204]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:22<00:00,  5.63batch/s, loss=0.0682]]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:23<00:00,  5.59batch/s, loss=0.0317]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:22<00:00,  5.62batch/s, loss=0.0622]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:20<00:00,  5.67batch/s, loss=0.0183]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:22<00:00,  5.63batch/s, loss=0.056] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:20<00:00,  5.68batch/s, loss=0.026]  \n",
      "Epoch 12: 100%|██████████| 800/800 [02:21<00:00,  5.63batch/s, loss=0.0616] \n",
      "Epoch 14: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.0286] \n",
      "Epoch 13:  75%|███████▌  | 601/800 [01:45<00:34,  5.83batch/s, loss=0.0601]s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 800/800 [02:21<00:00,  5.67batch/s, loss=0.0552]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:24<00:00,  5.54batch/s, loss=0.0196]]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:21<00:00,  5.67batch/s, loss=0.0533]\n",
      "Epoch 1:  44%|████▍     | 356/800 [01:04<01:18,  5.64batch/s, loss=0.0234]3s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 800/800 [02:24<00:00,  5.55batch/s, loss=0.0349]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:20<00:00,  5.69batch/s, loss=0.0632]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:24<00:00,  5.54batch/s, loss=0.0234]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:20<00:00,  5.68batch/s, loss=0.0573]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:24<00:00,  5.53batch/s, loss=0.0344]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:20<00:00,  5.68batch/s, loss=0.0698]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:25<00:00,  5.51batch/s, loss=0.0409]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:21<00:00,  5.66batch/s, loss=0.0677]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:26<00:00,  5.48batch/s, loss=0.0259]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:21<00:00,  5.66batch/s, loss=0.0638]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:25<00:00,  5.50batch/s, loss=0.0594]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:21<00:00,  5.65batch/s, loss=0.0841]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:24<00:00,  5.52batch/s, loss=0.0239]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:21<00:00,  5.65batch/s, loss=0.0519]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:25<00:00,  5.52batch/s, loss=0.0352]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:20<00:00,  5.69batch/s, loss=0.0487]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:24<00:00,  5.53batch/s, loss=0.0243]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:21<00:00,  5.67batch/s, loss=0.0652]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:25<00:00,  5.49batch/s, loss=0.0406]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:21<00:00,  5.66batch/s, loss=0.05]  \n",
      "Epoch 11: 100%|██████████| 800/800 [02:25<00:00,  5.49batch/s, loss=0.0189]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:20<00:00,  5.68batch/s, loss=0.0569]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:24<00:00,  5.53batch/s, loss=0.0324]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:21<00:00,  5.65batch/s, loss=0.0457]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:20<00:00,  5.69batch/s, loss=0.0262] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:24<00:00,  5.52batch/s, loss=0.042] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:20<00:00,  5.68batch/s, loss=0.0346] \n",
      "Epoch 14: 100%|██████████| 800/800 [02:24<00:00,  5.53batch/s, loss=0.0177]\n",
      "Epoch 14:   5%|▌         | 43/800 [00:07<02:16,  5.55batch/s, loss=0.0195]3s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 800/800 [02:20<00:00,  5.69batch/s, loss=0.0175]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.0414]\n",
      "  0%|          | 0/800 [00:00<?, ?batch/s] 46/64 [16:03:34<5:34:21, 1114.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:21<00:00,  5.67batch/s, loss=0.036] \n",
      "Epoch 1: 100%|██████████| 800/800 [02:20<00:00,  5.70batch/s, loss=0.0374]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:21<00:00,  5.67batch/s, loss=0.0318]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:20<00:00,  5.70batch/s, loss=0.0264]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:20<00:00,  5.70batch/s, loss=0.0474]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:19<00:00,  5.75batch/s, loss=0.0138]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.0397]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:20<00:00,  5.70batch/s, loss=0.0175]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:20<00:00,  5.69batch/s, loss=0.0362] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:20<00:00,  5.71batch/s, loss=0.0466]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:21<00:00,  5.66batch/s, loss=0.0384] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:20<00:00,  5.70batch/s, loss=0.0461]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:21<00:00,  5.66batch/s, loss=0.0443]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:20<00:00,  5.70batch/s, loss=0.0257]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:21<00:00,  5.67batch/s, loss=0.026] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:20<00:00,  5.70batch/s, loss=0.025] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:21<00:00,  5.65batch/s, loss=0.0242]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:21<00:00,  5.66batch/s, loss=0.039] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:20<00:00,  5.70batch/s, loss=0.0226] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:21<00:00,  5.67batch/s, loss=0.0405]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.0243]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:20<00:00,  5.69batch/s, loss=0.0318]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:20<00:00,  5.69batch/s, loss=0.0367] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:20<00:00,  5.68batch/s, loss=0.032] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:20<00:00,  5.69batch/s, loss=0.028] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:21<00:00,  5.66batch/s, loss=0.0246]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:19<00:00,  5.72batch/s, loss=0.0268] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:21<00:00,  5.67batch/s, loss=0.031] \n",
      "  0%|          | 0/800 [00:00<?, ?batch/s] 47/64 [16:44:11<7:08:10, 1511.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:24<00:00,  5.52batch/s, loss=0.0367]]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:21<00:00,  5.65batch/s, loss=0.0349]\n",
      "Epoch 1:   0%|          | 1/800 [00:00<02:22,  5.60batch/s, loss=0.022]1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 800/800 [02:27<00:00,  5.43batch/s, loss=0.0193]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:27<00:00,  5.42batch/s, loss=0.0522]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:26<00:00,  5.45batch/s, loss=0.027]  \n",
      "Epoch 1: 100%|██████████| 800/800 [02:27<00:00,  5.43batch/s, loss=0.0614]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:28<00:00,  5.39batch/s, loss=0.0548] \n",
      "Epoch 3: 100%|██████████| 800/800 [02:29<00:00,  5.34batch/s, loss=0.0327]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:27<00:00,  5.41batch/s, loss=0.0629] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:27<00:00,  5.41batch/s, loss=0.0266]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:25<00:00,  5.50batch/s, loss=0.0217] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:26<00:00,  5.47batch/s, loss=0.0592]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:27<00:00,  5.43batch/s, loss=0.0464] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:27<00:00,  5.42batch/s, loss=0.0277]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:26<00:00,  5.47batch/s, loss=0.0425] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:27<00:00,  5.44batch/s, loss=0.0566]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:25<00:00,  5.49batch/s, loss=0.0192] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:26<00:00,  5.46batch/s, loss=0.0575]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:26<00:00,  5.45batch/s, loss=0.035] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:27<00:00,  5.44batch/s, loss=0.0612]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:26<00:00,  5.47batch/s, loss=0.0237] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:26<00:00,  5.46batch/s, loss=0.0454]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:28<00:00,  5.39batch/s, loss=0.0285] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:28<00:00,  5.38batch/s, loss=0.0612]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:27<00:00,  5.44batch/s, loss=0.0187] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:27<00:00,  5.44batch/s, loss=0.0603]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:26<00:00,  5.44batch/s, loss=0.0211] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:27<00:00,  5.42batch/s, loss=0.057] \n",
      "Epoch 14: 100%|██████████| 800/800 [02:26<00:00,  5.45batch/s, loss=0.0285] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:27<00:00,  5.43batch/s, loss=0.0761]\n",
      "  0%|          | 0/800 [00:00<?, ?batch/s] 49/64 [17:29:28<6:24:52, 1539.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:27<00:00,  5.42batch/s, loss=0.0627]]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:27<00:00,  5.41batch/s, loss=0.0586]\n",
      "Epoch 1:   2%|▏         | 14/800 [00:02<02:17,  5.71batch/s, loss=0.0577]00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 800/800 [02:26<00:00,  5.48batch/s, loss=0.0625] \n",
      "Epoch 0: 100%|██████████| 800/800 [02:27<00:00,  5.42batch/s, loss=0.0675]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:28<00:00,  5.39batch/s, loss=0.0638] \n",
      "Epoch 1: 100%|██████████| 800/800 [02:28<00:00,  5.38batch/s, loss=0.0814]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:28<00:00,  5.40batch/s, loss=0.0245] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:27<00:00,  5.42batch/s, loss=0.0771]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:28<00:00,  5.39batch/s, loss=0.0249] \n",
      "Epoch 3: 100%|██████████| 800/800 [02:27<00:00,  5.41batch/s, loss=0.0299]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:27<00:00,  5.44batch/s, loss=0.0177]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:27<00:00,  5.42batch/s, loss=0.0305]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:27<00:00,  5.42batch/s, loss=0.0227] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:28<00:00,  5.40batch/s, loss=0.0829]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:27<00:00,  5.41batch/s, loss=0.0275] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:27<00:00,  5.41batch/s, loss=0.0527]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:28<00:00,  5.37batch/s, loss=0.0239] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:29<00:00,  5.35batch/s, loss=0.0421]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:28<00:00,  5.37batch/s, loss=0.0363] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:29<00:00,  5.36batch/s, loss=0.0442]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:26<00:00,  5.46batch/s, loss=0.0308] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:25<00:00,  5.50batch/s, loss=0.0785]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:21<00:00,  5.64batch/s, loss=0.0222] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:22<00:00,  5.63batch/s, loss=0.0466]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:18<00:00,  5.77batch/s, loss=0.0107] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:18<00:00,  5.76batch/s, loss=0.0761]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0264] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:16<00:00,  5.86batch/s, loss=0.0315]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0184] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:16<00:00,  5.88batch/s, loss=0.0684]\n",
      "  0%|          | 0/800 [00:00<?, ?batch/s] 51/64 [18:14:12<5:34:17, 1542.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:16<00:00,  5.84batch/s, loss=0.101] ] \n",
      "Epoch 14: 100%|██████████| 800/800 [02:16<00:00,  5.87batch/s, loss=0.113] \n",
      "Epoch 1:   2%|▎         | 20/800 [00:03<02:12,  5.88batch/s, loss=0.0861]67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.063]]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:16<00:00,  5.88batch/s, loss=0.0303]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:16<00:00,  5.85batch/s, loss=0.059]]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:16<00:00,  5.88batch/s, loss=0.0612]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:16<00:00,  5.88batch/s, loss=0.0996]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:16<00:00,  5.86batch/s, loss=0.0452]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.134]  \n",
      "Epoch 3: 100%|██████████| 800/800 [02:15<00:00,  5.88batch/s, loss=0.0495]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:16<00:00,  5.85batch/s, loss=0.0983] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0782]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0812]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:16<00:00,  5.88batch/s, loss=0.0648]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.102] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0791]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:17<00:00,  5.84batch/s, loss=0.0985] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:17<00:00,  5.83batch/s, loss=0.0866]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:17<00:00,  5.83batch/s, loss=0.129] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:16<00:00,  5.85batch/s, loss=0.0601]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:16<00:00,  5.85batch/s, loss=0.127] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:18<00:00,  5.78batch/s, loss=0.0714]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:17<00:00,  5.83batch/s, loss=0.0771] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:17<00:00,  5.82batch/s, loss=0.031] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:17<00:00,  5.81batch/s, loss=0.0799]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:17<00:00,  5.84batch/s, loss=0.0341]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0988] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:15<00:00,  5.88batch/s, loss=0.0305]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0787]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:16<00:00,  5.88batch/s, loss=0.0155]\n",
      "Epoch 0:   0%|          | 0/800 [00:00<?, ?batch/s]8:56:16<4:34:33, 1497.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.089] ]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:16<00:00,  5.87batch/s, loss=0.0276]\n",
      "Epoch 1:   3%|▎         | 25/800 [00:04<02:06,  6.15batch/s, loss=0.0602]54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.107] \n",
      "Epoch 0: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0282]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0749]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0402]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0722]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:15<00:00,  5.88batch/s, loss=0.0406]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0509] \n",
      "Epoch 3: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0604]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.133]  \n",
      "Epoch 4: 100%|██████████| 800/800 [02:16<00:00,  5.87batch/s, loss=0.0407]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0651]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0287]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:16<00:00,  5.84batch/s, loss=0.0984] \n",
      "Epoch 6: 100%|██████████| 800/800 [02:16<00:00,  5.87batch/s, loss=0.0723]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.106]] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:16<00:00,  5.87batch/s, loss=0.0306]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:16<00:00,  5.87batch/s, loss=0.152] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0886]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0887]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0493]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:16<00:00,  5.88batch/s, loss=0.0968] \n",
      "Epoch 10: 100%|██████████| 800/800 [02:16<00:00,  5.85batch/s, loss=0.0752]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:16<00:00,  5.84batch/s, loss=0.0882] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:16<00:00,  5.88batch/s, loss=0.0571]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:16<00:00,  5.84batch/s, loss=0.121]  \n",
      "Epoch 12: 100%|██████████| 800/800 [02:17<00:00,  5.83batch/s, loss=0.0697]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:17<00:00,  5.84batch/s, loss=0.117]] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:17<00:00,  5.83batch/s, loss=0.0536]\n",
      "  0%|          | 0/800 [00:00<?, ?batch/s] 55/64 [19:38:11<3:40:58, 1473.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:16<00:00,  5.86batch/s, loss=0.0449]]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:16<00:00,  5.84batch/s, loss=0.0876]\n",
      "Epoch 1:   4%|▎         | 28/800 [00:04<02:20,  5.49batch/s, loss=0.0312]05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 800/800 [02:17<00:00,  5.83batch/s, loss=0.0838]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:17<00:00,  5.80batch/s, loss=0.0398]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:17<00:00,  5.81batch/s, loss=0.0304]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:17<00:00,  5.83batch/s, loss=0.0166]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:16<00:00,  5.84batch/s, loss=0.0722] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:17<00:00,  5.83batch/s, loss=0.0264]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:17<00:00,  5.80batch/s, loss=0.0829] \n",
      "Epoch 3: 100%|██████████| 800/800 [02:18<00:00,  5.79batch/s, loss=0.0122]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:18<00:00,  5.79batch/s, loss=0.0537] \n",
      "Epoch 4: 100%|██████████| 800/800 [02:17<00:00,  5.81batch/s, loss=0.0224]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:18<00:00,  5.79batch/s, loss=0.0948] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:17<00:00,  5.83batch/s, loss=0.0276]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:18<00:00,  5.79batch/s, loss=0.05]  \n",
      "Epoch 6: 100%|██████████| 800/800 [02:17<00:00,  5.81batch/s, loss=0.0195]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:16<00:00,  5.85batch/s, loss=0.023]  \n",
      "Epoch 8: 100%|██████████| 800/800 [02:17<00:00,  5.81batch/s, loss=0.0481]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:16<00:00,  5.85batch/s, loss=0.0347] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:17<00:00,  5.83batch/s, loss=0.0445]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:17<00:00,  5.83batch/s, loss=0.0166]]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:17<00:00,  5.81batch/s, loss=0.0713]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:17<00:00,  5.81batch/s, loss=0.0222] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:17<00:00,  5.80batch/s, loss=0.118] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:17<00:00,  5.82batch/s, loss=0.0348]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:17<00:00,  5.81batch/s, loss=0.0621]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:17<00:00,  5.83batch/s, loss=0.028]  \n",
      "Epoch 13: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0503]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0284]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0794]\n",
      "Epoch 14:   3%|▎         | 21/800 [00:03<02:12,  5.87batch/s, loss=0.0269]8s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0239] \n",
      "Epoch 0: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0585]\n",
      "  0%|          | 0/800 [00:00<?, ?batch/s] 58/64 [20:23:15<1:47:40, 1076.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0285]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0667]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0429] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0619]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.026]  \n",
      "Epoch 3: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0561]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0222]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:16<00:00,  5.88batch/s, loss=0.0532]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0384] \n",
      "Epoch 5: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0544]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0214]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0678]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0348] \n",
      "Epoch 7: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0441]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0212] \n",
      "Epoch 8: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0549]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0246] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0671]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0181]]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0549]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0203] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0616]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:15<00:00,  5.88batch/s, loss=0.023]  \n",
      "Epoch 12: 100%|██████████| 800/800 [02:16<00:00,  5.87batch/s, loss=0.0506]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0405] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:16<00:00,  5.86batch/s, loss=0.0637]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0235] \n",
      "Epoch 14: 100%|██████████| 800/800 [02:17<00:00,  5.83batch/s, loss=0.0476]\n",
      "Epoch 14:   8%|▊         | 68/800 [00:11<02:01,  6.03batch/s, loss=0.0222]9s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 800/800 [02:16<00:00,  5.86batch/s, loss=0.0232] \n",
      "Epoch 0: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0416]\n",
      "  0%|          | 0/800 [00:00<?, ?batch/s] 60/64 [21:05:00<1:11:11, 1067.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:16<00:00,  5.88batch/s, loss=0.0269]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0359]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0278]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:16<00:00,  5.88batch/s, loss=0.022] \n",
      "Epoch 2: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0312]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:16<00:00,  5.88batch/s, loss=0.0363]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:16<00:00,  5.87batch/s, loss=0.0199]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0467]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:16<00:00,  5.88batch/s, loss=0.0246]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0383]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0251]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0389]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0216]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:16<00:00,  5.86batch/s, loss=0.0322]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.023]]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0245]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0415] \n",
      "Epoch 9: 100%|██████████| 800/800 [02:16<00:00,  5.88batch/s, loss=0.0298]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0212]]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:16<00:00,  5.88batch/s, loss=0.0201]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0327]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:16<00:00,  5.86batch/s, loss=0.0401]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0186]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0187]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.033] \n",
      "Epoch 13: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0251]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:16<00:00,  5.86batch/s, loss=0.0258] \n",
      "Epoch 14: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0183]\n",
      "Epoch 14:  10%|█         | 84/800 [00:14<01:59,  5.97batch/s, loss=0.0197]4s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0196]\n",
      "Epoch 0: 100%|██████████| 800/800 [02:14<00:00,  5.95batch/s, loss=0.0169]\n",
      "  0%|          | 0/800 [00:00<?, ?batch/s] | 62/64 [21:46:48<35:27, 1063.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0586]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0681]\n",
      "Epoch 1: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0679]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0307]\n",
      "Epoch 2: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0642]\n",
      "Epoch 3: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.026] \n",
      "Epoch 3: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0577]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0298]\n",
      "Epoch 4: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0533]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0251]\n",
      "Epoch 5: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0518]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0245]\n",
      "Epoch 6: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0619]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:15<00:00,  5.92batch/s, loss=0.0184]\n",
      "Epoch 7: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0577]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0603]\n",
      "Epoch 8: 100%|██████████| 800/800 [02:16<00:00,  5.87batch/s, loss=0.0902]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0209]\n",
      "Epoch 9: 100%|██████████| 800/800 [02:15<00:00,  5.91batch/s, loss=0.0635]]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:15<00:00,  5.93batch/s, loss=0.0149]\n",
      "Epoch 10: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0685] \n",
      "Epoch 11: 100%|██████████| 800/800 [02:14<00:00,  5.94batch/s, loss=0.0404]\n",
      "Epoch 11: 100%|██████████| 800/800 [02:14<00:00,  5.93batch/s, loss=0.0637]\n",
      "Epoch 12: 100%|██████████| 800/800 [02:15<00:00,  5.93batch/s, loss=0.016] \n",
      "Epoch 12: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0508]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0391]\n",
      "Epoch 13: 100%|██████████| 800/800 [02:15<00:00,  5.90batch/s, loss=0.0547]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:15<00:00,  5.89batch/s, loss=0.0347]\n",
      "Epoch 14: 100%|██████████| 800/800 [02:13<00:00,  5.99batch/s, loss=0.0555]s/it]\n",
      "100%|██████████████████████████████████████| 64/64 [22:28:31<00:00, 1264.25s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "with Pool(processes=2) as pool:\n",
    "\n",
    "   # results = pool.map(train, all_tests[:2])\n",
    "    results = list(tqdm.tqdm(pool.imap(train, all_tests), total=len(all_tests)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b37ea12a-507c-4d93-a376-fecf675db8fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bi_mamba_stacks': 1,\n",
       "  'n_layers': 4,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.3067895493656397,\n",
       "   0.03390580795472488,\n",
       "   0.032322347688023,\n",
       "   0.03132162234047428,\n",
       "   0.030240751815726982,\n",
       "   0.029762514978647233,\n",
       "   0.029390031604561953,\n",
       "   0.029231180521892382,\n",
       "   0.029209821295225993,\n",
       "   0.029123787719290705,\n",
       "   0.028857748399022966,\n",
       "   0.02880083557451144,\n",
       "   0.028747750032925977,\n",
       "   0.028728191961999983,\n",
       "   0.028635324047645554],\n",
       "  'running_test_loss': [0.0031824295860715212,\n",
       "   0.0030858672359026966,\n",
       "   0.0030312170274555683,\n",
       "   0.002798447168432176,\n",
       "   0.002754696330521256,\n",
       "   0.0027466432040091603,\n",
       "   0.002688110772520304,\n",
       "   0.002685898307245225,\n",
       "   0.0027361952336505055,\n",
       "   0.002691027383785695,\n",
       "   0.0026815428524278105,\n",
       "   0.002696739363949746,\n",
       "   0.0027185050128027796,\n",
       "   0.002659532550256699,\n",
       "   0.002660215267445892]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'n_layers': 8,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.05563533940934576,\n",
       "   0.03151487461174838,\n",
       "   0.029989040883956476,\n",
       "   0.029429739678744227,\n",
       "   0.02919628681614995,\n",
       "   0.02907651647692546,\n",
       "   0.028915695513132958,\n",
       "   0.02889897149056196,\n",
       "   0.028895781928440556,\n",
       "   0.028716975594870745,\n",
       "   0.028879114361479878,\n",
       "   0.028665519206551835,\n",
       "   0.028839224060066046,\n",
       "   0.02870353017002344,\n",
       "   0.028840576778165997],\n",
       "  'running_test_loss': [0.003060955446213484,\n",
       "   0.0028425963441841306,\n",
       "   0.002780423577409238,\n",
       "   0.002730047909077257,\n",
       "   0.0027250764854252338,\n",
       "   0.002696964002214372,\n",
       "   0.0026871644062921407,\n",
       "   0.0026686267317272724,\n",
       "   0.0027484993422403933,\n",
       "   0.002728018285240978,\n",
       "   0.0026791845969855784,\n",
       "   0.0026834093159995975,\n",
       "   0.0027073504161089657,\n",
       "   0.0027253934694454075,\n",
       "   0.0026972088203765453]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'n_layers': 16,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.35921544627752156,\n",
       "   0.04161122824065387,\n",
       "   0.031844963900512085,\n",
       "   0.030448891167761758,\n",
       "   0.029782431478379293,\n",
       "   0.029452144325478002,\n",
       "   0.029209242803044617,\n",
       "   0.029133855742402376,\n",
       "   0.028862432801397518,\n",
       "   0.028915793622145428,\n",
       "   0.028861587620340287,\n",
       "   0.028881106555927544,\n",
       "   0.028767137764953078,\n",
       "   0.02879192659514956,\n",
       "   0.028651667032390832],\n",
       "  'running_test_loss': [0.00452611562050879,\n",
       "   0.0032063542753458024,\n",
       "   0.002864530234131962,\n",
       "   0.002797496095765382,\n",
       "   0.0027808934776112435,\n",
       "   0.002718766102101654,\n",
       "   0.0027263950179331003,\n",
       "   0.0027111975350417195,\n",
       "   0.002699030706193298,\n",
       "   0.002724877466913313,\n",
       "   0.0027056396240368484,\n",
       "   0.0026914408928714693,\n",
       "   0.002780526366084814,\n",
       "   0.0026677001155912877,\n",
       "   0.0027513679028488697]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'n_layers': 32,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.037054357696324584,\n",
       "   0.029946963298134507,\n",
       "   0.029209736685734244,\n",
       "   0.028893210712121798,\n",
       "   0.028848645746475086,\n",
       "   0.028836623906390742,\n",
       "   0.02879569018492475,\n",
       "   0.02874410534161143,\n",
       "   0.028631301516434178,\n",
       "   0.028656735788099467,\n",
       "   0.028483604420907794,\n",
       "   0.028341971230693163,\n",
       "   0.028256154275732116,\n",
       "   0.02821010066079907,\n",
       "   0.02808416259009391],\n",
       "  'running_test_loss': [0.002824706359766424,\n",
       "   0.002705548370722681,\n",
       "   0.002722494510933757,\n",
       "   0.002687567398417741,\n",
       "   0.0027130696792155504,\n",
       "   0.0026778153479099275,\n",
       "   0.002686494113411754,\n",
       "   0.0027071963213384153,\n",
       "   0.0026603938015177845,\n",
       "   0.002653404267504811,\n",
       "   0.0026255967463366687,\n",
       "   0.0026638932046480477,\n",
       "   0.00263126963051036,\n",
       "   0.0026246282253414395,\n",
       "   0.0025727266920730473]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'n_layers': 4,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.1400443569291383,\n",
       "   0.08567612498067319,\n",
       "   0.06822330588940531,\n",
       "   0.05629619278712198,\n",
       "   0.036918395371176305,\n",
       "   0.03431797840166837,\n",
       "   0.033051071800291536,\n",
       "   0.03233071318711154,\n",
       "   0.03201584755908698,\n",
       "   0.03188937312690541,\n",
       "   0.031575039860326795,\n",
       "   0.03138614048366435,\n",
       "   0.031212802957743407,\n",
       "   0.03090134360594675,\n",
       "   0.030719848728040233],\n",
       "  'running_test_loss': [0.008462592406198382,\n",
       "   0.007246712233871222,\n",
       "   0.005845111730508506,\n",
       "   0.003419095365330577,\n",
       "   0.003097085466608405,\n",
       "   0.0029975438145920636,\n",
       "   0.0029586947723291815,\n",
       "   0.0029494570405222474,\n",
       "   0.0029440390444360675,\n",
       "   0.0029003263749182223,\n",
       "   0.0029041892681270836,\n",
       "   0.0028795402674004437,\n",
       "   0.0028997526997700334,\n",
       "   0.0028514566072262823,\n",
       "   0.0028100086832419037]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'n_layers': 8,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.22022754891542717,\n",
       "   0.07039150130003691,\n",
       "   0.06735515740467235,\n",
       "   0.06589524107868783,\n",
       "   0.06434892769553699,\n",
       "   0.06353339660796337,\n",
       "   0.06282495956867934,\n",
       "   0.06216247107600793,\n",
       "   0.061440515217836944,\n",
       "   0.06106635123374872,\n",
       "   0.060658634454011914,\n",
       "   0.060327288066037,\n",
       "   0.06012507284176536,\n",
       "   0.06017281145323068,\n",
       "   0.060060722685884686],\n",
       "  'running_test_loss': [0.006565896488726139,\n",
       "   0.0061991096446290615,\n",
       "   0.005928545817267149,\n",
       "   0.005799231718294323,\n",
       "   0.0057747455141507085,\n",
       "   0.005655642394442112,\n",
       "   0.005644098702818155,\n",
       "   0.005531636417843402,\n",
       "   0.005546968448441476,\n",
       "   0.005514146164059639,\n",
       "   0.005479939058423042,\n",
       "   0.005468834072817117,\n",
       "   0.005428333819378167,\n",
       "   0.005445430854801089,\n",
       "   0.005388581614010036]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'n_layers': 16,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.08662367391865701,\n",
       "   0.04323860729578882,\n",
       "   0.03890580974984914,\n",
       "   0.036108008627779783,\n",
       "   0.03316685844794847,\n",
       "   0.03212751314160414,\n",
       "   0.0314030112628825,\n",
       "   0.03078758109593764,\n",
       "   0.030430795891443266,\n",
       "   0.030169891997938975,\n",
       "   0.02992652183631435,\n",
       "   0.029829141240334138,\n",
       "   0.029693714557215572,\n",
       "   0.029651462745387106,\n",
       "   0.029499095058999957],\n",
       "  'running_test_loss': [0.004287938945926726,\n",
       "   0.0036553138848394156,\n",
       "   0.0033430404225364327,\n",
       "   0.0031079637464135885,\n",
       "   0.002961127135436982,\n",
       "   0.002900802240241319,\n",
       "   0.002847667987458408,\n",
       "   0.002832196104340255,\n",
       "   0.0028062357450835407,\n",
       "   0.002840491207782179,\n",
       "   0.00278395792003721,\n",
       "   0.002756095780991018,\n",
       "   0.002766827227547765,\n",
       "   0.0027574867703951894,\n",
       "   0.002756798350252211]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'n_layers': 32,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.1156213899096474,\n",
       "   0.07070901710074395,\n",
       "   0.06448985448572785,\n",
       "   0.06318402205128222,\n",
       "   0.06275189475622028,\n",
       "   0.06251969186589122,\n",
       "   0.062272350382991136,\n",
       "   0.06175232034176588,\n",
       "   0.06023057613521814,\n",
       "   0.04015675593633205,\n",
       "   0.03302956550847739,\n",
       "   0.0314764991030097,\n",
       "   0.030559538572560996,\n",
       "   0.029811924138339236,\n",
       "   0.0292612663237378],\n",
       "  'running_test_loss': [0.007099572617560625,\n",
       "   0.006406862627714872,\n",
       "   0.006127072237432003,\n",
       "   0.0061037266813218595,\n",
       "   0.006090365026146173,\n",
       "   0.006109311494976282,\n",
       "   0.0060802877545356755,\n",
       "   0.0060827019494026895,\n",
       "   0.004709338977001608,\n",
       "   0.0031063126372173427,\n",
       "   0.002881313381716609,\n",
       "   0.0028270604689605533,\n",
       "   0.0027983366842381655,\n",
       "   0.0027438278323970735,\n",
       "   0.002712351901922375]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'n_layers': 4,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.13687149221776054,\n",
       "   0.0356440721033141,\n",
       "   0.03285460946732201,\n",
       "   0.03182622980442829,\n",
       "   0.03134875276358798,\n",
       "   0.031150008863769472,\n",
       "   0.03100009169545956,\n",
       "   0.030441381342243404,\n",
       "   0.0299283946712967,\n",
       "   0.029528703308897092,\n",
       "   0.029266203587176277,\n",
       "   0.029147395823383705,\n",
       "   0.029009188670897856,\n",
       "   0.028873576001496985,\n",
       "   0.02889234761707485],\n",
       "  'running_test_loss': [0.0035004707043990494,\n",
       "   0.003135069577489048,\n",
       "   0.0030165418228134512,\n",
       "   0.002946849106810987,\n",
       "   0.0028999941679649054,\n",
       "   0.002865246401168406,\n",
       "   0.0028739190213382245,\n",
       "   0.0028971874192357063,\n",
       "   0.002725429316982627,\n",
       "   0.002716126154176891,\n",
       "   0.0027198770488612353,\n",
       "   0.002676389640197158,\n",
       "   0.002663435129914433,\n",
       "   0.0027151642772369087,\n",
       "   0.0026851189015433194]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'n_layers': 8,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.40745486550033094,\n",
       "   0.08562904183752834,\n",
       "   0.051022455622442064,\n",
       "   0.0332827136258129,\n",
       "   0.03064967774436809,\n",
       "   0.03022378823021427,\n",
       "   0.030186169140506536,\n",
       "   0.029823148712748663,\n",
       "   0.02973904718295671,\n",
       "   0.029713791304966434,\n",
       "   0.029545856169424952,\n",
       "   0.029286114678252487,\n",
       "   0.02928198284120299,\n",
       "   0.028913921517087146,\n",
       "   0.029057719907723368],\n",
       "  'running_test_loss': [0.00810503800585866,\n",
       "   0.00764934965223074,\n",
       "   0.003014931885059923,\n",
       "   0.002782137211412191,\n",
       "   0.002710142836906016,\n",
       "   0.0027613651216961445,\n",
       "   0.0029888175991363824,\n",
       "   0.002730060578323901,\n",
       "   0.002727261243853718,\n",
       "   0.002708926815073937,\n",
       "   0.002726471256930381,\n",
       "   0.002776898991316557,\n",
       "   0.002718616028316319,\n",
       "   0.002677414314355701,\n",
       "   0.002698187184520066]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'n_layers': 16,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.06943443797063083,\n",
       "   0.06415328348986804,\n",
       "   0.05006620450178161,\n",
       "   0.031458074151305485,\n",
       "   0.030057294525904582,\n",
       "   0.029249284212710336,\n",
       "   0.028944437282625586,\n",
       "   0.029026516110170632,\n",
       "   0.028980494077550246,\n",
       "   0.02882703540031798,\n",
       "   0.028958799543324858,\n",
       "   0.02889116156497039,\n",
       "   0.02897065395489335,\n",
       "   0.028847584776813165,\n",
       "   0.028831201549619438],\n",
       "  'running_test_loss': [0.006273954778909683,\n",
       "   0.006215669931843876,\n",
       "   0.0029316517594270407,\n",
       "   0.0029242744036018847,\n",
       "   0.0027040517046116293,\n",
       "   0.00273553020786494,\n",
       "   0.002744426330085844,\n",
       "   0.002677582547534257,\n",
       "   0.0027123746736906467,\n",
       "   0.0027190094674006105,\n",
       "   0.002659596375655383,\n",
       "   0.002686522575560957,\n",
       "   0.0026853415728546677,\n",
       "   0.0026916093844920395,\n",
       "   0.002701898099388927]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'n_layers': 32,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.42705619604792444,\n",
       "   0.03823608824517578,\n",
       "   0.034572966663399714,\n",
       "   0.03334661510423757,\n",
       "   0.03252995520248078,\n",
       "   0.031907130181789395,\n",
       "   0.03152807582053356,\n",
       "   0.03095910668023862,\n",
       "   0.030453612591372803,\n",
       "   0.03015955225098878,\n",
       "   0.030058685537660494,\n",
       "   0.02947350445901975,\n",
       "   0.029390049694338814,\n",
       "   0.029408012244384735,\n",
       "   0.029299922320060432],\n",
       "  'running_test_loss': [0.003947833243757486,\n",
       "   0.0033467540154233574,\n",
       "   0.003095868655014783,\n",
       "   0.00311090131662786,\n",
       "   0.0029385051908902824,\n",
       "   0.002957031782716513,\n",
       "   0.0029096815777011216,\n",
       "   0.0028661596183665095,\n",
       "   0.0028223140044137834,\n",
       "   0.0028495545671321453,\n",
       "   0.0027431480293162165,\n",
       "   0.0026957111470401287,\n",
       "   0.0028433327595703303,\n",
       "   0.0026964270770549774,\n",
       "   0.002693023299332708]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'n_layers': 4,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.07594526139087975,\n",
       "   0.06329428900033235,\n",
       "   0.04071530610090122,\n",
       "   0.033795777867781,\n",
       "   0.03286463349126279,\n",
       "   0.03241012779064476,\n",
       "   0.031883028659503904,\n",
       "   0.031498638798948375,\n",
       "   0.03156237570336089,\n",
       "   0.031313837605994196,\n",
       "   0.031212030975148082,\n",
       "   0.031090594250708817,\n",
       "   0.031010880124522373,\n",
       "   0.030883732901420446,\n",
       "   0.030921933577628805],\n",
       "  'running_test_loss': [0.006307997532188892,\n",
       "   0.005384027205407619,\n",
       "   0.003122980808839202,\n",
       "   0.003028659163042903,\n",
       "   0.0029970012037083505,\n",
       "   0.002981990728061646,\n",
       "   0.0029519274379126727,\n",
       "   0.0029325166908092796,\n",
       "   0.002938391039147973,\n",
       "   0.002902062759269029,\n",
       "   0.0029165025325492023,\n",
       "   0.0028778846920467912,\n",
       "   0.002881816797424108,\n",
       "   0.0028815463329665363,\n",
       "   0.0028483166354708375]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'n_layers': 8,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.09280269474256783,\n",
       "   0.09290456728544086,\n",
       "   0.09291490587871522,\n",
       "   0.0928972395369783,\n",
       "   0.0933173030614853,\n",
       "   0.092965491367504,\n",
       "   0.09307337473146618,\n",
       "   0.092973882262595,\n",
       "   0.09290522252209485,\n",
       "   0.09305930282454938,\n",
       "   0.0931589776230976,\n",
       "   0.09301071980502457,\n",
       "   0.09283238364849239,\n",
       "   0.0930285946186632,\n",
       "   0.0928602115670219],\n",
       "  'running_test_loss': [0.00879004817456007,\n",
       "   0.008796324003487826,\n",
       "   0.008801836369559168,\n",
       "   0.008802147526293993,\n",
       "   0.008791143909096718,\n",
       "   0.008775777900591492,\n",
       "   0.008764961451292038,\n",
       "   0.008786577671766282,\n",
       "   0.008784664237871767,\n",
       "   0.008796251464635134,\n",
       "   0.008788301369175315,\n",
       "   0.008757649021223187,\n",
       "   0.008774533070623874,\n",
       "   0.008811950186267495,\n",
       "   0.008786380497738718]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'n_layers': 16,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.05764673350146041,\n",
       "   0.03600729044759646,\n",
       "   0.034148644142551345,\n",
       "   0.03336986318579875,\n",
       "   0.032686549589270725,\n",
       "   0.03237084169872105,\n",
       "   0.03209356133593246,\n",
       "   0.03185564400162548,\n",
       "   0.03177913255640306,\n",
       "   0.03153008570545353,\n",
       "   0.03162681972142309,\n",
       "   0.031428511990234255,\n",
       "   0.03137044864706695,\n",
       "   0.03121944402111694,\n",
       "   0.031189704816788434],\n",
       "  'running_test_loss': [0.0034132069554179906,\n",
       "   0.0032022704589180647,\n",
       "   0.003081432898528874,\n",
       "   0.0030378584931604565,\n",
       "   0.0029981271536089478,\n",
       "   0.0029927930696867404,\n",
       "   0.0029449329460039734,\n",
       "   0.0029300011759623885,\n",
       "   0.00293701222166419,\n",
       "   0.0029310723436065016,\n",
       "   0.002954204512760043,\n",
       "   0.0029015134572982788,\n",
       "   0.0029313988531939686,\n",
       "   0.00287865326134488,\n",
       "   0.002872221983037889]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'n_layers': 32,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.08608438100200147,\n",
       "   0.06971037265844643,\n",
       "   0.06678991908207536,\n",
       "   0.05476022143382579,\n",
       "   0.03271245977259241,\n",
       "   0.029064817026956007,\n",
       "   0.028761438060319052,\n",
       "   0.028702591933542864,\n",
       "   0.02875002610962838,\n",
       "   0.028778215913334862,\n",
       "   0.028798104317393155,\n",
       "   0.028747123628854752,\n",
       "   0.028683759024133906,\n",
       "   0.028658403315348552,\n",
       "   0.028753394432133064],\n",
       "  'running_test_loss': [0.006735951466485858,\n",
       "   0.0064815640803426505,\n",
       "   0.006241855166852474,\n",
       "   0.0037039035549387336,\n",
       "   0.0027708590519614517,\n",
       "   0.002700858365278691,\n",
       "   0.0026803375645540655,\n",
       "   0.002694366634823382,\n",
       "   0.0026722425459884108,\n",
       "   0.0026995208295993505,\n",
       "   0.0026968063288368284,\n",
       "   0.002692897800821811,\n",
       "   0.002687353151384741,\n",
       "   0.002675751756411046,\n",
       "   0.002672290304210037]},\n",
       " {'bi_mamba_stacks': 2,\n",
       "  'n_layers': 4,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.04313845641794614,\n",
       "   0.030341875767335297,\n",
       "   0.02902410170645453,\n",
       "   0.028915980763267724,\n",
       "   0.028634223985718563,\n",
       "   0.027665003796573728,\n",
       "   0.026525082951411604,\n",
       "   0.026368019096553327,\n",
       "   0.025884888758882882,\n",
       "   0.025081545948050916,\n",
       "   0.024386052229674535,\n",
       "   0.024128309722291307,\n",
       "   0.02405280275270343,\n",
       "   0.024057049573748372,\n",
       "   0.024010792700573803],\n",
       "  'running_test_loss': [0.0030111480560153724,\n",
       "   0.002696950037032366,\n",
       "   0.0027309746709652245,\n",
       "   0.0026570160975679757,\n",
       "   0.002568908950779587,\n",
       "   0.0024387449654750524,\n",
       "   0.0023764920830726625,\n",
       "   0.0023434089659713207,\n",
       "   0.002280060098040849,\n",
       "   0.002220853708218783,\n",
       "   0.002149559191893786,\n",
       "   0.0021400477061979473,\n",
       "   0.002121868209913373,\n",
       "   0.0021315205548889933,\n",
       "   0.0021481720285955815]},\n",
       " {'bi_mamba_stacks': 2,\n",
       "  'n_layers': 8,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.05109867558348924,\n",
       "   0.030573754308279605,\n",
       "   0.029456570795737207,\n",
       "   0.028662784398766235,\n",
       "   0.02743609538418241,\n",
       "   0.02641175972064957,\n",
       "   0.025905840492341667,\n",
       "   0.025621844687266276,\n",
       "   0.025455981876002624,\n",
       "   0.025049399488489145,\n",
       "   0.02512936888029799,\n",
       "   0.02504471235210076,\n",
       "   0.0255774668965023,\n",
       "   0.025269339121878148,\n",
       "   0.02511224458226934],\n",
       "  'running_test_loss': [0.002841018695384264,\n",
       "   0.002729903289116919,\n",
       "   0.0026726391604170205,\n",
       "   0.002559678500518203,\n",
       "   0.0024498924473300575,\n",
       "   0.0023726107096299527,\n",
       "   0.0023440178125165405,\n",
       "   0.002372781068086624,\n",
       "   0.0022005894081667064,\n",
       "   0.0022014656704850495,\n",
       "   0.002182123178616166,\n",
       "   0.00224211270804517,\n",
       "   0.0022302729398943483,\n",
       "   0.0022763403421267865,\n",
       "   0.002247438901104033]},\n",
       " {'bi_mamba_stacks': 2,\n",
       "  'n_layers': 16,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.03188851788174361,\n",
       "   0.028935792042175307,\n",
       "   0.027827824603300542,\n",
       "   0.026282468741992488,\n",
       "   0.025249436002923176,\n",
       "   0.024522675197804347,\n",
       "   0.024281585035496392,\n",
       "   0.02409284790395759,\n",
       "   0.02419660878716968,\n",
       "   0.023975398199399933,\n",
       "   0.023782693662215023,\n",
       "   0.02362584855873138,\n",
       "   0.02370741340680979,\n",
       "   0.023839108668616972,\n",
       "   0.023980862903408708],\n",
       "  'running_test_loss': [0.0027161659775301814,\n",
       "   0.0026733447043225168,\n",
       "   0.002403954434208572,\n",
       "   0.0023202779078856112,\n",
       "   0.0022075929483398797,\n",
       "   0.002165608117589727,\n",
       "   0.0021656446158885958,\n",
       "   0.002168498998507857,\n",
       "   0.0022729051359929144,\n",
       "   0.002169540755916387,\n",
       "   0.002163327575195581,\n",
       "   0.0021227295119315387,\n",
       "   0.0021155165527015924,\n",
       "   0.0022530458993278444,\n",
       "   0.002182789743877947]},\n",
       " {'bi_mamba_stacks': 2,\n",
       "  'n_layers': 32,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.033500571014592424,\n",
       "   0.02861885315272957,\n",
       "   0.026324310844065623,\n",
       "   0.025296500625554472,\n",
       "   0.024780201142420993,\n",
       "   0.024396093882387504,\n",
       "   0.02428769855527207,\n",
       "   0.024264168015215547,\n",
       "   0.024097189345629887,\n",
       "   0.02373292344622314,\n",
       "   0.023579282556893302,\n",
       "   0.02395140490727499,\n",
       "   0.0237994842720218,\n",
       "   0.023667094482225367,\n",
       "   0.023586664377944544],\n",
       "  'running_test_loss': [0.002689115236978978,\n",
       "   0.0026819923692382873,\n",
       "   0.00229429646814242,\n",
       "   0.0021932569900527596,\n",
       "   0.0021698341760784385,\n",
       "   0.002187085041310638,\n",
       "   0.002137327635195106,\n",
       "   0.002104207064956427,\n",
       "   0.0021376308207400143,\n",
       "   0.0021099707931280134,\n",
       "   0.0020647443695925175,\n",
       "   0.0021046939785592258,\n",
       "   0.0020588459582068027,\n",
       "   0.002117907712003216,\n",
       "   0.0021090364158153534]},\n",
       " {'bi_mamba_stacks': 2,\n",
       "  'n_layers': 4,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.06331232223310508,\n",
       "   0.061092179036932065,\n",
       "   0.06043287773849443,\n",
       "   0.06001332503394224,\n",
       "   0.05964639682322741,\n",
       "   0.059880868057953195,\n",
       "   0.05942656536237337,\n",
       "   0.05949529055971652,\n",
       "   0.05957840534858406,\n",
       "   0.059463169223163276,\n",
       "   0.05948208021698519,\n",
       "   0.059670883694197985,\n",
       "   0.059488525114720685,\n",
       "   0.059582058975938705,\n",
       "   0.05972112976829522],\n",
       "  'running_test_loss': [0.0055597513457760215,\n",
       "   0.005458716141991317,\n",
       "   0.005412319944240153,\n",
       "   0.00538327380316332,\n",
       "   0.0054055379759520295,\n",
       "   0.005389193931594491,\n",
       "   0.005392273526638746,\n",
       "   0.00540551324468106,\n",
       "   0.005405210540164262,\n",
       "   0.005376535379560664,\n",
       "   0.005401674347231164,\n",
       "   0.005390716024208814,\n",
       "   0.0054452702426351605,\n",
       "   0.0054088167296722535,\n",
       "   0.005387611007783562]},\n",
       " {'bi_mamba_stacks': 2,\n",
       "  'n_layers': 8,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.04419754196656868,\n",
       "   0.030727212198544292,\n",
       "   0.029701698456192388,\n",
       "   0.029225194335449487,\n",
       "   0.02902734393835999,\n",
       "   0.028957833261229096,\n",
       "   0.028783078683773056,\n",
       "   0.028749788011191414,\n",
       "   0.028388037324184552,\n",
       "   0.028213993107201532,\n",
       "   0.028092283083242364,\n",
       "   0.028122330874903127,\n",
       "   0.027843359984690324,\n",
       "   0.02757983214687556,\n",
       "   0.027691047623520718],\n",
       "  'running_test_loss': [0.0028457935908809302,\n",
       "   0.002741214007139206,\n",
       "   0.0027045674943365155,\n",
       "   0.002696359115187079,\n",
       "   0.002696598713286221,\n",
       "   0.0026737348441965877,\n",
       "   0.0026589964986778796,\n",
       "   0.002646146124228835,\n",
       "   0.002592367934528738,\n",
       "   0.0025789938964881002,\n",
       "   0.002583370906766504,\n",
       "   0.002575839752331376,\n",
       "   0.0025318700396455823,\n",
       "   0.002541890464257449,\n",
       "   0.002551031499635428]},\n",
       " {'bi_mamba_stacks': 2,\n",
       "  'n_layers': 16,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.07170811881776899,\n",
       "   0.062476634425111116,\n",
       "   0.06204276892356574,\n",
       "   0.06148066781461239,\n",
       "   0.0612255583552178,\n",
       "   0.06093007060699165,\n",
       "   0.060677315321518106,\n",
       "   0.06052576610352844,\n",
       "   0.06014819984673522,\n",
       "   0.06008416046621278,\n",
       "   0.06000497748260386,\n",
       "   0.06002429640502669,\n",
       "   0.05986880983691663,\n",
       "   0.05961501786485315,\n",
       "   0.059402585378848016],\n",
       "  'running_test_loss': [0.005718745429068804,\n",
       "   0.005606583632994443,\n",
       "   0.005591099824290723,\n",
       "   0.005527291236910969,\n",
       "   0.005507868683431298,\n",
       "   0.005526981877628714,\n",
       "   0.005472962498199195,\n",
       "   0.005443310399539768,\n",
       "   0.0054844228727743026,\n",
       "   0.005446701530832797,\n",
       "   0.005411358737386763,\n",
       "   0.005423325993586332,\n",
       "   0.0054015245311893525,\n",
       "   0.005363930905703455,\n",
       "   0.005406364755704999]},\n",
       " {'bi_mamba_stacks': 2,\n",
       "  'n_layers': 32,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.06389871674356983,\n",
       "   0.036463123731082306,\n",
       "   0.03291365001583472,\n",
       "   0.03057490795501508,\n",
       "   0.029338829077314586,\n",
       "   0.028764769757399337,\n",
       "   0.028453737382078542,\n",
       "   0.028163326903013514,\n",
       "   0.027898840826237573,\n",
       "   0.027916551347589122,\n",
       "   0.027722124960273504,\n",
       "   0.027649954759981485,\n",
       "   0.027426259734202176,\n",
       "   0.02728080383618362,\n",
       "   0.027500246884301305],\n",
       "  'running_test_loss': [0.003343761392869055,\n",
       "   0.003036255670711398,\n",
       "   0.002803568786010146,\n",
       "   0.0027132004406303168,\n",
       "   0.0026955477497540414,\n",
       "   0.0025808546906337143,\n",
       "   0.0025611589406616987,\n",
       "   0.0025532077052630485,\n",
       "   0.0026782907648012044,\n",
       "   0.0025385327911935747,\n",
       "   0.0025978158987127243,\n",
       "   0.002542784208897501,\n",
       "   0.0025155371841974554,\n",
       "   0.0025012643886730074,\n",
       "   0.002474654764868319]},\n",
       " {'bi_mamba_stacks': 2,\n",
       "  'n_layers': 4,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.07479751138831489,\n",
       "   0.05975998674810398,\n",
       "   0.059533418305218216,\n",
       "   0.059639481758931655,\n",
       "   0.05948875047382898,\n",
       "   0.059551423574448564,\n",
       "   0.059656187893124296,\n",
       "   0.05968212403007783,\n",
       "   0.059767797261010856,\n",
       "   0.05964524901006371,\n",
       "   0.059466021362459286,\n",
       "   0.059502722695469856,\n",
       "   0.05954859922756441,\n",
       "   0.05947480530361645,\n",
       "   0.05963097259169445],\n",
       "  'running_test_loss': [0.0054141804564278575,\n",
       "   0.005368713904172182,\n",
       "   0.0054008087986148895,\n",
       "   0.0054058199664577845,\n",
       "   0.0053751673852093515,\n",
       "   0.005408428906928748,\n",
       "   0.005372804560465738,\n",
       "   0.0054098104797303675,\n",
       "   0.0053703188407234845,\n",
       "   0.0053837053054012355,\n",
       "   0.005407771650701761,\n",
       "   0.005382627240382135,\n",
       "   0.005403959202580154,\n",
       "   0.005407668417319655,\n",
       "   0.005373524073511362]},\n",
       " {'bi_mamba_stacks': 2,\n",
       "  'n_layers': 8,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.04862356676138006,\n",
       "   0.02904430960305035,\n",
       "   0.02878047984559089,\n",
       "   0.02877386612351984,\n",
       "   0.028856577281840146,\n",
       "   0.0288057466794271,\n",
       "   0.02867228553397581,\n",
       "   0.028809198136441408,\n",
       "   0.02883225446450524,\n",
       "   0.02875307055423036,\n",
       "   0.028764095556689427,\n",
       "   0.028770625817705876,\n",
       "   0.028739636678947137,\n",
       "   0.028737982532475143,\n",
       "   0.02882199517567642],\n",
       "  'running_test_loss': [0.00269141710922122,\n",
       "   0.002682954310439527,\n",
       "   0.002659868584945798,\n",
       "   0.002673558189533651,\n",
       "   0.0026969686658121646,\n",
       "   0.0027016710112802685,\n",
       "   0.002704188499599695,\n",
       "   0.0027436229810118675,\n",
       "   0.002697023214772344,\n",
       "   0.0027067276793532073,\n",
       "   0.0026893078424036505,\n",
       "   0.0026910649882629515,\n",
       "   0.002678690206259489,\n",
       "   0.00268017737288028,\n",
       "   0.0026740702120587228]},\n",
       " {'bi_mamba_stacks': 2,\n",
       "  'n_layers': 16,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.034772861073724924,\n",
       "   0.029246629566187038,\n",
       "   0.027109963308321314,\n",
       "   0.02666518301819451,\n",
       "   0.026186591095756738,\n",
       "   0.025944163199746982,\n",
       "   0.02589433902874589,\n",
       "   0.0257294918037951,\n",
       "   0.02582310326397419,\n",
       "   0.025362003933405503,\n",
       "   0.02554515704512596,\n",
       "   0.0253182505373843,\n",
       "   0.025527374075027184,\n",
       "   0.02555323842796497,\n",
       "   0.025354146555764602],\n",
       "  'running_test_loss': [0.0027903861911036075,\n",
       "   0.002536040031351149,\n",
       "   0.0024225847227498888,\n",
       "   0.0023452694644220174,\n",
       "   0.002371117674279958,\n",
       "   0.0023018103893846275,\n",
       "   0.002292940135113895,\n",
       "   0.002277980508748442,\n",
       "   0.0022819382352754474,\n",
       "   0.002282462078612298,\n",
       "   0.002286255347542465,\n",
       "   0.0023090268303640185,\n",
       "   0.0022768376930616795,\n",
       "   0.0022927325880154966,\n",
       "   0.0022643703278154136]},\n",
       " {'bi_mamba_stacks': 2,\n",
       "  'n_layers': 32,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.03761312674614601,\n",
       "   0.02893045995850116,\n",
       "   0.027942608988378196,\n",
       "   0.027292121357750148,\n",
       "   0.02614629238960333,\n",
       "   0.0255580765043851,\n",
       "   0.025181559487245975,\n",
       "   0.024911788232857363,\n",
       "   0.024981061295839026,\n",
       "   0.02519593295175582,\n",
       "   0.02484131691046059,\n",
       "   0.024757425368297847,\n",
       "   0.024818269072566183,\n",
       "   0.024609401951311158,\n",
       "   0.02467597239650786],\n",
       "  'running_test_loss': [0.002745447918307036,\n",
       "   0.002578365307301283,\n",
       "   0.002563024874776602,\n",
       "   0.0023460680465213954,\n",
       "   0.002250029247254133,\n",
       "   0.0022336192494258286,\n",
       "   0.0021903374018147586,\n",
       "   0.0021984058315865696,\n",
       "   0.002212739891372621,\n",
       "   0.002232037055538967,\n",
       "   0.0022002389235422017,\n",
       "   0.0021849924069829284,\n",
       "   0.0021776553019881247,\n",
       "   0.0021888196389190854,\n",
       "   0.002185293696820736]},\n",
       " {'bi_mamba_stacks': 2,\n",
       "  'n_layers': 4,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.04005962887080386,\n",
       "   0.034309233168605714,\n",
       "   0.03277067447314039,\n",
       "   0.03139160786638968,\n",
       "   0.030477646662620827,\n",
       "   0.030119618907338008,\n",
       "   0.029757889745524152,\n",
       "   0.02958138003828935,\n",
       "   0.02933627222198993,\n",
       "   0.029124338743276895,\n",
       "   0.02900036639184691,\n",
       "   0.028924318986246363,\n",
       "   0.028727831875439733,\n",
       "   0.028588803313905373,\n",
       "   0.028415317201288415],\n",
       "  'running_test_loss': [0.003173364169895649,\n",
       "   0.003078364299144596,\n",
       "   0.002928329123184085,\n",
       "   0.0028287708279676736,\n",
       "   0.002766802532132715,\n",
       "   0.002762626656331122,\n",
       "   0.002735920357052237,\n",
       "   0.0027128743841312824,\n",
       "   0.0027207463206723333,\n",
       "   0.002684917320031673,\n",
       "   0.002685362415853888,\n",
       "   0.002646649079397321,\n",
       "   0.002642847415059805,\n",
       "   0.0026452763238921763,\n",
       "   0.0026071413084864615]},\n",
       " {'bi_mamba_stacks': 2,\n",
       "  'n_layers': 8,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.04791595987509936,\n",
       "   0.03559473161934875,\n",
       "   0.032877993014408274,\n",
       "   0.03147437881329097,\n",
       "   0.030474074240773915,\n",
       "   0.02981016018311493,\n",
       "   0.02933983226539567,\n",
       "   0.0289602439571172,\n",
       "   0.028666265219217165,\n",
       "   0.028417126398999244,\n",
       "   0.028183561513433233,\n",
       "   0.028079925957135855,\n",
       "   0.027880874740658326,\n",
       "   0.027713388997362927,\n",
       "   0.02744012653711252],\n",
       "  'running_test_loss': [0.003133690296206623,\n",
       "   0.002852319664787501,\n",
       "   0.0027720523960888385,\n",
       "   0.0026809345902875066,\n",
       "   0.0026799462116323413,\n",
       "   0.0026435916884802284,\n",
       "   0.002628522894810885,\n",
       "   0.002622820726595819,\n",
       "   0.002602806576527655,\n",
       "   0.0025644900617189706,\n",
       "   0.002545552965719253,\n",
       "   0.002540681203827262,\n",
       "   0.0025042053000070156,\n",
       "   0.0024990192865952847,\n",
       "   0.002459219909273088]},\n",
       " {'bi_mamba_stacks': 2,\n",
       "  'n_layers': 16,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.04929562121629715,\n",
       "   0.035653890919638796,\n",
       "   0.03407426219433546,\n",
       "   0.032788358305115256,\n",
       "   0.031930059205042195,\n",
       "   0.031115385227603837,\n",
       "   0.030348668289370835,\n",
       "   0.029652960761450232,\n",
       "   0.02943645410705358,\n",
       "   0.02914010494481772,\n",
       "   0.029101790101267396,\n",
       "   0.028988784349057824,\n",
       "   0.02899009003303945,\n",
       "   0.028879656231729313,\n",
       "   0.028926779609173538],\n",
       "  'running_test_loss': [0.0033875214187428357,\n",
       "   0.0032155255149118604,\n",
       "   0.0030335256713442506,\n",
       "   0.0029053339739330115,\n",
       "   0.002823777746874839,\n",
       "   0.002785217334982008,\n",
       "   0.0027118143406696616,\n",
       "   0.002708621812518686,\n",
       "   0.002688350641634315,\n",
       "   0.0026783916796557605,\n",
       "   0.0026690815878100695,\n",
       "   0.002672297931741923,\n",
       "   0.0026763667641207575,\n",
       "   0.00268721565278247,\n",
       "   0.0026938844295218587]},\n",
       " {'bi_mamba_stacks': 2,\n",
       "  'n_layers': 32,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.056497037617955354,\n",
       "   0.03524424290284514,\n",
       "   0.03280658238567412,\n",
       "   0.03165569973527454,\n",
       "   0.030816687664482742,\n",
       "   0.030110749177401884,\n",
       "   0.029749448572983964,\n",
       "   0.029447118298849093,\n",
       "   0.02928735322901048,\n",
       "   0.029182977324817328,\n",
       "   0.029024640480056405,\n",
       "   0.029032034354750068,\n",
       "   0.028934525831136853,\n",
       "   0.028900444670580328,\n",
       "   0.028949291323078795],\n",
       "  'running_test_loss': [0.0034344232780858875,\n",
       "   0.0031207770039327442,\n",
       "   0.00293899397412315,\n",
       "   0.0028676693770103155,\n",
       "   0.0028214089451357722,\n",
       "   0.0027637499906122682,\n",
       "   0.002741714347153902,\n",
       "   0.002726118332706392,\n",
       "   0.0027261979430913924,\n",
       "   0.002704167124349624,\n",
       "   0.0027014104831032454,\n",
       "   0.0026850742087699475,\n",
       "   0.0027068335223011672,\n",
       "   0.0027069147983565926,\n",
       "   0.0026858759433962404]},\n",
       " {'bi_mamba_stacks': 3,\n",
       "  'n_layers': 4,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.06249127218034119,\n",
       "   0.059321588589809834,\n",
       "   0.05821327970828861,\n",
       "   0.05789119350723922,\n",
       "   0.057628164598718286,\n",
       "   0.05700970431324095,\n",
       "   0.05707662676461041,\n",
       "   0.05677474706433713,\n",
       "   0.056723652547225355,\n",
       "   0.05651336160954088,\n",
       "   0.056568921469151975,\n",
       "   0.05644286100752652,\n",
       "   0.056434073210693894,\n",
       "   0.056258872207254174,\n",
       "   0.05640072986949235],\n",
       "  'running_test_loss': [0.00581538832001388,\n",
       "   0.005609488489106297,\n",
       "   0.00555906468257308,\n",
       "   0.0055384879987686875,\n",
       "   0.00548955812305212,\n",
       "   0.005515431869775057,\n",
       "   0.0054871133454144,\n",
       "   0.00545951059833169,\n",
       "   0.005418551098555327,\n",
       "   0.005389210734516382,\n",
       "   0.0054627902116626504,\n",
       "   0.0054410028941929344,\n",
       "   0.00541103863529861,\n",
       "   0.005428212184458971,\n",
       "   0.005404966603964567]},\n",
       " {'bi_mamba_stacks': 3,\n",
       "  'n_layers': 8,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.03080472492147237,\n",
       "   0.026785692629637196,\n",
       "   0.025324775878107175,\n",
       "   0.02426221961854026,\n",
       "   0.023793814178789036,\n",
       "   0.023420501820510253,\n",
       "   0.023117847504327074,\n",
       "   0.023111365709919483,\n",
       "   0.02297982739750296,\n",
       "   0.022681490563554688,\n",
       "   0.022541390159749427,\n",
       "   0.022446264954051002,\n",
       "   0.022385581916896626,\n",
       "   0.02227683149278164,\n",
       "   0.022287941926624626],\n",
       "  'running_test_loss': [0.0025296489503234625,\n",
       "   0.0023322012447752056,\n",
       "   0.00222977883531712,\n",
       "   0.002134639464085922,\n",
       "   0.002139182950602844,\n",
       "   0.0020696186595596374,\n",
       "   0.0020666915248148143,\n",
       "   0.0020312112253159285,\n",
       "   0.0020307713393121958,\n",
       "   0.0019784595263190566,\n",
       "   0.0019765678131952883,\n",
       "   0.001965786523418501,\n",
       "   0.001989194731693715,\n",
       "   0.0019728074164595457,\n",
       "   0.0019164256167132408]},\n",
       " {'bi_mamba_stacks': 3,\n",
       "  'n_layers': 16,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.03933236445882358,\n",
       "   0.02607037236331962,\n",
       "   0.024967948151752354,\n",
       "   0.024602582223014906,\n",
       "   0.024320404942845927,\n",
       "   0.024357232830952855,\n",
       "   0.023904802778270097,\n",
       "   0.023832124336622657,\n",
       "   0.023815767375053837,\n",
       "   0.023887373830657452,\n",
       "   0.02342003176221624,\n",
       "   0.023493027494987474,\n",
       "   0.023476575212553145,\n",
       "   0.023394634240539745,\n",
       "   0.02333005128079094],\n",
       "  'running_test_loss': [0.002648596750572324,\n",
       "   0.0022386478343978523,\n",
       "   0.0022081675387453288,\n",
       "   0.0021767809917218983,\n",
       "   0.002229107762686908,\n",
       "   0.0021842520851641893,\n",
       "   0.0021577756812330333,\n",
       "   0.0021429959456436337,\n",
       "   0.0021202256185933947,\n",
       "   0.0020935992680024356,\n",
       "   0.002132711344398558,\n",
       "   0.0021317491561640054,\n",
       "   0.0021028015478514136,\n",
       "   0.0020886190040037035,\n",
       "   0.002062746997922659]},\n",
       " {'bi_mamba_stacks': 3,\n",
       "  'n_layers': 32,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.09294087905436754,\n",
       "   0.09274679159745575,\n",
       "   0.09289795422926544,\n",
       "   0.09295158309396356,\n",
       "   0.09271576331462711,\n",
       "   0.09281192151829601,\n",
       "   0.09326451476197689,\n",
       "   0.09286141929216683,\n",
       "   0.09300699217710644,\n",
       "   0.09285488601773978,\n",
       "   0.0930772639112547,\n",
       "   0.09292935739737004,\n",
       "   0.09314512046519667,\n",
       "   0.09313456950243562,\n",
       "   0.09316335764713585],\n",
       "  'running_test_loss': [0.008781698733568192,\n",
       "   0.00879407019354403,\n",
       "   0.008817836429923773,\n",
       "   0.008803196139633656,\n",
       "   0.008793540136888623,\n",
       "   0.008723419070243835,\n",
       "   0.008799729032441974,\n",
       "   0.008778526751324534,\n",
       "   0.008821711158379913,\n",
       "   0.008783159786835313,\n",
       "   0.00880211847834289,\n",
       "   0.008789151115342975,\n",
       "   0.00878779087215662,\n",
       "   0.008784251252189278,\n",
       "   0.008743615191429853]},\n",
       " {'bi_mamba_stacks': 3,\n",
       "  'n_layers': 4,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.06353060120251029,\n",
       "   0.061115114082349466,\n",
       "   0.06014823604491539,\n",
       "   0.05999656634172425,\n",
       "   0.05966284188092686,\n",
       "   0.059604782592505214,\n",
       "   0.059343901786487546,\n",
       "   0.05965830133762211,\n",
       "   0.059580049155047166,\n",
       "   0.05939319643366616,\n",
       "   0.05955163704813458,\n",
       "   0.059395148468902335,\n",
       "   0.05974806566839106,\n",
       "   0.05974438173929229,\n",
       "   0.05943017238634638],\n",
       "  'running_test_loss': [0.005597231922205538,\n",
       "   0.005424174503423274,\n",
       "   0.005453327296301723,\n",
       "   0.005416214765049518,\n",
       "   0.0053754085786640645,\n",
       "   0.00541506653605029,\n",
       "   0.005421573060099036,\n",
       "   0.005376347593031823,\n",
       "   0.005368616280844435,\n",
       "   0.005386333525646478,\n",
       "   0.005414470047689975,\n",
       "   0.005385587948840111,\n",
       "   0.0053899812404997645,\n",
       "   0.005391550756059587,\n",
       "   0.005348516495432705]},\n",
       " {'bi_mamba_stacks': 3,\n",
       "  'n_layers': 8,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.04235482910182327,\n",
       "   0.03214760375092737,\n",
       "   0.030202501334715633,\n",
       "   0.029429174149408936,\n",
       "   0.028882319257827475,\n",
       "   0.028785869583953173,\n",
       "   0.02868126898421906,\n",
       "   0.02853726651868783,\n",
       "   0.028539534487063065,\n",
       "   0.028326983770821244,\n",
       "   0.028040090253343804,\n",
       "   0.02779095050180331,\n",
       "   0.027655264547793195,\n",
       "   0.027426057569682596,\n",
       "   0.027096519075566903],\n",
       "  'running_test_loss': [0.003073004005011171,\n",
       "   0.0028015816775150597,\n",
       "   0.002727849936578423,\n",
       "   0.002687131093814969,\n",
       "   0.0026749009126797317,\n",
       "   0.0026707453574053943,\n",
       "   0.0026781394868157803,\n",
       "   0.0026379791675135494,\n",
       "   0.002633294910658151,\n",
       "   0.0026049197264946997,\n",
       "   0.002558534079231322,\n",
       "   0.0025474317129701375,\n",
       "   0.00250467669358477,\n",
       "   0.002488915109541267,\n",
       "   0.002476972707081586]},\n",
       " {'bi_mamba_stacks': 3,\n",
       "  'n_layers': 16,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.08010344626381993,\n",
       "   0.06725025504827499,\n",
       "   0.06469178662169725,\n",
       "   0.0643759138090536,\n",
       "   0.06393207288812847,\n",
       "   0.06215046138502658,\n",
       "   0.060406232783570886,\n",
       "   0.060034112758003175,\n",
       "   0.05960448231548071,\n",
       "   0.05929432765115052,\n",
       "   0.05896749393548816,\n",
       "   0.05848182762041688,\n",
       "   0.05799583001993597,\n",
       "   0.05752815764863044,\n",
       "   0.05741746068932116],\n",
       "  'running_test_loss': [0.006540533341467381,\n",
       "   0.006122738592326641,\n",
       "   0.006110581856220961,\n",
       "   0.006058064267039299,\n",
       "   0.00605231305770576,\n",
       "   0.006014099253341555,\n",
       "   0.0059934159126132725,\n",
       "   0.00595064559020102,\n",
       "   0.005902970543131232,\n",
       "   0.00586808779463172,\n",
       "   0.005851360999047756,\n",
       "   0.005782566707581282,\n",
       "   0.005725730931386352,\n",
       "   0.005702193684875965,\n",
       "   0.005700443547219038]},\n",
       " {'bi_mamba_stacks': 3,\n",
       "  'n_layers': 32,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.09307399733457715,\n",
       "   0.09313817411195487,\n",
       "   0.09288885529618711,\n",
       "   0.09303314931690693,\n",
       "   0.09298571932129562,\n",
       "   0.09284008848015218,\n",
       "   0.09286760278046131,\n",
       "   0.09292026731185615,\n",
       "   0.09305036479141564,\n",
       "   0.09302751998882741,\n",
       "   0.09295605469960719,\n",
       "   0.09293102256953717,\n",
       "   0.09281504156533629,\n",
       "   0.0927998213795945,\n",
       "   0.0929544767132029],\n",
       "  'running_test_loss': [0.008771635921671987,\n",
       "   0.008762145670130849,\n",
       "   0.0087943284958601,\n",
       "   0.0087958352137357,\n",
       "   0.008804328445345164,\n",
       "   0.008794788770377636,\n",
       "   0.008743435738608241,\n",
       "   0.008771805291995406,\n",
       "   0.008795815853402018,\n",
       "   0.00879218832962215,\n",
       "   0.008777650827541948,\n",
       "   0.008780048124492169,\n",
       "   0.008721705622971058,\n",
       "   0.008768173085525632,\n",
       "   0.008829189399257302]},\n",
       " {'bi_mamba_stacks': 3,\n",
       "  'n_layers': 4,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.043819548160536216,\n",
       "   0.028077040866483004,\n",
       "   0.0264408395672217,\n",
       "   0.025522134492639452,\n",
       "   0.025229706576792525,\n",
       "   0.024919559792615474,\n",
       "   0.025049571527633815,\n",
       "   0.02487180845346302,\n",
       "   0.024682603653054684,\n",
       "   0.024436748352600263,\n",
       "   0.02446770907728933,\n",
       "   0.024301553220720962,\n",
       "   0.024176303908461705,\n",
       "   0.024198516827309505,\n",
       "   0.024112533524166792],\n",
       "  'running_test_loss': [0.002750304592307657,\n",
       "   0.002368157535791397,\n",
       "   0.0022879280531778933,\n",
       "   0.0021972437826916573,\n",
       "   0.0022197396142873914,\n",
       "   0.002192567731719464,\n",
       "   0.0022303350190632045,\n",
       "   0.002289331779349595,\n",
       "   0.0022161627672612665,\n",
       "   0.0021445522815920414,\n",
       "   0.002151063439901918,\n",
       "   0.002150820499751717,\n",
       "   0.002159248549491167,\n",
       "   0.0021579590225592254,\n",
       "   0.002161405751015991]},\n",
       " {'bi_mamba_stacks': 3,\n",
       "  'n_layers': 8,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.030252695619128644,\n",
       "   0.027310679802903904,\n",
       "   0.025488628023304045,\n",
       "   0.025256110130576416,\n",
       "   0.02510970369097777,\n",
       "   0.025088164506014435,\n",
       "   0.02502828437485732,\n",
       "   0.02515925405663438,\n",
       "   0.024855804627295582,\n",
       "   0.024827754949219526,\n",
       "   0.02462872624048032,\n",
       "   0.024636231458280235,\n",
       "   0.024662012687185778,\n",
       "   0.02456563788000494,\n",
       "   0.02446483290870674],\n",
       "  'running_test_loss': [0.002700803017709404,\n",
       "   0.002276713515166193,\n",
       "   0.002256601388566196,\n",
       "   0.002263334792573005,\n",
       "   0.0022497487142682077,\n",
       "   0.002205554478103295,\n",
       "   0.002178107358049601,\n",
       "   0.002187622460536659,\n",
       "   0.0021848276550881563,\n",
       "   0.0021830084070097655,\n",
       "   0.0021682029450312257,\n",
       "   0.0021635731239803137,\n",
       "   0.002172042032238096,\n",
       "   0.0021466493313200773,\n",
       "   0.0022375128837302328]},\n",
       " {'bi_mamba_stacks': 3,\n",
       "  'n_layers': 16,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.06386236523278058,\n",
       "   0.032418900516349825,\n",
       "   0.02740132490522228,\n",
       "   0.02646047162474133,\n",
       "   0.025896846463438122,\n",
       "   0.025672353124246,\n",
       "   0.025458697353024036,\n",
       "   0.025304694140795617,\n",
       "   0.025444712758762762,\n",
       "   0.025304019182221964,\n",
       "   0.025195536240935325,\n",
       "   0.024930489918915555,\n",
       "   0.024938790585147218,\n",
       "   0.024944566694321112,\n",
       "   0.025014231922104956],\n",
       "  'running_test_loss': [0.0060559900794178246,\n",
       "   0.002587951008230448,\n",
       "   0.002468990330584347,\n",
       "   0.002456625799648464,\n",
       "   0.0023676699954085054,\n",
       "   0.00231315560080111,\n",
       "   0.0023667005975730715,\n",
       "   0.0023605919945985077,\n",
       "   0.002271396710537374,\n",
       "   0.0022861698265187443,\n",
       "   0.002200890544336289,\n",
       "   0.002251841186080128,\n",
       "   0.0022523719924502075,\n",
       "   0.0022256214045919477,\n",
       "   0.002245125547517091]},\n",
       " {'bi_mamba_stacks': 3,\n",
       "  'n_layers': 32,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.06310077784582972,\n",
       "   0.06026887674350292,\n",
       "   0.05936230518389493,\n",
       "   0.05892824689857662,\n",
       "   0.05860667038243264,\n",
       "   0.05852196803782135,\n",
       "   0.0582994281174615,\n",
       "   0.0580994319031015,\n",
       "   0.05816953811328858,\n",
       "   0.0578214584197849,\n",
       "   0.057990164081566035,\n",
       "   0.05800517364870757,\n",
       "   0.057901874864473936,\n",
       "   0.05806784674525261,\n",
       "   0.058074107449501755],\n",
       "  'running_test_loss': [0.005823387051001191,\n",
       "   0.005638670116662979,\n",
       "   0.0056073356959968805,\n",
       "   0.005640191921964288,\n",
       "   0.005591649789363146,\n",
       "   0.005578993389382958,\n",
       "   0.005567496104165912,\n",
       "   0.005610934048891067,\n",
       "   0.005558303939178586,\n",
       "   0.005576167356222868,\n",
       "   0.005535666236653924,\n",
       "   0.005540714997798205,\n",
       "   0.005553094245493412,\n",
       "   0.005560560723766685,\n",
       "   0.005520304303616285]},\n",
       " {'bi_mamba_stacks': 3,\n",
       "  'n_layers': 4,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.05013360567390919,\n",
       "   0.03277848165947944,\n",
       "   0.03174453158397227,\n",
       "   0.03135386003647,\n",
       "   0.030628353792708368,\n",
       "   0.0303100328030996,\n",
       "   0.029848930943990125,\n",
       "   0.029215880120173097,\n",
       "   0.029003880609525367,\n",
       "   0.02882527169655077,\n",
       "   0.02890192854218185,\n",
       "   0.028812852955888958,\n",
       "   0.02875936710741371,\n",
       "   0.028721791799180208,\n",
       "   0.028802681777160614],\n",
       "  'running_test_loss': [0.0030131031000055373,\n",
       "   0.002964946186635643,\n",
       "   0.002912407766561955,\n",
       "   0.0028547575743868947,\n",
       "   0.002855981431435794,\n",
       "   0.002798803312703967,\n",
       "   0.0027791468189097943,\n",
       "   0.002696273038163781,\n",
       "   0.0027016865997575223,\n",
       "   0.0027011853181757033,\n",
       "   0.00269309607706964,\n",
       "   0.002670924087520689,\n",
       "   0.0026786115039139986,\n",
       "   0.002717135223094374,\n",
       "   0.002691645920742303]},\n",
       " {'bi_mamba_stacks': 3,\n",
       "  'n_layers': 8,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.06728687002323568,\n",
       "   0.06461721308529376,\n",
       "   0.0636326973605901,\n",
       "   0.06287538535892963,\n",
       "   0.061028107586316764,\n",
       "   0.05982449139002711,\n",
       "   0.059531971369870006,\n",
       "   0.05933371884282678,\n",
       "   0.05915017219725996,\n",
       "   0.05879974014125764,\n",
       "   0.0588086396548897,\n",
       "   0.05620954024838284,\n",
       "   0.03532578721176833,\n",
       "   0.029825846552848815,\n",
       "   0.028864772364031524],\n",
       "  'running_test_loss': [0.006347246812656522,\n",
       "   0.006210084406659007,\n",
       "   0.006127585276961327,\n",
       "   0.006094294134527445,\n",
       "   0.006089667478576303,\n",
       "   0.0060787798929959535,\n",
       "   0.006095144670456648,\n",
       "   0.006056226592510938,\n",
       "   0.006046458816155792,\n",
       "   0.006059257030487061,\n",
       "   0.006039454158395529,\n",
       "   0.004380811586976051,\n",
       "   0.002974280063994229,\n",
       "   0.0027063340162858367,\n",
       "   0.0026663875179365275]},\n",
       " {'bi_mamba_stacks': 3,\n",
       "  'n_layers': 16,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.039223894943716,\n",
       "   0.03243076868471689,\n",
       "   0.03053807229385711,\n",
       "   0.029592682824004443,\n",
       "   0.029133442303864287,\n",
       "   0.029082584128482268,\n",
       "   0.028876131639117376,\n",
       "   0.028931912130210548,\n",
       "   0.028917211460648105,\n",
       "   0.028876642461400478,\n",
       "   0.028822112205671147,\n",
       "   0.028759050472872333,\n",
       "   0.028835425466531887,\n",
       "   0.02858490582671948,\n",
       "   0.02827583810198121],\n",
       "  'running_test_loss': [0.002965059308335185,\n",
       "   0.0027464334298856557,\n",
       "   0.0026896179015748205,\n",
       "   0.002685969652608037,\n",
       "   0.002709558660630137,\n",
       "   0.002667205420322716,\n",
       "   0.002694103763438761,\n",
       "   0.0026883429908193647,\n",
       "   0.0026915821498259902,\n",
       "   0.0026902448497712612,\n",
       "   0.0026802108357660472,\n",
       "   0.002674308734945953,\n",
       "   0.0026819448280148207,\n",
       "   0.002629466678481549,\n",
       "   0.0025807452467270196]},\n",
       " {'bi_mamba_stacks': 3,\n",
       "  'n_layers': 32,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.05693066786276177,\n",
       "   0.039665187748614696,\n",
       "   0.03662970177829265,\n",
       "   0.0332144295796752,\n",
       "   0.03135561082395725,\n",
       "   0.030581613876856862,\n",
       "   0.030025859387824313,\n",
       "   0.029811089406721294,\n",
       "   0.029529502062359825,\n",
       "   0.029289567304076627,\n",
       "   0.028699512702878564,\n",
       "   0.028180680534569548,\n",
       "   0.027744554051896558,\n",
       "   0.02767455775872804,\n",
       "   0.02728733807685785],\n",
       "  'running_test_loss': [0.003735314500518143,\n",
       "   0.0035392022533342242,\n",
       "   0.003050763907842338,\n",
       "   0.0027970178779214623,\n",
       "   0.0027083472604863346,\n",
       "   0.002707851328421384,\n",
       "   0.002698075010906905,\n",
       "   0.0026872995351441206,\n",
       "   0.002665045398287475,\n",
       "   0.00261389651754871,\n",
       "   0.0025396943357773125,\n",
       "   0.00247794354474172,\n",
       "   0.002447512799873948,\n",
       "   0.002441853879485279,\n",
       "   0.002424464388284832]},\n",
       " {'bi_mamba_stacks': 4,\n",
       "  'n_layers': 4,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.030608021222287788,\n",
       "   0.0254543945833575,\n",
       "   0.024704368842067198,\n",
       "   0.024619142306037248,\n",
       "   0.024556163798552008,\n",
       "   0.02445349744753912,\n",
       "   0.024263356939191,\n",
       "   0.024448172703851013,\n",
       "   0.024307101571466774,\n",
       "   0.024222214242909105,\n",
       "   0.02412169630988501,\n",
       "   0.024212929219938815,\n",
       "   0.024073255744879132,\n",
       "   0.023869037764379753,\n",
       "   0.02385475831455551],\n",
       "  'running_test_loss': [0.00240054695867002,\n",
       "   0.002183822894934565,\n",
       "   0.002170897400472313,\n",
       "   0.00218543257471174,\n",
       "   0.002187148841563612,\n",
       "   0.0021330547211691735,\n",
       "   0.0021685156314633787,\n",
       "   0.0021598330000415444,\n",
       "   0.0021861301702447234,\n",
       "   0.002155344888102263,\n",
       "   0.002169950136914849,\n",
       "   0.0021814977745525537,\n",
       "   0.0020985112986527383,\n",
       "   0.0021258861599490046,\n",
       "   0.0021099539604038]},\n",
       " {'bi_mamba_stacks': 4,\n",
       "  'n_layers': 8,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.06299594671931118,\n",
       "   0.061230853749439124,\n",
       "   0.05852619554847479,\n",
       "   0.05769963979255408,\n",
       "   0.057453773724846544,\n",
       "   0.05695925257634372,\n",
       "   0.05652095836121589,\n",
       "   0.05706863992847502,\n",
       "   0.056937751430086794,\n",
       "   0.0567344180541113,\n",
       "   0.056709468527697024,\n",
       "   0.056461900901049376,\n",
       "   0.05644215113017708,\n",
       "   0.056346246083267035,\n",
       "   0.056155285160057244],\n",
       "  'running_test_loss': [0.006065553171560168,\n",
       "   0.005732176434248686,\n",
       "   0.005554712112993002,\n",
       "   0.00555275553278625,\n",
       "   0.00555983198992908,\n",
       "   0.005503017166629434,\n",
       "   0.005439257893711328,\n",
       "   0.005509466912597418,\n",
       "   0.005496504582464695,\n",
       "   0.005483020026236772,\n",
       "   0.005480343664065003,\n",
       "   0.005442495906725526,\n",
       "   0.005406575443223119,\n",
       "   0.005472153063863516,\n",
       "   0.00534771160967648]},\n",
       " {'bi_mamba_stacks': 4,\n",
       "  'n_layers': 16,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.06293748372234403,\n",
       "   0.062151763672009114,\n",
       "   0.06215382555034012,\n",
       "   0.05065184869454242,\n",
       "   0.02531926426803693,\n",
       "   0.02461624103714712,\n",
       "   0.02423869044519961,\n",
       "   0.024126872880151494,\n",
       "   0.023923630137578585,\n",
       "   0.02381050323951058,\n",
       "   0.02355205922969617,\n",
       "   0.023639542796299793,\n",
       "   0.023713612504070624,\n",
       "   0.0235612989065703,\n",
       "   0.023519074922660366],\n",
       "  'running_test_loss': [0.00607985651306808,\n",
       "   0.006073997914791107,\n",
       "   0.006000654285773635,\n",
       "   0.0023310183715075255,\n",
       "   0.0022339049871079623,\n",
       "   0.002174532397184521,\n",
       "   0.002221052492270246,\n",
       "   0.0021683369078673424,\n",
       "   0.00220412709005177,\n",
       "   0.002150244670221582,\n",
       "   0.0021412254339084028,\n",
       "   0.0021337012911681087,\n",
       "   0.0021364500680938364,\n",
       "   0.002118312912993133,\n",
       "   0.0021126456977799534]},\n",
       " {'bi_mamba_stacks': 4,\n",
       "  'n_layers': 32,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.061421887903707104,\n",
       "   0.059517030287533995,\n",
       "   0.059528447464108465,\n",
       "   0.05955839548143558,\n",
       "   0.05963569864048623,\n",
       "   0.059352820904459806,\n",
       "   0.05940919426269829,\n",
       "   0.059279552216175944,\n",
       "   0.05923993589007295,\n",
       "   0.05930224252399057,\n",
       "   0.05926481937873177,\n",
       "   0.059360622100066394,\n",
       "   0.05939746784279123,\n",
       "   0.05937023385777138,\n",
       "   0.05949726065911818],\n",
       "  'running_test_loss': [0.005385593729093671,\n",
       "   0.005404112365562469,\n",
       "   0.005410640303511172,\n",
       "   0.00538717878703028,\n",
       "   0.0054040870014578106,\n",
       "   0.005384440348483622,\n",
       "   0.00537204483198002,\n",
       "   0.005360997707117349,\n",
       "   0.005339600848965347,\n",
       "   0.005323074384592474,\n",
       "   0.005375139816198498,\n",
       "   0.005392772573977709,\n",
       "   0.005392473589396104,\n",
       "   0.005353399111423641,\n",
       "   0.00535304661700502]},\n",
       " {'bi_mamba_stacks': 4,\n",
       "  'n_layers': 4,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.09285360478330404,\n",
       "   0.09292102286592126,\n",
       "   0.09292060411069542,\n",
       "   0.09280461754184216,\n",
       "   0.09308554984163493,\n",
       "   0.09294406452216208,\n",
       "   0.09284951435402036,\n",
       "   0.09313441916368902,\n",
       "   0.09287637762725354,\n",
       "   0.09299300957936793,\n",
       "   0.09305802970658988,\n",
       "   0.09305746679659933,\n",
       "   0.09308419199194759,\n",
       "   0.09308209631592035,\n",
       "   0.09306989246513694],\n",
       "  'running_test_loss': [0.00880152610130608,\n",
       "   0.00878216896392405,\n",
       "   0.008779489336535334,\n",
       "   0.00880542928352952,\n",
       "   0.008794893315061926,\n",
       "   0.008812931252643466,\n",
       "   0.008772117329761386,\n",
       "   0.008778559993952512,\n",
       "   0.00879108957760036,\n",
       "   0.008774866120889784,\n",
       "   0.00876757925748825,\n",
       "   0.008789440367370845,\n",
       "   0.008794244900345803,\n",
       "   0.00879700973443687,\n",
       "   0.008846407493576408]},\n",
       " {'bi_mamba_stacks': 4,\n",
       "  'n_layers': 8,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.06988647461752408,\n",
       "   0.060394459347007795,\n",
       "   0.05999582028831355,\n",
       "   0.059896464190096595,\n",
       "   0.05971835658710915,\n",
       "   0.059602978837210684,\n",
       "   0.05947846054565162,\n",
       "   0.05951470422092825,\n",
       "   0.05967436395585537,\n",
       "   0.056693940809345804,\n",
       "   0.03412093306949828,\n",
       "   0.027904397858073936,\n",
       "   0.02719100221991539,\n",
       "   0.02632894300739281,\n",
       "   0.025753754138713704],\n",
       "  'running_test_loss': [0.005659217320848256,\n",
       "   0.005410206141415984,\n",
       "   0.00542085546720773,\n",
       "   0.00541863315878436,\n",
       "   0.005395117160398513,\n",
       "   0.005359697624109685,\n",
       "   0.00540137885697186,\n",
       "   0.005434572647791356,\n",
       "   0.005380258886143565,\n",
       "   0.0038506904446985574,\n",
       "   0.002652837738394737,\n",
       "   0.0025261495634913444,\n",
       "   0.002415757673792541,\n",
       "   0.002334622086957097,\n",
       "   0.0023010571524500847]},\n",
       " {'bi_mamba_stacks': 4,\n",
       "  'n_layers': 16,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.09282433106098324,\n",
       "   0.09277925494592637,\n",
       "   0.0929678453039378,\n",
       "   0.09332966498564929,\n",
       "   0.09309139663819224,\n",
       "   0.09301194474101067,\n",
       "   0.09321813294664025,\n",
       "   0.09285795769654215,\n",
       "   0.09309991417918355,\n",
       "   0.09294449724722653,\n",
       "   0.0929033917747438,\n",
       "   0.09278658218681812,\n",
       "   0.09269520030822605,\n",
       "   0.09280606708023698,\n",
       "   0.09277878930792212],\n",
       "  'running_test_loss': [0.008777821393683553,\n",
       "   0.008772265991196036,\n",
       "   0.008779123783111572,\n",
       "   0.0088056434802711,\n",
       "   0.008832757258787752,\n",
       "   0.008760469669476151,\n",
       "   0.008791144201532007,\n",
       "   0.008778159463778138,\n",
       "   0.00876451893337071,\n",
       "   0.008752805234864354,\n",
       "   0.008828584019094705,\n",
       "   0.008790538946166635,\n",
       "   0.008778184173628687,\n",
       "   0.008766130870208144,\n",
       "   0.008789505450055003]},\n",
       " {'bi_mamba_stacks': 4,\n",
       "  'n_layers': 32,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.06434850831632502,\n",
       "   0.0602751913620159,\n",
       "   0.05986805859953165,\n",
       "   0.05974204014870338,\n",
       "   0.05959856542758644,\n",
       "   0.05959954631747678,\n",
       "   0.05945691886474378,\n",
       "   0.05964217048604041,\n",
       "   0.05973649190389551,\n",
       "   0.05966926566907205,\n",
       "   0.0596593969251262,\n",
       "   0.05946968543285038,\n",
       "   0.05972989669418894,\n",
       "   0.05960408089100383,\n",
       "   0.0597758308344055],\n",
       "  'running_test_loss': [0.005467103191185743,\n",
       "   0.005394509895704686,\n",
       "   0.005386882855091244,\n",
       "   0.0054030090691521765,\n",
       "   0.005409506449475884,\n",
       "   0.005396571268327534,\n",
       "   0.005404287034180015,\n",
       "   0.005414383881259709,\n",
       "   0.005411169534549117,\n",
       "   0.005390684603014961,\n",
       "   0.005400429042056203,\n",
       "   0.005413920548278838,\n",
       "   0.005384524478111416,\n",
       "   0.005388896097429097,\n",
       "   0.005383950024843216]},\n",
       " {'bi_mamba_stacks': 4,\n",
       "  'n_layers': 4,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.06263101993245072,\n",
       "   0.059468111445894464,\n",
       "   0.05938222802011296,\n",
       "   0.059637226793565784,\n",
       "   0.05936639083432965,\n",
       "   0.05968354078475386,\n",
       "   0.059679510241257956,\n",
       "   0.059636203565169126,\n",
       "   0.059494072267552836,\n",
       "   0.0594839607225731,\n",
       "   0.0593859778449405,\n",
       "   0.05931237592827529,\n",
       "   0.05948598088230938,\n",
       "   0.05966639500926249,\n",
       "   0.05959820351679809],\n",
       "  'running_test_loss': [0.0054010201944038275,\n",
       "   0.005364491618238389,\n",
       "   0.0054258585278876125,\n",
       "   0.005381024405825883,\n",
       "   0.005400706177344546,\n",
       "   0.005364619376603514,\n",
       "   0.005364684603177011,\n",
       "   0.005419233541470021,\n",
       "   0.005394739562179893,\n",
       "   0.0053464483008719985,\n",
       "   0.005365123690571636,\n",
       "   0.005376553193666041,\n",
       "   0.005347614729311318,\n",
       "   0.005370418707374484,\n",
       "   0.005425776050426066]},\n",
       " {'bi_mamba_stacks': 4,\n",
       "  'n_layers': 8,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.03521272878162563,\n",
       "   0.02670762234716676,\n",
       "   0.025399470283882692,\n",
       "   0.024945789027260617,\n",
       "   0.02472931253258139,\n",
       "   0.024647071375511587,\n",
       "   0.02463993089273572,\n",
       "   0.024416572435293347,\n",
       "   0.024447618345730008,\n",
       "   0.024431228711036965,\n",
       "   0.024485121411271394,\n",
       "   0.02426892308634706,\n",
       "   0.02435978164197877,\n",
       "   0.024599571969592945,\n",
       "   0.024498256297083573],\n",
       "  'running_test_loss': [0.002715157651808113,\n",
       "   0.002233161319978535,\n",
       "   0.0022250196314416828,\n",
       "   0.0021815309384837747,\n",
       "   0.0021743459440767767,\n",
       "   0.002250188364647329,\n",
       "   0.002156874749343842,\n",
       "   0.0021686376798897983,\n",
       "   0.002180950590642169,\n",
       "   0.0021612240932881834,\n",
       "   0.00216103370860219,\n",
       "   0.0021609943960793315,\n",
       "   0.0021685180575586856,\n",
       "   0.0021727641657926142,\n",
       "   0.0021319256427232178]},\n",
       " {'bi_mamba_stacks': 4,\n",
       "  'n_layers': 16,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.06420220121275634,\n",
       "   0.06179312429390848,\n",
       "   0.06035928938537836,\n",
       "   0.0599220430618152,\n",
       "   0.059262840123847124,\n",
       "   0.05866648546420038,\n",
       "   0.057932588984258473,\n",
       "   0.057933261981233955,\n",
       "   0.05777011522091925,\n",
       "   0.05766349775716662,\n",
       "   0.05738477559294552,\n",
       "   0.05734616113360971,\n",
       "   0.057230727188289164,\n",
       "   0.05713894439395517,\n",
       "   0.05700533765833825],\n",
       "  'running_test_loss': [0.0060753374677151445,\n",
       "   0.005829132776707411,\n",
       "   0.005721510834991932,\n",
       "   0.005702133234590292,\n",
       "   0.005702692087739706,\n",
       "   0.0055886350609362125,\n",
       "   0.005622534831985831,\n",
       "   0.005554001960903407,\n",
       "   0.0056208655908703805,\n",
       "   0.005577148215845228,\n",
       "   0.005558568188920617,\n",
       "   0.0055571331810206175,\n",
       "   0.005625154852867126,\n",
       "   0.005612996047362685,\n",
       "   0.005576641833409667]},\n",
       " {'bi_mamba_stacks': 4,\n",
       "  'n_layers': 32,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'running_regression_total_loss': [0.03175551398540847,\n",
       "   0.026516274319728837,\n",
       "   0.02573916004737839,\n",
       "   0.024984928142512217,\n",
       "   0.024865628938423468,\n",
       "   0.02469335832982324,\n",
       "   0.024401343198260293,\n",
       "   0.024280155667802318,\n",
       "   0.02470658109989017,\n",
       "   0.024093332425691186,\n",
       "   0.023972163973376155,\n",
       "   0.023683425078634173,\n",
       "   0.023677942036883906,\n",
       "   0.02396437508868985,\n",
       "   0.02355349489953369],\n",
       "  'running_test_loss': [0.0025581174083054064,\n",
       "   0.0022869954355992377,\n",
       "   0.002225874119438231,\n",
       "   0.002270152486860752,\n",
       "   0.002151035800343379,\n",
       "   0.0021575716617517173,\n",
       "   0.0022192557593807578,\n",
       "   0.002146994861541316,\n",
       "   0.002138879428151995,\n",
       "   0.0020811153396498414,\n",
       "   0.002149068173021078,\n",
       "   0.0021248165820725263,\n",
       "   0.0020773088280111552,\n",
       "   0.0022462469707243147,\n",
       "   0.0020680020228028297]},\n",
       " {'bi_mamba_stacks': 4,\n",
       "  'n_layers': 4,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.055015614361036566,\n",
       "   0.037666674468200656,\n",
       "   0.03276657564449124,\n",
       "   0.03064643291872926,\n",
       "   0.030033581731840966,\n",
       "   0.029617039008298887,\n",
       "   0.029440955752506853,\n",
       "   0.029121211628662422,\n",
       "   0.028986495938152074,\n",
       "   0.02874547376530245,\n",
       "   0.028607041325885804,\n",
       "   0.02811619277112186,\n",
       "   0.02769742000149563,\n",
       "   0.027155005360255016,\n",
       "   0.026992545840330423],\n",
       "  'running_test_loss': [0.0038329924014396965,\n",
       "   0.002989019743166864,\n",
       "   0.002799908224027604,\n",
       "   0.0027184929396025835,\n",
       "   0.0027143413829617204,\n",
       "   0.002716131767258048,\n",
       "   0.002681973428931087,\n",
       "   0.0026820417973212897,\n",
       "   0.0026969589507207276,\n",
       "   0.0026702312896959486,\n",
       "   0.0026410857639275493,\n",
       "   0.0025861368589103224,\n",
       "   0.0024881130498833953,\n",
       "   0.002423421415966004,\n",
       "   0.0023874169206246736]},\n",
       " {'bi_mamba_stacks': 4,\n",
       "  'n_layers': 8,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.043629550526384266,\n",
       "   0.03343988533015363,\n",
       "   0.03124670671415515,\n",
       "   0.02994060921133496,\n",
       "   0.029206062500597908,\n",
       "   0.028757671155035497,\n",
       "   0.028352751990314572,\n",
       "   0.028014812108594925,\n",
       "   0.027607350499602036,\n",
       "   0.027228163701947777,\n",
       "   0.02682414003764279,\n",
       "   0.026221135318046436,\n",
       "   0.02583149782847613,\n",
       "   0.025641307660844177,\n",
       "   0.02547501993831247],\n",
       "  'running_test_loss': [0.003113773182965815,\n",
       "   0.002858194010797888,\n",
       "   0.002748373955953866,\n",
       "   0.0026720505747944118,\n",
       "   0.002636386100202799,\n",
       "   0.0025982372323051094,\n",
       "   0.0025598851754330097,\n",
       "   0.002534706525038928,\n",
       "   0.0024601774006150663,\n",
       "   0.0024328444534912706,\n",
       "   0.0023717933176085355,\n",
       "   0.0023104887665249406,\n",
       "   0.002264390317723155,\n",
       "   0.0022509597004391254,\n",
       "   0.0022209904245100915]},\n",
       " {'bi_mamba_stacks': 4,\n",
       "  'n_layers': 16,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.059522314234636726,\n",
       "   0.03450620376272127,\n",
       "   0.03275674287113361,\n",
       "   0.03126867095124908,\n",
       "   0.030343995379516854,\n",
       "   0.029588620042195545,\n",
       "   0.028974532185820863,\n",
       "   0.02869640743592754,\n",
       "   0.028353666324401273,\n",
       "   0.027974849584279583,\n",
       "   0.027612999661359935,\n",
       "   0.027465072366176173,\n",
       "   0.027199227403616532,\n",
       "   0.027293066836427897,\n",
       "   0.02714275713544339],\n",
       "  'running_test_loss': [0.0033189618811011313,\n",
       "   0.002974268213380128,\n",
       "   0.002893102316185832,\n",
       "   0.0027996451696380973,\n",
       "   0.002703399324789643,\n",
       "   0.002649059692863375,\n",
       "   0.002598469065967947,\n",
       "   0.0025560831814073026,\n",
       "   0.0024946869001723827,\n",
       "   0.0024859714100603013,\n",
       "   0.0024489032425917684,\n",
       "   0.0024448047131299974,\n",
       "   0.002439952006097883,\n",
       "   0.002410028261132538,\n",
       "   0.002458606753963977]},\n",
       " {'bi_mamba_stacks': 4,\n",
       "  'n_layers': 32,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.0001,\n",
       "  'running_regression_total_loss': [0.07080905905924738,\n",
       "   0.0643275820184499,\n",
       "   0.06289228205103427,\n",
       "   0.06263538960367442,\n",
       "   0.06236232097260654,\n",
       "   0.06160364114679396,\n",
       "   0.06008583545684815,\n",
       "   0.059279560977593064,\n",
       "   0.05897097817156464,\n",
       "   0.058822114453651014,\n",
       "   0.05882643446791917,\n",
       "   0.05892416302580386,\n",
       "   0.05858895961660892,\n",
       "   0.058646665615960956,\n",
       "   0.058701911540701986],\n",
       "  'running_test_loss': [0.006347021302208305,\n",
       "   0.006098479248583317,\n",
       "   0.006083719668909907,\n",
       "   0.006065429385751486,\n",
       "   0.006039652889594436,\n",
       "   0.005899745963513851,\n",
       "   0.005671855572611094,\n",
       "   0.005665441079065203,\n",
       "   0.0056372479815036056,\n",
       "   0.0056401946693658825,\n",
       "   0.005625877419486642,\n",
       "   0.005648995328694582,\n",
       "   0.005669138649478555,\n",
       "   0.0056180223822593685,\n",
       "   0.005629182102158666]}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7f35c0c-792e-46a4-a818-0b29ba0b2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c5c667c-d503-4754-8335-a38e62ed267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('8ksample_2ktest_20part_T200_30_regression.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d0668-ca4c-4c43-9f4a-dfadc0b248ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
