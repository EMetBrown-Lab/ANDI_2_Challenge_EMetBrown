{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a4c625-1262-40f8-8bf6-10af6a1f0509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_both_at_same_time import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e29ef47-b60d-48c2-ac8a-8cfac322103c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m.lavaud/miniconda3/envs/torch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Epoch 0: 100%|██████████| 8/8 [00:05<00:00,  1.35batch/s, loss=1.83]\n",
      "Epoch 1: 100%|██████████| 8/8 [00:05<00:00,  1.35batch/s, loss=0.963]\n",
      "Epoch 2: 100%|██████████| 8/8 [00:06<00:00,  1.26batch/s, loss=0.623]\n",
      "Epoch 3: 100%|██████████| 8/8 [00:05<00:00,  1.35batch/s, loss=0.339]\n",
      "Epoch 4: 100%|██████████| 8/8 [00:05<00:00,  1.39batch/s, loss=0.257]\n",
      "Epoch 5: 100%|██████████| 8/8 [00:06<00:00,  1.30batch/s, loss=0.213]\n",
      "Epoch 6: 100%|██████████| 8/8 [00:04<00:00,  1.61batch/s, loss=0.138]\n",
      "Epoch 7: 100%|██████████| 8/8 [00:05<00:00,  1.38batch/s, loss=0.102]\n",
      "Epoch 8: 100%|██████████| 8/8 [00:05<00:00,  1.37batch/s, loss=0.0886]\n",
      "Epoch 9: 100%|██████████| 8/8 [00:06<00:00,  1.23batch/s, loss=0.0711]\n",
      "Epoch 10: 100%|██████████| 8/8 [00:06<00:00,  1.26batch/s, loss=0.0779]\n",
      "Epoch 11: 100%|██████████| 8/8 [00:06<00:00,  1.30batch/s, loss=0.065] \n",
      "Epoch 12: 100%|██████████| 8/8 [00:05<00:00,  1.45batch/s, loss=0.0953]\n",
      "Epoch 13: 100%|██████████| 8/8 [00:05<00:00,  1.35batch/s, loss=0.0858]\n",
      "Epoch 14: 100%|██████████| 8/8 [00:05<00:00,  1.47batch/s, loss=0.093] \n"
     ]
    }
   ],
   "source": [
    "a=train((1,1,0.05,1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ddd18fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bi_mamba_stacks': 1,\n",
       " 'conv_stack': 1,\n",
       " 'dropout': 0.05,\n",
       " 'learning_rate': 0.001,\n",
       " 'running_regression_total_loss': [2.81465545296669,\n",
       "  1.4522654861211777,\n",
       "  0.7507461756467819,\n",
       "  0.4595734365284443,\n",
       "  0.3013584166765213,\n",
       "  0.2215379923582077,\n",
       "  0.17046700231730938,\n",
       "  0.12720509991049767,\n",
       "  0.09378154668956995,\n",
       "  0.0837926622480154,\n",
       "  0.07824699440971017,\n",
       "  0.07783779129385948,\n",
       "  0.07376928767189384,\n",
       "  0.07262654602527618,\n",
       "  0.07184614054858685],\n",
       " 'running_test_regression': [1.5571388006210327,\n",
       "  0.7562628984451294,\n",
       "  0.45003464818000793,\n",
       "  0.2833322584629059,\n",
       "  0.20253458619117737,\n",
       "  0.15914078056812286,\n",
       "  0.11585694551467896,\n",
       "  0.0880587100982666,\n",
       "  0.0727209523320198,\n",
       "  0.06402952969074249,\n",
       "  0.06366270035505295,\n",
       "  0.06449573487043381,\n",
       "  0.061472225934267044,\n",
       "  0.06025798246264458,\n",
       "  0.055781859904527664],\n",
       " 'runnning_test_classification': 0.9205262660980225}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9142e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43c0a4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x705df405b530>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([0.09305416837334633,\n",
    "  0.049423510637134314,\n",
    "  0.042982536181807515,\n",
    "  0.03988937536254525,\n",
    "  0.03857466798275709,\n",
    "  0.03706808654591441,\n",
    "  0.03733404904603958,\n",
    "  0.03586350856348872,\n",
    "  0.03535375786945224,\n",
    "  0.03523683818057179])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a047659-d3b7-4f0f-82cd-87efffa28f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_mamba_stacks = [1,10,20]\n",
    "conv_stacks = 1\n",
    "dropouts = [0.05,0.1,0.15]\n",
    "learning_rate = [1e-2,1e-3]\n",
    "import itertools\n",
    "all_tests = list(itertools.product(bi_mamba_stacks, conv_stacks, dropouts, learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864cab03-d441-4d0d-813f-2cccc36e8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "from torch.multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7bd9a83-f5ae-43c4-bb28-ba822f61bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3405345-e421-4781-a29f-eb610b86a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b9dd9e9-dd5a-4862-a5a7-9978d842a85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                  | 0/54 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n",
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n",
      "importing gpu Mamba\n",
      "importing gpu Mamba\n",
      "importing gpu Mamba\n",
      "importing gpu Mamba\n",
      "importing gpu Mamba\n",
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 100/100 [00:23<00:00,  4.27batch/s, classification_loss=0.548, loss=0.696, regression_loss=0.148]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:23<00:00,  4.20batch/s, classification_loss=0.513, loss=0.647, regression_loss=0.134]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:23<00:00,  4.19batch/s, classification_loss=0.874, loss=1.08, regression_loss=0.206] \n",
      "Epoch 0: 100%|██████████| 100/100 [00:23<00:00,  4.18batch/s, classification_loss=1.07, loss=1.25, regression_loss=0.181] \n",
      "Epoch 0: 100%|██████████| 100/100 [00:24<00:00,  4.12batch/s, classification_loss=1.22, loss=1.38, regression_loss=0.158]]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:24<00:00,  4.09batch/s, classification_loss=0.47, loss=0.68, regression_loss=0.21]  \n",
      "Epoch 0: 100%|██████████| 100/100 [00:24<00:00,  4.03batch/s, classification_loss=0.537, loss=0.66, regression_loss=0.123]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:24<00:00,  4.03batch/s, classification_loss=1.16, loss=1.37, regression_loss=0.207]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:25<00:00,  3.99batch/s, classification_loss=0.405, loss=0.673, regression_loss=0.268]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:22<00:00,  4.51batch/s, classification_loss=0.282, loss=0.402, regression_loss=0.121]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:22<00:00,  4.46batch/s, classification_loss=0.433, loss=0.557, regression_loss=0.124]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:22<00:00,  4.46batch/s, classification_loss=0.616, loss=0.747, regression_loss=0.132]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:22<00:00,  4.42batch/s, classification_loss=0.752, loss=0.97, regression_loss=0.218]]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:22<00:00,  4.39batch/s, classification_loss=0.64, loss=0.783, regression_loss=0.142] \n",
      "Epoch 1: 100%|██████████| 100/100 [00:22<00:00,  4.37batch/s, classification_loss=0.533, loss=0.675, regression_loss=0.142]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:22<00:00,  4.35batch/s, classification_loss=0.654, loss=0.762, regression_loss=0.108]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:23<00:00,  4.25batch/s, classification_loss=0.593, loss=0.697, regression_loss=0.104]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:23<00:00,  4.18batch/s, classification_loss=0.428, loss=0.648, regression_loss=0.219]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:23<00:00,  4.30batch/s, classification_loss=0.384, loss=0.475, regression_loss=0.091]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:23<00:00,  4.21batch/s, classification_loss=0.46, loss=0.554, regression_loss=0.0941]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:23<00:00,  4.22batch/s, classification_loss=0.41, loss=0.495, regression_loss=0.0848]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:24<00:00,  4.09batch/s, classification_loss=0.62, loss=0.803, regression_loss=0.184] \n",
      "Epoch 2: 100%|██████████| 100/100 [00:23<00:00,  4.18batch/s, classification_loss=0.488, loss=0.617, regression_loss=0.129]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:24<00:00,  4.13batch/s, classification_loss=0.554, loss=0.686, regression_loss=0.132]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:24<00:00,  4.04batch/s, classification_loss=0.423, loss=0.54, regression_loss=0.117]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:25<00:00,  3.93batch/s, classification_loss=0.682, loss=0.833, regression_loss=0.151]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:24<00:00,  4.04batch/s, classification_loss=0.41, loss=0.582, regression_loss=0.172] \n",
      "Epoch 3: 100%|██████████| 100/100 [00:24<00:00,  4.13batch/s, classification_loss=0.455, loss=0.68, regression_loss=0.225] \n",
      "Epoch 3: 100%|██████████| 100/100 [00:24<00:00,  4.15batch/s, classification_loss=0.549, loss=0.677, regression_loss=0.128]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:24<00:00,  4.00batch/s, classification_loss=0.514, loss=0.632, regression_loss=0.117]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:24<00:00,  4.15batch/s, classification_loss=0.498, loss=0.622, regression_loss=0.124]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:24<00:00,  4.10batch/s, classification_loss=0.463, loss=0.607, regression_loss=0.144]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:24<00:00,  4.11batch/s, classification_loss=0.46, loss=0.597, regression_loss=0.136]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:23<00:00,  4.18batch/s, classification_loss=0.398, loss=0.641, regression_loss=0.242]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:24<00:00,  4.05batch/s, classification_loss=0.437, loss=0.576, regression_loss=0.139]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:24<00:00,  4.02batch/s, classification_loss=0.443, loss=0.551, regression_loss=0.107]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:22<00:00,  4.41batch/s, classification_loss=0.429, loss=0.53, regression_loss=0.101] \n",
      "Epoch 4: 100%|██████████| 100/100 [00:22<00:00,  4.37batch/s, classification_loss=0.534, loss=0.674, regression_loss=0.14] \n",
      "Epoch 4: 100%|██████████| 100/100 [00:23<00:00,  4.31batch/s, classification_loss=0.435, loss=0.55, regression_loss=0.115] \n",
      "Epoch 4: 100%|██████████| 100/100 [00:22<00:00,  4.40batch/s, classification_loss=0.638, loss=0.794, regression_loss=0.156]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:23<00:00,  4.29batch/s, classification_loss=0.5, loss=0.644, regression_loss=0.143]]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:23<00:00,  4.27batch/s, classification_loss=0.558, loss=0.701, regression_loss=0.142]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:23<00:00,  4.22batch/s, classification_loss=0.412, loss=0.638, regression_loss=0.226]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:23<00:00,  4.22batch/s, classification_loss=0.549, loss=0.68, regression_loss=0.131]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:23<00:00,  4.20batch/s, classification_loss=0.384, loss=0.509, regression_loss=0.125]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:22<00:00,  4.35batch/s, classification_loss=0.46, loss=0.6, regression_loss=0.14]    \n",
      "Epoch 5: 100%|██████████| 100/100 [00:23<00:00,  4.31batch/s, classification_loss=0.329, loss=0.424, regression_loss=0.0956]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:23<00:00,  4.34batch/s, classification_loss=0.512, loss=0.626, regression_loss=0.115]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:23<00:00,  4.21batch/s, classification_loss=0.481, loss=0.589, regression_loss=0.108]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:23<00:00,  4.25batch/s, classification_loss=0.426, loss=0.545, regression_loss=0.119]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:23<00:00,  4.27batch/s, classification_loss=0.451, loss=0.598, regression_loss=0.147]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:22<00:00,  4.36batch/s, classification_loss=0.565, loss=0.753, regression_loss=0.188]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:23<00:00,  4.27batch/s, classification_loss=0.437, loss=0.565, regression_loss=0.127]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:23<00:00,  4.18batch/s, classification_loss=0.52, loss=0.653, regression_loss=0.133] \n",
      "Epoch 6: 100%|██████████| 100/100 [00:23<00:00,  4.28batch/s, classification_loss=0.427, loss=0.562, regression_loss=0.134]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:23<00:00,  4.18batch/s, classification_loss=0.417, loss=0.516, regression_loss=0.0991]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:23<00:00,  4.18batch/s, classification_loss=0.395, loss=0.511, regression_loss=0.116]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:23<00:00,  4.18batch/s, classification_loss=0.526, loss=0.654, regression_loss=0.128]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:23<00:00,  4.19batch/s, classification_loss=0.394, loss=0.494, regression_loss=0.0995]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:23<00:00,  4.19batch/s, classification_loss=0.514, loss=0.633, regression_loss=0.12]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:24<00:00,  4.13batch/s, classification_loss=0.435, loss=0.622, regression_loss=0.187]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:24<00:00,  4.01batch/s, classification_loss=0.377, loss=0.489, regression_loss=0.112]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:25<00:00,  4.00batch/s, classification_loss=0.488, loss=0.612, regression_loss=0.124]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:23<00:00,  4.30batch/s, classification_loss=0.498, loss=0.619, regression_loss=0.121]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:23<00:00,  4.21batch/s, classification_loss=0.537, loss=0.673, regression_loss=0.136]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:23<00:00,  4.29batch/s, classification_loss=0.497, loss=0.613, regression_loss=0.116]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:23<00:00,  4.32batch/s, classification_loss=0.434, loss=0.547, regression_loss=0.113]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:23<00:00,  4.21batch/s, classification_loss=0.501, loss=0.636, regression_loss=0.135]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:23<00:00,  4.26batch/s, classification_loss=0.556, loss=0.685, regression_loss=0.129]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:23<00:00,  4.19batch/s, classification_loss=0.47, loss=0.657, regression_loss=0.186] \n",
      "Epoch 7: 100%|██████████| 100/100 [00:23<00:00,  4.19batch/s, classification_loss=0.399, loss=0.509, regression_loss=0.109]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:24<00:00,  4.16batch/s, classification_loss=0.396, loss=0.524, regression_loss=0.127]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:23<00:00,  4.30batch/s, classification_loss=0.456, loss=0.602, regression_loss=0.146]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:23<00:00,  4.23batch/s, classification_loss=0.471, loss=0.602, regression_loss=0.131]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:23<00:00,  4.21batch/s, classification_loss=0.461, loss=0.586, regression_loss=0.125]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:23<00:00,  4.25batch/s, classification_loss=0.515, loss=0.66, regression_loss=0.146] \n",
      "Epoch 8: 100%|██████████| 100/100 [00:23<00:00,  4.21batch/s, classification_loss=0.395, loss=0.495, regression_loss=0.1] \n",
      "Epoch 8: 100%|██████████| 100/100 [00:23<00:00,  4.23batch/s, classification_loss=0.46, loss=0.567, regression_loss=0.107]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:24<00:00,  4.16batch/s, classification_loss=0.469, loss=0.679, regression_loss=0.209]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:24<00:00,  4.13batch/s, classification_loss=0.478, loss=0.603, regression_loss=0.125]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:24<00:00,  4.13batch/s, classification_loss=0.473, loss=0.614, regression_loss=0.141]\n",
      "Epoch 9: 100%|██████████| 100/100 [00:23<00:00,  4.27batch/s, classification_loss=0.467, loss=0.614, regression_loss=0.147]\n",
      "Epoch 0:   0%|          | 0/100 [00:00<?, ?batch/s]2batch/s, classification_loss=0.489, loss=0.585, regression_loss=0.0964]it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [00:23<00:00,  4.24batch/s, classification_loss=0.479, loss=0.629, regression_loss=0.15] \n",
      "Epoch 9:  68%|██████▊   | 68/100 [00:17<00:07,  4.34batch/s, classification_loss=0.385, loss=0.492, regression_loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [00:23<00:00,  4.20batch/s, classification_loss=0.495, loss=0.601, regression_loss=0.106]\n",
      "Epoch 9: 100%|██████████| 100/100 [00:23<00:00,  4.22batch/s, classification_loss=0.372, loss=0.495, regression_loss=0.123]\n",
      "Epoch 9:  98%|█████████▊| 98/100 [00:23<00:00,  4.71batch/s, classification_loss=0.483, loss=0.61, regression_loss=0.127]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:  99%|█████████▉| 99/100 [00:23<00:00,  4.65batch/s, classification_loss=0.452, loss=0.593, regression_loss=0.14]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [00:23<00:00,  4.17batch/s, classification_loss=0.463, loss=0.579, regression_loss=0.116]\n",
      "Epoch 9: 100%|██████████| 100/100 [00:24<00:00,  4.16batch/s, classification_loss=0.403, loss=0.524, regression_loss=0.122]\n",
      "Epoch 0:   2%|▏         | 2/100 [00:00<00:26,  3.75batch/s, classification_loss=1.45, loss=1.7, regression_loss=0.253]12] /it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  22%|██▏       | 22/100 [00:05<00:18,  4.26batch/s, classification_loss=1.33, loss=1.54, regression_loss=0.215]] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [00:23<00:00,  4.22batch/s, classification_loss=0.519, loss=0.697, regression_loss=0.179]\n",
      "Epoch 0:  11%|█         | 11/100 [00:02<00:22,  4.03batch/s, classification_loss=1.17, loss=1.44, regression_loss=0.27] 5]/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [00:24<00:00,  4.05batch/s, classification_loss=0.456, loss=0.588, regression_loss=0.132]\n",
      "Epoch 9: 100%|██████████| 100/100 [00:25<00:00,  3.95batch/s, classification_loss=0.462, loss=0.586, regression_loss=0.123]it]\n",
      "Epoch 0:  14%|█▍        | 14/100 [00:03<00:20,  4.15batch/s, classification_loss=1.41, loss=1.66, regression_loss=0.249]9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n",
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 100/100 [00:25<00:00,  3.86batch/s, classification_loss=0.996, loss=1.2, regression_loss=0.204]] \n",
      "Epoch 0: 100%|██████████| 100/100 [00:25<00:00,  3.87batch/s, classification_loss=0.56, loss=0.766, regression_loss=0.206]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:25<00:00,  3.85batch/s, classification_loss=1.16, loss=1.38, regression_loss=0.221] \n",
      "Epoch 0: 100%|██████████| 100/100 [00:27<00:00,  3.67batch/s, classification_loss=0.428, loss=0.524, regression_loss=0.096]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:27<00:00,  3.61batch/s, classification_loss=0.357, loss=0.583, regression_loss=0.226]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:28<00:00,  3.54batch/s, classification_loss=1.08, loss=1.29, regression_loss=0.214]]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:27<00:00,  3.61batch/s, classification_loss=1.31, loss=1.55, regression_loss=0.243]]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:27<00:00,  3.60batch/s, classification_loss=0.845, loss=0.991, regression_loss=0.146]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:28<00:00,  3.48batch/s, classification_loss=0.74, loss=0.867, regression_loss=0.127]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:27<00:00,  3.62batch/s, classification_loss=0.857, loss=0.982, regression_loss=0.125]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:26<00:00,  3.71batch/s, classification_loss=0.386, loss=0.595, regression_loss=0.209]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:27<00:00,  3.70batch/s, classification_loss=0.844, loss=1.07, regression_loss=0.222]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:29<00:00,  3.35batch/s, classification_loss=0.477, loss=0.582, regression_loss=0.105]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:29<00:00,  3.39batch/s, classification_loss=0.633, loss=0.853, regression_loss=0.22]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:30<00:00,  3.31batch/s, classification_loss=0.352, loss=0.571, regression_loss=0.219]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:29<00:00,  3.43batch/s, classification_loss=0.799, loss=1.07, regression_loss=0.27]]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:29<00:00,  3.42batch/s, classification_loss=0.607, loss=0.717, regression_loss=0.111]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:30<00:00,  3.31batch/s, classification_loss=0.617, loss=0.761, regression_loss=0.144]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:26<00:00,  3.71batch/s, classification_loss=0.559, loss=0.667, regression_loss=0.108]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:26<00:00,  3.76batch/s, classification_loss=0.505, loss=0.656, regression_loss=0.15]]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:27<00:00,  3.68batch/s, classification_loss=0.713, loss=0.915, regression_loss=0.202]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:28<00:00,  3.52batch/s, classification_loss=0.477, loss=0.601, regression_loss=0.125]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:27<00:00,  3.57batch/s, classification_loss=0.503, loss=0.699, regression_loss=0.196]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:27<00:00,  3.59batch/s, classification_loss=0.485, loss=0.702, regression_loss=0.217]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:27<00:00,  3.65batch/s, classification_loss=0.511, loss=0.772, regression_loss=0.261]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:27<00:00,  3.61batch/s, classification_loss=0.521, loss=0.654, regression_loss=0.133]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:27<00:00,  3.58batch/s, classification_loss=0.699, loss=0.831, regression_loss=0.132]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:26<00:00,  3.84batch/s, classification_loss=0.461, loss=0.603, regression_loss=0.142]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:26<00:00,  3.76batch/s, classification_loss=0.354, loss=0.459, regression_loss=0.105]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:26<00:00,  3.78batch/s, classification_loss=0.504, loss=0.709, regression_loss=0.205]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:27<00:00,  3.59batch/s, classification_loss=0.535, loss=0.647, regression_loss=0.112]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:27<00:00,  3.65batch/s, classification_loss=0.488, loss=0.686, regression_loss=0.198]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:27<00:00,  3.59batch/s, classification_loss=0.51, loss=0.773, regression_loss=0.262] \n",
      "Epoch 3: 100%|██████████| 100/100 [00:27<00:00,  3.64batch/s, classification_loss=0.617, loss=0.878, regression_loss=0.261]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:27<00:00,  3.61batch/s, classification_loss=0.485, loss=0.617, regression_loss=0.132]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:29<00:00,  3.41batch/s, classification_loss=0.619, loss=0.771, regression_loss=0.152]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:26<00:00,  3.78batch/s, classification_loss=0.554, loss=0.679, regression_loss=0.125]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:26<00:00,  3.80batch/s, classification_loss=0.569, loss=0.717, regression_loss=0.148]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:26<00:00,  3.75batch/s, classification_loss=0.555, loss=0.744, regression_loss=0.189]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:27<00:00,  3.66batch/s, classification_loss=0.439, loss=0.616, regression_loss=0.178]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:28<00:00,  3.54batch/s, classification_loss=0.54, loss=0.655, regression_loss=0.115]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:27<00:00,  3.66batch/s, classification_loss=0.464, loss=0.737, regression_loss=0.273]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:27<00:00,  3.67batch/s, classification_loss=0.442, loss=0.715, regression_loss=0.273]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:27<00:00,  3.65batch/s, classification_loss=0.652, loss=0.803, regression_loss=0.152]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:27<00:00,  3.65batch/s, classification_loss=0.653, loss=0.827, regression_loss=0.174]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:26<00:00,  3.84batch/s, classification_loss=0.387, loss=0.5, regression_loss=0.113]  \n",
      "Epoch 5: 100%|██████████| 100/100 [00:26<00:00,  3.79batch/s, classification_loss=0.47, loss=0.587, regression_loss=0.117]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:26<00:00,  3.74batch/s, classification_loss=0.443, loss=0.656, regression_loss=0.213]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:28<00:00,  3.53batch/s, classification_loss=0.359, loss=0.593, regression_loss=0.234]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:28<00:00,  3.45batch/s, classification_loss=0.329, loss=0.464, regression_loss=0.135]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:29<00:00,  3.37batch/s, classification_loss=0.349, loss=0.479, regression_loss=0.131]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:29<00:00,  3.44batch/s, classification_loss=0.395, loss=0.536, regression_loss=0.141]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:28<00:00,  3.52batch/s, classification_loss=0.585, loss=0.719, regression_loss=0.134]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:29<00:00,  3.38batch/s, classification_loss=0.7, loss=0.856, regression_loss=0.156]  \n",
      "Epoch 6: 100%|██████████| 100/100 [00:26<00:00,  3.76batch/s, classification_loss=0.525, loss=0.671, regression_loss=0.146]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:27<00:00,  3.60batch/s, classification_loss=0.412, loss=0.531, regression_loss=0.119]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:27<00:00,  3.61batch/s, classification_loss=0.421, loss=0.614, regression_loss=0.192]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:28<00:00,  3.54batch/s, classification_loss=0.389, loss=0.597, regression_loss=0.208]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:28<00:00,  3.55batch/s, classification_loss=0.395, loss=0.529, regression_loss=0.134]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:28<00:00,  3.49batch/s, classification_loss=0.548, loss=0.704, regression_loss=0.156]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:28<00:00,  3.53batch/s, classification_loss=0.372, loss=0.525, regression_loss=0.153]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:28<00:00,  3.50batch/s, classification_loss=0.481, loss=0.595, regression_loss=0.114]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:26<00:00,  3.79batch/s, classification_loss=0.394, loss=0.518, regression_loss=0.123]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:27<00:00,  3.58batch/s, classification_loss=0.441, loss=0.557, regression_loss=0.116]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:26<00:00,  3.77batch/s, classification_loss=0.43, loss=0.562, regression_loss=0.132]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:27<00:00,  3.67batch/s, classification_loss=0.503, loss=0.717, regression_loss=0.214]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:28<00:00,  3.49batch/s, classification_loss=0.5, loss=0.612, regression_loss=0.112]] \n",
      "Epoch 7: 100%|██████████| 100/100 [00:29<00:00,  3.39batch/s, classification_loss=0.405, loss=0.614, regression_loss=0.209]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:28<00:00,  3.49batch/s, classification_loss=0.569, loss=0.715, regression_loss=0.147]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:29<00:00,  3.42batch/s, classification_loss=0.628, loss=0.75, regression_loss=0.122] \n",
      "Epoch 7: 100%|██████████| 100/100 [00:28<00:00,  3.54batch/s, classification_loss=0.431, loss=0.534, regression_loss=0.103]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:26<00:00,  3.74batch/s, classification_loss=0.554, loss=0.692, regression_loss=0.139]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:27<00:00,  3.62batch/s, classification_loss=0.417, loss=0.533, regression_loss=0.116]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:26<00:00,  3.72batch/s, classification_loss=0.512, loss=0.635, regression_loss=0.123]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:26<00:00,  3.71batch/s, classification_loss=0.412, loss=0.612, regression_loss=0.2]] \n",
      "Epoch 8: 100%|██████████| 100/100 [00:27<00:00,  3.61batch/s, classification_loss=0.504, loss=0.66, regression_loss=0.156] \n",
      "Epoch 8: 100%|██████████| 100/100 [00:27<00:00,  3.65batch/s, classification_loss=0.394, loss=0.526, regression_loss=0.132]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:26<00:00,  3.74batch/s, classification_loss=0.447, loss=0.589, regression_loss=0.142]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:28<00:00,  3.54batch/s, classification_loss=0.485, loss=0.719, regression_loss=0.235]\n",
      "Epoch 9: 100%|██████████| 100/100 [00:25<00:00,  3.86batch/s, classification_loss=0.406, loss=0.52, regression_loss=0.114] \n",
      "Epoch 0:   0%|          | 0/100 [00:00<?, ?batch/s]7batch/s, classification_loss=0.439, loss=0.555, regression_loss=0.116]/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 100/100 [00:28<00:00,  3.49batch/s, classification_loss=0.489, loss=0.603, regression_loss=0.113]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:27<00:00,  3.58batch/s, classification_loss=0.434, loss=0.542, regression_loss=0.109]\n",
      "Epoch 9: 100%|██████████| 100/100 [00:26<00:00,  3.80batch/s, classification_loss=0.396, loss=0.499, regression_loss=0.103]\n",
      "Epoch 9:  29%|██▉       | 29/100 [00:08<00:20,  3.46batch/s, classification_loss=0.47, loss=0.693, regression_loss=0.223]] it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [00:26<00:00,  3.71batch/s, classification_loss=0.455, loss=0.668, regression_loss=0.213]\n",
      "Epoch 9:  30%|███       | 30/100 [00:08<00:21,  3.27batch/s, classification_loss=0.558, loss=0.671, regression_loss=0.113]/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [00:27<00:00,  3.58batch/s, classification_loss=0.42, loss=0.535, regression_loss=0.116] \n",
      "Epoch 9: 100%|██████████| 100/100 [00:27<00:00,  3.58batch/s, classification_loss=0.444, loss=0.561, regression_loss=0.117]\n",
      "Epoch 9: 100%|██████████| 100/100 [00:28<00:00,  3.47batch/s, classification_loss=0.479, loss=0.606, regression_loss=0.128]\n",
      " 24%|█████████████████████▍                                                                   | 13/54 [08:43<21:08, 30.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [00:29<00:00,  3.44batch/s, classification_loss=0.5, loss=0.786, regression_loss=0.286]] \n",
      "Epoch 0:  39%|███▉      | 39/100 [00:16<00:22,  2.69batch/s, classification_loss=0.988, loss=1.19, regression_loss=0.199] /it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n",
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:  95%|█████████▌| 95/100 [00:26<00:01,  3.92batch/s, classification_loss=0.465, loss=0.585, regression_loss=0.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [00:27<00:00,  3.58batch/s, classification_loss=0.409, loss=0.531, regression_loss=0.122]\n",
      "Epoch 9: 100%|██████████| 100/100 [00:26<00:00,  3.71batch/s, classification_loss=0.492, loss=0.626, regression_loss=0.134]it]\n",
      "Epoch 0:  53%|█████▎    | 53/100 [00:22<00:21,  2.17batch/s, classification_loss=0.453, loss=0.66, regression_loss=0.207] /it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  14%|█▍        | 14/100 [00:06<00:43,  1.99batch/s, classification_loss=1.29, loss=1.49, regression_loss=0.201]] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 100/100 [00:41<00:00,  2.43batch/s, classification_loss=0.524, loss=0.746, regression_loss=0.222]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:43<00:00,  2.30batch/s, classification_loss=0.88, loss=1.03, regression_loss=0.149]]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:46<00:00,  2.14batch/s, classification_loss=0.658, loss=0.811, regression_loss=0.153]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:51<00:00,  1.95batch/s, classification_loss=1.17, loss=1.39, regression_loss=0.218]]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:51<00:00,  1.93batch/s, classification_loss=0.714, loss=0.883, regression_loss=0.169]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:52<00:00,  1.91batch/s, classification_loss=1.34, loss=1.48, regression_loss=0.135]]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:53<00:00,  1.88batch/s, classification_loss=0.62, loss=0.794, regression_loss=0.173]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=1.07, loss=1.23, regression_loss=0.162]]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:54<00:00,  1.85batch/s, classification_loss=0.736, loss=0.855, regression_loss=0.119]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:52<00:00,  1.89batch/s, classification_loss=0.538, loss=0.747, regression_loss=0.209]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:52<00:00,  1.89batch/s, classification_loss=0.615, loss=0.739, regression_loss=0.123]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:53<00:00,  1.88batch/s, classification_loss=0.588, loss=0.748, regression_loss=0.16]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.756, loss=0.972, regression_loss=0.216]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:52<00:00,  1.89batch/s, classification_loss=0.545, loss=0.651, regression_loss=0.106]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:53<00:00,  1.88batch/s, classification_loss=0.918, loss=1.06, regression_loss=0.141]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:54<00:00,  1.85batch/s, classification_loss=0.403, loss=0.511, regression_loss=0.108]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:53<00:00,  1.86batch/s, classification_loss=0.809, loss=0.984, regression_loss=0.175]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:54<00:00,  1.85batch/s, classification_loss=0.743, loss=0.875, regression_loss=0.132]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:53<00:00,  1.88batch/s, classification_loss=0.408, loss=0.607, regression_loss=0.199]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.511, loss=0.606, regression_loss=0.0955]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:53<00:00,  1.86batch/s, classification_loss=0.581, loss=0.741, regression_loss=0.16]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.714, loss=0.928, regression_loss=0.214]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.97, loss=1.11, regression_loss=0.137]6]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.868, loss=1.02, regression_loss=0.152] \n",
      "Epoch 2: 100%|██████████| 100/100 [00:53<00:00,  1.86batch/s, classification_loss=0.744, loss=0.868, regression_loss=0.124]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:54<00:00,  1.84batch/s, classification_loss=0.518, loss=0.641, regression_loss=0.123]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:54<00:00,  1.84batch/s, classification_loss=0.535, loss=0.655, regression_loss=0.12]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:53<00:00,  1.88batch/s, classification_loss=0.885, loss=1.11, regression_loss=0.224]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.469, loss=0.599, regression_loss=0.13] \n",
      "Epoch 3: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.557, loss=0.781, regression_loss=0.223]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:53<00:00,  1.88batch/s, classification_loss=0.643, loss=0.854, regression_loss=0.211]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.514, loss=0.647, regression_loss=0.133]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.611, loss=0.731, regression_loss=0.119]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:54<00:00,  1.84batch/s, classification_loss=0.899, loss=1.04, regression_loss=0.143]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:54<00:00,  1.85batch/s, classification_loss=0.468, loss=0.582, regression_loss=0.114]\n",
      "Epoch 3: 100%|██████████| 100/100 [00:54<00:00,  1.84batch/s, classification_loss=0.922, loss=1.08, regression_loss=0.155]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.713, loss=0.903, regression_loss=0.19] \n",
      "Epoch 4: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.529, loss=0.621, regression_loss=0.092]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:53<00:00,  1.88batch/s, classification_loss=0.489, loss=0.676, regression_loss=0.187]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.454, loss=0.682, regression_loss=0.228]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.644, loss=0.772, regression_loss=0.128]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:53<00:00,  1.88batch/s, classification_loss=0.55, loss=0.699, regression_loss=0.149] \n",
      "Epoch 4: 100%|██████████| 100/100 [00:54<00:00,  1.83batch/s, classification_loss=0.842, loss=0.993, regression_loss=0.15] \n",
      "Epoch 4: 100%|██████████| 100/100 [00:53<00:00,  1.86batch/s, classification_loss=0.36, loss=0.5, regression_loss=0.14]    \n",
      "Epoch 4: 100%|██████████| 100/100 [00:54<00:00,  1.84batch/s, classification_loss=0.757, loss=0.878, regression_loss=0.121]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:53<00:00,  1.86batch/s, classification_loss=0.736, loss=0.946, regression_loss=0.21]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.532, loss=0.646, regression_loss=0.113]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:53<00:00,  1.88batch/s, classification_loss=0.615, loss=0.819, regression_loss=0.204]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.611, loss=0.832, regression_loss=0.221]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.617, loss=0.771, regression_loss=0.154]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.426, loss=0.533, regression_loss=0.107]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:54<00:00,  1.84batch/s, classification_loss=0.957, loss=1.09, regression_loss=0.131] \n",
      "Epoch 5: 100%|██████████| 100/100 [00:53<00:00,  1.85batch/s, classification_loss=0.474, loss=0.59, regression_loss=0.117] \n",
      "Epoch 5: 100%|██████████| 100/100 [00:53<00:00,  1.86batch/s, classification_loss=0.935, loss=1.11, regression_loss=0.179] \n",
      "Epoch 6: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=1.05, loss=1.25, regression_loss=0.204] \n",
      "Epoch 6: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.477, loss=0.586, regression_loss=0.109]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:53<00:00,  1.86batch/s, classification_loss=0.612, loss=0.799, regression_loss=0.187]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:53<00:00,  1.86batch/s, classification_loss=0.453, loss=0.672, regression_loss=0.219]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.917, loss=1.06, regression_loss=0.147]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:53<00:00,  1.88batch/s, classification_loss=0.642, loss=0.773, regression_loss=0.13] \n",
      "Epoch 6: 100%|██████████| 100/100 [00:54<00:00,  1.84batch/s, classification_loss=0.88, loss=0.972, regression_loss=0.0916]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:53<00:00,  1.86batch/s, classification_loss=0.745, loss=0.881, regression_loss=0.136]\n",
      "Epoch 6: 100%|██████████| 100/100 [00:53<00:00,  1.86batch/s, classification_loss=0.744, loss=0.897, regression_loss=0.153]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:53<00:00,  1.88batch/s, classification_loss=0.786, loss=0.996, regression_loss=0.21] \n",
      "Epoch 7: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.474, loss=0.603, regression_loss=0.129]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.558, loss=0.754, regression_loss=0.196]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:53<00:00,  1.88batch/s, classification_loss=0.515, loss=0.729, regression_loss=0.214]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.747, loss=0.866, regression_loss=0.119]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.424, loss=0.561, regression_loss=0.137]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:54<00:00,  1.84batch/s, classification_loss=0.843, loss=0.945, regression_loss=0.101]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:54<00:00,  1.84batch/s, classification_loss=0.487, loss=0.605, regression_loss=0.118]\n",
      "Epoch 7: 100%|██████████| 100/100 [00:54<00:00,  1.85batch/s, classification_loss=0.778, loss=0.906, regression_loss=0.128]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.809, loss=1.02, regression_loss=0.212]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:53<00:00,  1.88batch/s, classification_loss=0.404, loss=0.534, regression_loss=0.129]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.422, loss=0.585, regression_loss=0.163]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.424, loss=0.629, regression_loss=0.205]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:52<00:00,  1.89batch/s, classification_loss=1.12, loss=1.26, regression_loss=0.145] ]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:53<00:00,  1.88batch/s, classification_loss=0.612, loss=0.727, regression_loss=0.115]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:54<00:00,  1.84batch/s, classification_loss=0.963, loss=1.09, regression_loss=0.13] ]\n",
      "Epoch 8: 100%|██████████| 100/100 [00:54<00:00,  1.84batch/s, classification_loss=0.55, loss=0.655, regression_loss=0.105] \n",
      "Epoch 9: 100%|██████████| 100/100 [00:53<00:00,  1.89batch/s, classification_loss=0.868, loss=1.1, regression_loss=0.228]] \n",
      "Epoch 8: 100%|██████████| 100/100 [00:54<00:00,  1.83batch/s, classification_loss=0.925, loss=1.09, regression_loss=0.165]/it]\n",
      "  0%|          | 0/100 [00:00<?, ?batch/s]:40,  1.99batch/s, classification_loss=0.864, loss=0.983, regression_loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [00:52<00:00,  1.90batch/s, classification_loss=0.704, loss=0.851, regression_loss=0.147]\n",
      "  0%|          | 0/100 [00:00<?, ?batch/s]:22,  2.05batch/s, classification_loss=0.41, loss=0.628, regression_loss=0.218] /it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [00:53<00:00,  1.88batch/s, classification_loss=0.646, loss=0.858, regression_loss=0.212]\n",
      "Epoch 9:  42%|████▏     | 42/100 [00:22<00:29,  1.99batch/s, classification_loss=0.376, loss=0.504, regression_loss=0.128]/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [00:53<00:00,  1.88batch/s, classification_loss=0.511, loss=0.713, regression_loss=0.203]\n",
      "Epoch 9: 100%|██████████| 100/100 [00:53<00:00,  1.87batch/s, classification_loss=0.954, loss=1.08, regression_loss=0.126] it]\n",
      "Epoch 9: 100%|██████████| 100/100 [00:53<00:00,  1.89batch/s, classification_loss=0.495, loss=0.623, regression_loss=0.129]it]\n",
      "Epoch 0:  65%|██████▌   | 65/100 [00:34<00:15,  2.30batch/s, classification_loss=1.04, loss=1.28, regression_loss=0.234]9]/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 1/100 [00:00<00:49,  2.01batch/s, classification_loss=1.34, loss=1.56, regression_loss=0.22]8]6]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?batch/s]56,  1.74batch/s, classification_loss=1.41, loss=1.68, regression_loss=0.273]]]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [00:54<00:00,  1.83batch/s, classification_loss=0.723, loss=0.833, regression_loss=0.111]\n",
      "Epoch 9: 100%|██████████| 100/100 [00:54<00:00,  1.85batch/s, classification_loss=0.493, loss=0.614, regression_loss=0.121]it]\n",
      "Epoch 0:  89%|████████▉ | 89/100 [00:48<00:05,  1.90batch/s, classification_loss=1.05, loss=1.31, regression_loss=0.258]]]/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:  99%|█████████▉| 99/100 [00:53<00:00,  1.84batch/s, classification_loss=0.864, loss=1.02, regression_loss=0.152] ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [00:54<00:00,  1.85batch/s, classification_loss=1.1, loss=1.25, regression_loss=0.149] ] \n",
      "Epoch 0: 100%|██████████| 100/100 [00:54<00:00,  1.84batch/s, classification_loss=0.95, loss=1.2, regression_loss=0.251] ] it]\n",
      "Epoch 0:  34%|███▍      | 34/100 [00:19<00:36,  1.80batch/s, classification_loss=0.673, loss=0.798, regression_loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 100/100 [00:55<00:00,  1.81batch/s, classification_loss=0.842, loss=0.979, regression_loss=0.137]\n",
      "Epoch 0: 100%|██████████| 100/100 [00:56<00:00,  1.76batch/s, classification_loss=1.19, loss=1.34, regression_loss=0.145]]\n",
      "Epoch 0: 100%|██████████| 100/100 [01:00<00:00,  1.65batch/s, classification_loss=0.445, loss=0.623, regression_loss=0.178]\n",
      "Epoch 0: 100%|██████████| 100/100 [01:00<00:00,  1.64batch/s, classification_loss=0.885, loss=1.13, regression_loss=0.248]\n",
      "Epoch 0: 100%|██████████| 100/100 [01:00<00:00,  1.66batch/s, classification_loss=0.761, loss=0.889, regression_loss=0.128]\n",
      "Epoch 0: 100%|██████████| 100/100 [01:01<00:00,  1.63batch/s, classification_loss=1.03, loss=1.23, regression_loss=0.2] ]] \n",
      "Epoch 0: 100%|██████████| 100/100 [01:01<00:00,  1.62batch/s, classification_loss=0.693, loss=0.892, regression_loss=0.199]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:59<00:00,  1.67batch/s, classification_loss=0.822, loss=1.06, regression_loss=0.236]\n",
      "Epoch 0: 100%|██████████| 100/100 [01:01<00:00,  1.61batch/s, classification_loss=0.99, loss=1.13, regression_loss=0.145]  \n",
      "Epoch 1: 100%|██████████| 100/100 [01:00<00:00,  1.66batch/s, classification_loss=0.796, loss=0.908, regression_loss=0.112]\n",
      "Epoch 1: 100%|██████████| 100/100 [00:59<00:00,  1.67batch/s, classification_loss=0.919, loss=1.05, regression_loss=0.128] \n",
      "Epoch 1: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=1.44, loss=1.63, regression_loss=0.186]] \n",
      "Epoch 1: 100%|██████████| 100/100 [01:01<00:00,  1.62batch/s, classification_loss=0.909, loss=1.04, regression_loss=0.134]\n",
      "Epoch 1: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.632, loss=0.897, regression_loss=0.265]\n",
      "Epoch 1: 100%|██████████| 100/100 [01:01<00:00,  1.62batch/s, classification_loss=0.798, loss=0.997, regression_loss=0.199]\n",
      "Epoch 1: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.776, loss=0.977, regression_loss=0.201]\n",
      "Epoch 2: 100%|██████████| 100/100 [00:59<00:00,  1.67batch/s, classification_loss=0.62, loss=0.894, regression_loss=0.275]\n",
      "Epoch 1: 100%|██████████| 100/100 [01:01<00:00,  1.61batch/s, classification_loss=0.893, loss=1.03, regression_loss=0.137] \n",
      "Epoch 2: 100%|██████████| 100/100 [01:00<00:00,  1.65batch/s, classification_loss=0.85, loss=0.976, regression_loss=0.126]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:00<00:00,  1.66batch/s, classification_loss=0.892, loss=1.02, regression_loss=0.128] \n",
      "Epoch 2: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=1.2, loss=1.37, regression_loss=0.171] ] \n",
      "Epoch 2: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.996, loss=1.15, regression_loss=0.153]]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.633, loss=0.878, regression_loss=0.245]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.706, loss=0.842, regression_loss=0.136]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:00<00:00,  1.66batch/s, classification_loss=0.557, loss=0.811, regression_loss=0.254]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.789, loss=0.986, regression_loss=0.197]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:01<00:00,  1.61batch/s, classification_loss=0.657, loss=0.759, regression_loss=0.102]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:00<00:00,  1.66batch/s, classification_loss=0.916, loss=1.04, regression_loss=0.127] \n",
      "Epoch 3: 100%|██████████| 100/100 [01:00<00:00,  1.66batch/s, classification_loss=0.632, loss=0.731, regression_loss=0.0988]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.94, loss=1.1, regression_loss=0.163]85]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:01<00:00,  1.62batch/s, classification_loss=0.777, loss=0.9, regression_loss=0.122]] \n",
      "Epoch 3: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.456, loss=0.673, regression_loss=0.218]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:00<00:00,  1.66batch/s, classification_loss=1.12, loss=1.38, regression_loss=0.261]] \n",
      "Epoch 3: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.693, loss=0.825, regression_loss=0.132]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.667, loss=0.764, regression_loss=0.0969]\n",
      "Epoch 4: 100%|██████████| 100/100 [00:59<00:00,  1.67batch/s, classification_loss=0.927, loss=1.04, regression_loss=0.117] \n",
      "Epoch 3: 100%|██████████| 100/100 [01:01<00:00,  1.61batch/s, classification_loss=0.748, loss=0.933, regression_loss=0.184]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:00<00:00,  1.66batch/s, classification_loss=0.929, loss=1.04, regression_loss=0.113] \n",
      "Epoch 4: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.793, loss=0.93, regression_loss=0.137] \n",
      "Epoch 4: 100%|██████████| 100/100 [01:01<00:00,  1.62batch/s, classification_loss=1.11, loss=1.25, regression_loss=0.138]]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.554, loss=0.773, regression_loss=0.219]\n",
      "Epoch 5: 100%|██████████| 100/100 [00:59<00:00,  1.67batch/s, classification_loss=0.597, loss=0.85, regression_loss=0.253] \n",
      "Epoch 4: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.513, loss=0.63, regression_loss=0.116]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=1.11, loss=1.28, regression_loss=0.171]] \n",
      "Epoch 5: 100%|██████████| 100/100 [01:00<00:00,  1.66batch/s, classification_loss=0.702, loss=0.806, regression_loss=0.104]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:01<00:00,  1.61batch/s, classification_loss=0.795, loss=0.968, regression_loss=0.174]\n",
      "Epoch 5: 100%|██████████| 100/100 [01:00<00:00,  1.67batch/s, classification_loss=0.744, loss=0.844, regression_loss=0.1]] \n",
      "Epoch 5: 100%|██████████| 100/100 [01:01<00:00,  1.62batch/s, classification_loss=1.03, loss=1.17, regression_loss=0.148]] \n",
      "Epoch 5: 100%|██████████| 100/100 [01:01<00:00,  1.61batch/s, classification_loss=0.757, loss=0.89, regression_loss=0.134]\n",
      "Epoch 5: 100%|██████████| 100/100 [01:01<00:00,  1.62batch/s, classification_loss=0.443, loss=0.658, regression_loss=0.214]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:00<00:00,  1.66batch/s, classification_loss=0.433, loss=0.692, regression_loss=0.259]\n",
      "Epoch 5: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.593, loss=0.72, regression_loss=0.127]]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:00<00:00,  1.67batch/s, classification_loss=0.808, loss=0.93, regression_loss=0.122] \n",
      "Epoch 5: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.854, loss=1.01, regression_loss=0.152]\n",
      "Epoch 5: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.453, loss=0.571, regression_loss=0.118]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:00<00:00,  1.65batch/s, classification_loss=1.29, loss=1.43, regression_loss=0.145]  \n",
      "Epoch 6: 100%|██████████| 100/100 [01:01<00:00,  1.62batch/s, classification_loss=0.712, loss=0.824, regression_loss=0.111]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.84, loss=0.959, regression_loss=0.119]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.423, loss=0.651, regression_loss=0.228]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:00<00:00,  1.65batch/s, classification_loss=0.441, loss=0.672, regression_loss=0.231]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:01<00:00,  1.63batch/s, classification_loss=0.602, loss=0.705, regression_loss=0.103]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:01<00:00,  1.64batch/s, classification_loss=0.821, loss=0.917, regression_loss=0.0961]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:01<00:00,  1.62batch/s, classification_loss=0.93, loss=1.06, regression_loss=0.126]]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:01<00:00,  1.62batch/s, classification_loss=0.574, loss=0.74, regression_loss=0.166] \n",
      "Epoch 7: 100%|██████████| 100/100 [01:01<00:00,  1.64batch/s, classification_loss=0.916, loss=1.03, regression_loss=0.115] \n",
      "Epoch 7: 100%|██████████| 100/100 [01:01<00:00,  1.62batch/s, classification_loss=0.945, loss=1.09, regression_loss=0.143] \n",
      "Epoch 7: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.996, loss=1.15, regression_loss=0.149]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.609, loss=0.852, regression_loss=0.244]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:00<00:00,  1.66batch/s, classification_loss=0.472, loss=0.759, regression_loss=0.287]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.517, loss=0.633, regression_loss=0.116]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:00<00:00,  1.66batch/s, classification_loss=0.957, loss=1.08, regression_loss=0.124] \n",
      "Epoch 7: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.742, loss=0.855, regression_loss=0.113]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.523, loss=0.647, regression_loss=0.124]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:00<00:00,  1.66batch/s, classification_loss=0.838, loss=0.986, regression_loss=0.148]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:01<00:00,  1.62batch/s, classification_loss=0.898, loss=1.02, regression_loss=0.122] \n",
      "Epoch 8: 100%|██████████| 100/100 [01:01<00:00,  1.62batch/s, classification_loss=0.97, loss=1.15, regression_loss=0.181]]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:02<00:00,  1.61batch/s, classification_loss=0.4, loss=0.654, regression_loss=0.255] \n",
      "Epoch 9: 100%|██████████| 100/100 [01:00<00:00,  1.65batch/s, classification_loss=0.419, loss=0.665, regression_loss=0.246]\n",
      "Epoch 9: 100%|██████████| 100/100 [00:59<00:00,  1.67batch/s, classification_loss=0.953, loss=1.07, regression_loss=0.118]]it]\n",
      " 54%|███████████████████████████████████████████████▎                                        | 29/54 [27:08<51:03, 122.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 100/100 [01:00<00:00,  1.64batch/s, classification_loss=0.522, loss=0.639, regression_loss=0.117]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:00<00:00,  1.65batch/s, classification_loss=0.822, loss=0.939, regression_loss=0.117]\n",
      "Epoch 9:  95%|█████████▌| 95/100 [00:56<00:02,  1.77batch/s, classification_loss=0.754, loss=0.859, regression_loss=0.105] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 100/100 [01:00<00:00,  1.65batch/s, classification_loss=0.606, loss=0.748, regression_loss=0.142]\n",
      "Epoch 9: 100%|██████████| 100/100 [00:59<00:00,  1.68batch/s, classification_loss=0.779, loss=0.886, regression_loss=0.108]\n",
      "  0%|          | 0/100 [00:00<?, ?batch/s]:45,  1.70batch/s, classification_loss=0.635, loss=0.753, regression_loss=0.118]/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [01:01<00:00,  1.64batch/s, classification_loss=0.778, loss=nan, regression_loss=nan]65] \n",
      "Epoch 9: 100%|██████████| 100/100 [01:01<00:00,  1.64batch/s, classification_loss=0.871, loss=1.01, regression_loss=0.14]]\n",
      "Epoch 9: 100%|██████████| 100/100 [01:00<00:00,  1.65batch/s, classification_loss=0.517, loss=0.751, regression_loss=0.234]it]\n",
      "  0%|          | 0/100 [00:00<?, ?batch/s]?batch/s]9batch/s, classification_loss=0.876, loss=0.991, regression_loss=0.115]/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n",
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:  91%|█████████ | 91/100 [00:54<00:04,  1.87batch/s, classification_loss=0.753, loss=0.857, regression_loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [01:00<00:00,  1.66batch/s, classification_loss=0.586, loss=0.733, regression_loss=0.147]\n",
      "Epoch 9: 100%|██████████| 100/100 [01:00<00:00,  1.66batch/s, classification_loss=0.982, loss=1.1, regression_loss=0.114]  it]\n",
      "Epoch 9:  99%|█████████▉| 99/100 [00:59<00:00,  1.83batch/s, classification_loss=0.585, loss=0.74, regression_loss=0.156] /it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [01:00<00:00,  1.66batch/s, classification_loss=0.588, loss=0.732, regression_loss=0.144]\n",
      "  0%|          | 0/100 [00:00<?, ?batch/s]:27,  1.19batch/s, classification_loss=1.25, loss=1.42, regression_loss=0.175]9]/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  23%|██▎       | 23/100 [00:21<01:09,  1.10batch/s, classification_loss=1.06, loss=1.26, regression_loss=0.208]]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 100/100 [01:35<00:00,  1.04batch/s, classification_loss=0.659, loss=0.843, regression_loss=0.184]\n",
      "Epoch 0: 100%|██████████| 100/100 [01:36<00:00,  1.03batch/s, classification_loss=1.12, loss=1.33, regression_loss=0.207]]\n",
      "Epoch 0: 100%|██████████| 100/100 [01:37<00:00,  1.02batch/s, classification_loss=0.955, loss=1.17, regression_loss=0.217]\n",
      "Epoch 0: 100%|██████████| 100/100 [01:39<00:00,  1.00batch/s, classification_loss=0.943, loss=1.15, regression_loss=0.204] \n",
      "Epoch 0: 100%|██████████| 100/100 [01:40<00:00,  1.00s/batch, classification_loss=0.909, loss=1.04, regression_loss=0.135]\n",
      "Epoch 0: 100%|██████████| 100/100 [01:40<00:00,  1.00s/batch, classification_loss=0.979, loss=1.2, regression_loss=0.221]]\n",
      "Epoch 0: 100%|██████████| 100/100 [01:42<00:00,  1.02s/batch, classification_loss=0.769, loss=1.01, regression_loss=0.239] \n",
      "Epoch 0: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.936, loss=1.16, regression_loss=0.22]]\n",
      "Epoch 0: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.814, loss=1, regression_loss=0.186]   \n",
      "Epoch 1: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.793, loss=0.993, regression_loss=0.2] \n",
      "Epoch 1: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.697, loss=0.863, regression_loss=0.166]\n",
      "Epoch 1: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.968, loss=1.19, regression_loss=0.22] \n",
      "Epoch 1: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.817, loss=1.04, regression_loss=0.22] \n",
      "Epoch 1: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.826, loss=1, regression_loss=0.174]   \n",
      "Epoch 1: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.871, loss=1.1, regression_loss=0.227]]\n",
      "Epoch 1: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.834, loss=1.09, regression_loss=0.256]\n",
      "Epoch 1: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.739, loss=0.936, regression_loss=0.196]\n",
      "Epoch 1: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.789, loss=0.974, regression_loss=0.185]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.759, loss=0.905, regression_loss=0.146]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.764, loss=0.991, regression_loss=0.227]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.961, loss=1.18, regression_loss=0.219]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.897, loss=1.12, regression_loss=0.224] \n",
      "Epoch 2: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.653, loss=0.779, regression_loss=0.126]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.917, loss=1.2, regression_loss=0.285]]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.765, loss=1.02, regression_loss=0.252]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.649, loss=0.87, regression_loss=0.221]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.985, loss=1.25, regression_loss=0.264]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:42<00:00,  1.02s/batch, classification_loss=0.801, loss=0.977, regression_loss=0.175]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.682, loss=0.895, regression_loss=0.213]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.93, loss=1.19, regression_loss=0.264]]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=1.01, loss=1.23, regression_loss=0.217]]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.556, loss=0.714, regression_loss=0.157]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.979, loss=1.29, regression_loss=0.313]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.655, loss=0.88, regression_loss=0.225]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.59, loss=0.792, regression_loss=0.202]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.811, loss=0.961, regression_loss=0.15]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.825, loss=1.05, regression_loss=0.22]]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:42<00:00,  1.02s/batch, classification_loss=0.621, loss=0.852, regression_loss=0.231]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.942, loss=1.15, regression_loss=0.206]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.875, loss=1.08, regression_loss=0.201] \n",
      "Epoch 4: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.457, loss=0.617, regression_loss=0.161]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.803, loss=1.08, regression_loss=0.281]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.653, loss=0.907, regression_loss=0.254]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.493, loss=0.717, regression_loss=0.224]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=1.05, loss=1.27, regression_loss=0.221]]\n",
      "Epoch 5: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.916, loss=1.12, regression_loss=0.202] \n",
      "Epoch 5: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.648, loss=0.832, regression_loss=0.184]\n",
      "Epoch 5: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.835, loss=1.01, regression_loss=0.178]\n",
      "Epoch 5: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.917, loss=1.13, regression_loss=0.214] \n",
      "Epoch 5: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.443, loss=0.576, regression_loss=0.133]\n",
      "Epoch 5: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.848, loss=1.07, regression_loss=0.218]\n",
      "Epoch 5: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.776, loss=1.01, regression_loss=0.23] \n",
      "Epoch 5: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.423, loss=0.633, regression_loss=0.211]\n",
      "Epoch 5: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.808, loss=1.01, regression_loss=0.202]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.736, loss=0.948, regression_loss=0.212]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.392, loss=0.579, regression_loss=0.187]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.741, loss=0.933, regression_loss=0.192]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.699, loss=0.898, regression_loss=0.198]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.582, loss=0.753, regression_loss=0.171]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.825, loss=1.05, regression_loss=0.23]]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.729, loss=0.947, regression_loss=0.218]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.508, loss=0.696, regression_loss=0.188]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.895, loss=1.1, regression_loss=0.204]]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.965, loss=1.14, regression_loss=0.173]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.583, loss=0.83, regression_loss=0.247]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=1.03, loss=1.24, regression_loss=0.208]]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.825, loss=1.04, regression_loss=0.213]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.618, loss=0.76, regression_loss=0.142]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.867, loss=1.11, regression_loss=0.242]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.837, loss=1.08, regression_loss=0.242]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.433, loss=0.65, regression_loss=0.216]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.841, loss=1.01, regression_loss=0.166]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.804, loss=1.02, regression_loss=0.221]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:42<00:00,  1.02s/batch, classification_loss=0.366, loss=0.565, regression_loss=0.199]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.931, loss=1.11, regression_loss=0.18]]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.814, loss=1.03, regression_loss=0.214] \n",
      "Epoch 8: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.515, loss=0.688, regression_loss=0.173]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.989, loss=1.24, regression_loss=0.251]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=0.775, loss=1.03, regression_loss=0.252]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.542, loss=0.749, regression_loss=0.207]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:43<00:00,  1.03s/batch, classification_loss=1.02, loss=1.19, regression_loss=0.174] \n",
      "Epoch 9: 100%|██████████| 100/100 [01:42<00:00,  1.03s/batch, classification_loss=0.768, loss=0.937, regression_loss=0.169]\n",
      "Epoch 9: 100%|██████████| 100/100 [01:42<00:00,  1.02s/batch, classification_loss=0.43, loss=0.603, regression_loss=0.173]/it]\n",
      "Epoch 9:  52%|█████▏    | 52/100 [00:52<00:42,  1.12batch/s, classification_loss=0.882, loss=1.09, regression_loss=0.212]]/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   7%|▋         | 7/100 [00:06<01:28,  1.06batch/s, classification_loss=1.61, loss=1.77, regression_loss=0.151]26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [01:41<00:00,  1.02s/batch, classification_loss=0.789, loss=0.969, regression_loss=0.18]\n",
      "Epoch 9:  45%|████▌     | 45/100 [00:44<00:52,  1.06batch/s, classification_loss=0.82, loss=1, regression_loss=0.185]    ]/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [01:40<00:00,  1.01s/batch, classification_loss=0.784, loss=0.992, regression_loss=0.208]\n",
      "Epoch 9: 100%|██████████| 100/100 [01:41<00:00,  1.01s/batch, classification_loss=1.03, loss=1.27, regression_loss=0.244]]\n",
      "Epoch 9: 100%|██████████| 100/100 [01:41<00:00,  1.02s/batch, classification_loss=0.497, loss=0.658, regression_loss=0.161]\n",
      "Epoch 9:  81%|████████  | 81/100 [01:20<00:15,  1.20batch/s, classification_loss=0.729, loss=0.894, regression_loss=0.165]/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?batch/s]:13,  1.18batch/s, classification_loss=0.745, loss=0.933, regression_loss=0.187]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n",
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 100/100 [01:40<00:00,  1.00s/batch, classification_loss=0.69, loss=0.91, regression_loss=0.22] ]\n",
      "Epoch 9: 100%|██████████| 100/100 [01:39<00:00,  1.00batch/s, classification_loss=0.373, loss=0.572, regression_loss=0.199]it]\n",
      "Epoch 9: 100%|██████████| 100/100 [01:39<00:00,  1.00batch/s, classification_loss=0.794, loss=0.993, regression_loss=0.199]it]\n",
      "Epoch 0:  16%|█▌        | 16/100 [00:16<01:17,  1.09batch/s, classification_loss=1.33, loss=1.55, regression_loss=0.216]]]/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  71%|███████   | 71/100 [01:10<00:25,  1.14batch/s, classification_loss=0.872, loss=1.02, regression_loss=0.143] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  83%|████████▎ | 83/100 [01:22<00:16,  1.05batch/s, classification_loss=1.31, loss=1.44, regression_loss=0.135]] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing gpu Mamba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 100/100 [01:40<00:00,  1.01s/batch, classification_loss=1.21, loss=1.33, regression_loss=0.128]]\n",
      "Epoch 0: 100%|██████████| 100/100 [01:41<00:00,  1.02s/batch, classification_loss=0.727, loss=0.845, regression_loss=0.118]\n",
      "Epoch 0: 100%|██████████| 100/100 [01:43<00:00,  1.04s/batch, classification_loss=1.2, loss=1.42, regression_loss=0.226]1]\n",
      "Epoch 0: 100%|██████████| 100/100 [01:48<00:00,  1.09s/batch, classification_loss=0.935, loss=1.13, regression_loss=0.198]\n",
      "Epoch 0: 100%|██████████| 100/100 [01:49<00:00,  1.10s/batch, classification_loss=0.509, loss=0.647, regression_loss=0.138]\n",
      "Epoch 0: 100%|██████████| 100/100 [01:50<00:00,  1.10s/batch, classification_loss=1.04, loss=1.27, regression_loss=0.229]\n",
      "Epoch 0: 100%|██████████| 100/100 [01:51<00:00,  1.11s/batch, classification_loss=1.16, loss=1.31, regression_loss=0.149] \n",
      "Epoch 0: 100%|██████████| 100/100 [01:53<00:00,  1.13s/batch, classification_loss=1.02, loss=1.25, regression_loss=0.232] \n",
      "Epoch 0: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=1.12, loss=1.26, regression_loss=0.141]]\n",
      "Epoch 1: 100%|██████████| 100/100 [01:51<00:00,  1.12s/batch, classification_loss=0.865, loss=1.02, regression_loss=0.158]\n",
      "Epoch 1: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=0.814, loss=0.97, regression_loss=0.156]\n",
      "Epoch 1: 100%|██████████| 100/100 [01:51<00:00,  1.12s/batch, classification_loss=0.795, loss=1.04, regression_loss=0.244]\n",
      "Epoch 1: 100%|██████████| 100/100 [01:53<00:00,  1.13s/batch, classification_loss=0.814, loss=1.02, regression_loss=0.205]\n",
      "Epoch 1: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.545, loss=0.76, regression_loss=0.215]\n",
      "Epoch 1: 100%|██████████| 100/100 [01:53<00:00,  1.13s/batch, classification_loss=0.758, loss=0.903, regression_loss=0.145]\n",
      "Epoch 1: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=0.963, loss=1.11, regression_loss=0.148]\n",
      "Epoch 1: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.848, loss=1.05, regression_loss=0.202]\n",
      "Epoch 1: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=0.921, loss=1.07, regression_loss=0.154]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:51<00:00,  1.12s/batch, classification_loss=0.708, loss=0.851, regression_loss=0.143]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:51<00:00,  1.12s/batch, classification_loss=0.886, loss=1.05, regression_loss=0.167]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=0.957, loss=1.21, regression_loss=0.254]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=0.963, loss=1.19, regression_loss=0.228]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=0.585, loss=0.794, regression_loss=0.208]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=0.772, loss=0.903, regression_loss=0.131]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=0.713, loss=0.848, regression_loss=0.135]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=0.685, loss=0.895, regression_loss=0.21]\n",
      "Epoch 2: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=0.697, loss=0.831, regression_loss=0.134]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:50<00:00,  1.11s/batch, classification_loss=0.856, loss=0.991, regression_loss=0.135]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:51<00:00,  1.11s/batch, classification_loss=0.903, loss=1.05, regression_loss=0.146]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:51<00:00,  1.12s/batch, classification_loss=0.875, loss=1.16, regression_loss=0.289]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=1.03, loss=1.26, regression_loss=0.225]] \n",
      "Epoch 3: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=0.48, loss=0.696, regression_loss=0.217]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=0.643, loss=0.758, regression_loss=0.115]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.927, loss=1.09, regression_loss=0.166] \n",
      "Epoch 3: 100%|██████████| 100/100 [01:51<00:00,  1.12s/batch, classification_loss=1.02, loss=1.25, regression_loss=0.233]]\n",
      "Epoch 3: 100%|██████████| 100/100 [01:51<00:00,  1.12s/batch, classification_loss=0.716, loss=0.846, regression_loss=0.13]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:50<00:00,  1.11s/batch, classification_loss=0.694, loss=0.826, regression_loss=0.132]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:50<00:00,  1.11s/batch, classification_loss=0.759, loss=0.892, regression_loss=0.133]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:51<00:00,  1.12s/batch, classification_loss=0.971, loss=1.21, regression_loss=0.241]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.729, loss=0.94, regression_loss=0.211]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=0.484, loss=0.691, regression_loss=0.207]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.778, loss=0.933, regression_loss=0.155]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=0.71, loss=0.841, regression_loss=0.131]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:51<00:00,  1.12s/batch, classification_loss=0.67, loss=0.866, regression_loss=0.197]\n",
      "Epoch 4: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.742, loss=0.889, regression_loss=0.147]\n",
      "Epoch 5: 100%|██████████| 100/100 [01:51<00:00,  1.11s/batch, classification_loss=0.588, loss=0.73, regression_loss=0.142] \n",
      "Epoch 5: 100%|██████████| 100/100 [01:51<00:00,  1.11s/batch, classification_loss=0.892, loss=1.02, regression_loss=0.13]  \n",
      "Epoch 5: 100%|██████████| 100/100 [01:51<00:00,  1.11s/batch, classification_loss=0.793, loss=1.03, regression_loss=0.235]\n",
      "Epoch 5: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.839, loss=1.05, regression_loss=0.207]\n",
      "Epoch 5: 100%|██████████| 100/100 [01:51<00:00,  1.12s/batch, classification_loss=0.522, loss=0.748, regression_loss=0.225]\n",
      "Epoch 5: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.882, loss=1.02, regression_loss=0.139] \n",
      "Epoch 5: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=1.25, loss=1.41, regression_loss=0.16]3]\n",
      "Epoch 5: 100%|██████████| 100/100 [01:53<00:00,  1.13s/batch, classification_loss=0.766, loss=0.98, regression_loss=0.214]\n",
      "Epoch 5: 100%|██████████| 100/100 [01:53<00:00,  1.13s/batch, classification_loss=0.674, loss=0.803, regression_loss=0.128]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:51<00:00,  1.12s/batch, classification_loss=0.514, loss=0.686, regression_loss=0.172]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:50<00:00,  1.11s/batch, classification_loss=1.1, loss=1.28, regression_loss=0.181]  \n",
      "Epoch 6: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.698, loss=0.938, regression_loss=0.24]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.716, loss=0.92, regression_loss=0.204]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.526, loss=0.739, regression_loss=0.213]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.717, loss=0.85, regression_loss=0.133]\n",
      "Epoch 6: 100%|██████████| 100/100 [01:53<00:00,  1.13s/batch, classification_loss=0.924, loss=1.08, regression_loss=0.156] \n",
      "Epoch 6: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.655, loss=0.855, regression_loss=0.2] \n",
      "Epoch 6: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=0.907, loss=1.09, regression_loss=0.179]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:51<00:00,  1.11s/batch, classification_loss=0.579, loss=0.738, regression_loss=0.159]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:51<00:00,  1.12s/batch, classification_loss=0.872, loss=1.04, regression_loss=0.164]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=0.851, loss=1.07, regression_loss=0.219]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.84, loss=1.04, regression_loss=0.204]]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.484, loss=0.696, regression_loss=0.212]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=1.07, loss=1.23, regression_loss=0.168] \n",
      "Epoch 7: 100%|██████████| 100/100 [01:53<00:00,  1.13s/batch, classification_loss=0.594, loss=0.713, regression_loss=0.12]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.684, loss=0.884, regression_loss=0.201]\n",
      "Epoch 7: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.763, loss=0.912, regression_loss=0.149]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:51<00:00,  1.12s/batch, classification_loss=0.596, loss=0.721, regression_loss=0.125]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:51<00:00,  1.11s/batch, classification_loss=0.718, loss=0.826, regression_loss=0.108]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:51<00:00,  1.12s/batch, classification_loss=0.737, loss=0.962, regression_loss=0.224]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.732, loss=0.928, regression_loss=0.196]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.411, loss=0.62, regression_loss=0.209]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.942, loss=1.12, regression_loss=0.174]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.81, loss=0.929, regression_loss=0.12]]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:52<00:00,  1.13s/batch, classification_loss=0.893, loss=1.11, regression_loss=0.219]\n",
      "Epoch 8: 100%|██████████| 100/100 [01:52<00:00,  1.12s/batch, classification_loss=0.892, loss=1.05, regression_loss=0.159] \n",
      "Epoch 9: 100%|██████████| 100/100 [01:51<00:00,  1.11s/batch, classification_loss=0.449, loss=0.582, regression_loss=0.133]\n",
      "Epoch 9: 100%|██████████| 100/100 [01:51<00:00,  1.11s/batch, classification_loss=0.776, loss=0.906, regression_loss=0.13]/it]\n",
      "Epoch 9: 100%|██████████| 100/100 [01:46<00:00,  1.07s/batch, classification_loss=0.924, loss=1.17, regression_loss=0.246]/it]\n",
      "Epoch 9: 100%|██████████| 100/100 [01:32<00:00,  1.08batch/s, classification_loss=0.967, loss=1.18, regression_loss=0.217]/it]\n",
      "Epoch 9: 100%|██████████| 100/100 [01:31<00:00,  1.10batch/s, classification_loss=0.455, loss=0.673, regression_loss=0.218]it]\n",
      "Epoch 9: 100%|██████████| 100/100 [01:30<00:00,  1.11batch/s, classification_loss=0.889, loss=1.04, regression_loss=0.149]/it]\n",
      "Epoch 9: 100%|██████████| 100/100 [01:18<00:00,  1.27batch/s, classification_loss=0.892, loss=1.05, regression_loss=0.158]/it]\n",
      "Epoch 9: 100%|██████████| 100/100 [01:17<00:00,  1.30batch/s, classification_loss=0.793, loss=1.02, regression_loss=0.222]/it]\n",
      "Epoch 9: 100%|██████████| 100/100 [01:13<00:00,  1.37batch/s, classification_loss=0.892, loss=1.07, regression_loss=0.181]/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 54/54 [1:03:45<00:00, 70.83s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "with Pool(processes=9) as pool:\n",
    "\n",
    "   # results = pool.map(train, all_tests[:2])\n",
    "    results = list(tqdm.tqdm(pool.imap(train, all_tests), total=len(all_tests)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b37ea12a-507c-4d93-a376-fecf675db8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bi_mamba_stacks': 1,\n",
       "  'conv_stack': 1,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [0.9471437186002731,\n",
       "   0.6781459105014801,\n",
       "   0.6374931555986404,\n",
       "   0.6481715422868729,\n",
       "   0.6011941874027252,\n",
       "   0.5730124297738075,\n",
       "   0.5731413260102272,\n",
       "   0.579996180832386,\n",
       "   0.5760558530688286,\n",
       "   0.5725092977285385],\n",
       "  'running_classification_total_loss': [0.7441304379701614,\n",
       "   0.49209186762571333,\n",
       "   0.45236151695251464,\n",
       "   0.4625474342703819,\n",
       "   0.4617332881689072,\n",
       "   0.44958816438913346,\n",
       "   0.44974243074655534,\n",
       "   0.4559462794661522,\n",
       "   0.4524635884165764,\n",
       "   0.44983031660318373],\n",
       "  'running_regression_total_loss': [0.20301328159868717,\n",
       "   0.18605404287576677,\n",
       "   0.18513163857162,\n",
       "   0.18562410794198514,\n",
       "   0.139460898861289,\n",
       "   0.12342426754534244,\n",
       "   0.12339889474213123,\n",
       "   0.12404990248382092,\n",
       "   0.12359226025640964,\n",
       "   0.12267898179590703]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'conv_stack': 1,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.3551732623577117,\n",
       "   0.9570073789358139,\n",
       "   0.8304852080345154,\n",
       "   0.6548484265804291,\n",
       "   0.5970098677277565,\n",
       "   0.5913438752293587,\n",
       "   0.5947250232100487,\n",
       "   0.5850448256731033,\n",
       "   0.578532680273056,\n",
       "   0.575497505068779],\n",
       "  'running_classification_total_loss': [0.9981088173389435,\n",
       "   0.7450161439180374,\n",
       "   0.6336030313372611,\n",
       "   0.5016389620304108,\n",
       "   0.4652042976021767,\n",
       "   0.46471446484327317,\n",
       "   0.467250594496727,\n",
       "   0.45942115485668183,\n",
       "   0.4537267252802849,\n",
       "   0.4512905332446098],\n",
       "  'running_regression_total_loss': [0.3570644445717335,\n",
       "   0.21199123650789262,\n",
       "   0.19688217729330063,\n",
       "   0.15320946142077446,\n",
       "   0.13180557236075402,\n",
       "   0.12662941195070743,\n",
       "   0.12747442580759524,\n",
       "   0.12562366940081118,\n",
       "   0.1248059543967247,\n",
       "   0.12420697160065174]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'conv_stack': 1,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [0.9027524217963219,\n",
       "   0.6259682974219323,\n",
       "   0.601558873951435,\n",
       "   0.5979138308763504,\n",
       "   0.5924309733510017,\n",
       "   0.5878683918714523,\n",
       "   0.588907433450222,\n",
       "   0.5857808217406273,\n",
       "   0.5843250316381454,\n",
       "   0.5874118089675904],\n",
       "  'running_classification_total_loss': [0.7293336084485054,\n",
       "   0.5004867714643478,\n",
       "   0.4772261843085289,\n",
       "   0.47350911259651185,\n",
       "   0.4684541314840317,\n",
       "   0.46390306323766706,\n",
       "   0.465598920583725,\n",
       "   0.46238153994083403,\n",
       "   0.46147064536809923,\n",
       "   0.46446881800889966],\n",
       "  'running_regression_total_loss': [0.17341881185770036,\n",
       "   0.1254815250635147,\n",
       "   0.12433269001543522,\n",
       "   0.12440471790730953,\n",
       "   0.12397684074938298,\n",
       "   0.12396532617509365,\n",
       "   0.12330851301550866,\n",
       "   0.12339928425848484,\n",
       "   0.12285438798367977,\n",
       "   0.12294299125671387]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'conv_stack': 1,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.453357903957367,\n",
       "   1.0319839894771576,\n",
       "   0.7193989980220795,\n",
       "   0.6340154874324798,\n",
       "   0.6078447639942169,\n",
       "   0.5916681325435639,\n",
       "   0.5894071772694588,\n",
       "   0.5893073388934136,\n",
       "   0.584381297826767,\n",
       "   0.5802932009100914],\n",
       "  'running_classification_total_loss': [1.2350421178340911,\n",
       "   0.8908900022506714,\n",
       "   0.591595216691494,\n",
       "   0.5075253555178643,\n",
       "   0.4822071620821953,\n",
       "   0.46696369975805285,\n",
       "   0.4651053601503372,\n",
       "   0.4655639311671257,\n",
       "   0.4604817309975624,\n",
       "   0.45673863440752027],\n",
       "  'running_regression_total_loss': [0.218315789103508,\n",
       "   0.14109398826956748,\n",
       "   0.12780377969145776,\n",
       "   0.12649013198912143,\n",
       "   0.12563760623335837,\n",
       "   0.12470443569123744,\n",
       "   0.1243018151819706,\n",
       "   0.12374340936541557,\n",
       "   0.12389956660568714,\n",
       "   0.12355456791818142]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'conv_stack': 1,\n",
       "  'dropout': 0.15,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [0.8584309703111649,\n",
       "   0.6199646151065826,\n",
       "   0.5952431201934815,\n",
       "   0.5823733866214752,\n",
       "   0.5844330736994743,\n",
       "   0.5897856691479683,\n",
       "   0.5836174201965332,\n",
       "   0.5799112892150879,\n",
       "   0.5839438846707344,\n",
       "   0.5855309936404228],\n",
       "  'running_classification_total_loss': [0.6324648067355156,\n",
       "   0.4675261175632477,\n",
       "   0.4719886901974678,\n",
       "   0.4587660446763039,\n",
       "   0.46051577389240267,\n",
       "   0.46659127473831175,\n",
       "   0.46019775688648223,\n",
       "   0.4565949335694313,\n",
       "   0.460831855237484,\n",
       "   0.4618227806687355],\n",
       "  'running_regression_total_loss': [0.225966165214777,\n",
       "   0.152438497915864,\n",
       "   0.12325443021953106,\n",
       "   0.12360734313726425,\n",
       "   0.12391730159521103,\n",
       "   0.12319439217448235,\n",
       "   0.12341966100037098,\n",
       "   0.1233163534849882,\n",
       "   0.12311203122138976,\n",
       "   0.12370821103453636]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'conv_stack': 1,\n",
       "  'dropout': 0.15,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.626227102279663,\n",
       "   1.054887329339981,\n",
       "   0.7958984166383744,\n",
       "   0.6930240684747696,\n",
       "   0.6324248987436295,\n",
       "   0.6236766028404236,\n",
       "   0.6187668618559837,\n",
       "   0.6125032690167427,\n",
       "   0.6104718706011772,\n",
       "   0.6087095353007317],\n",
       "  'running_classification_total_loss': [1.3010537767410277,\n",
       "   0.9098186147212982,\n",
       "   0.6541482728719711,\n",
       "   0.5552680572867393,\n",
       "   0.4971544229984283,\n",
       "   0.4896387293934822,\n",
       "   0.4847518754005432,\n",
       "   0.4802675077319145,\n",
       "   0.4795721772313118,\n",
       "   0.47930342763662337],\n",
       "  'running_regression_total_loss': [0.32517333440482615,\n",
       "   0.14506871365010737,\n",
       "   0.14175014421343804,\n",
       "   0.13775601349771022,\n",
       "   0.13527047604322434,\n",
       "   0.13403787560760974,\n",
       "   0.13401498787105084,\n",
       "   0.1322357616573572,\n",
       "   0.13089969396591186,\n",
       "   0.12940610736608504]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'conv_stack': 10,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [0.855734976530075,\n",
       "   0.6451456746459008,\n",
       "   0.6381046712398529,\n",
       "   0.6374484333395958,\n",
       "   0.6422145083546639,\n",
       "   0.6399355617165565,\n",
       "   0.6416608595848083,\n",
       "   0.6371568408608437,\n",
       "   0.6390287539362908,\n",
       "   0.6438907891511917],\n",
       "  'running_classification_total_loss': [0.6394256100058555,\n",
       "   0.4599979943037033,\n",
       "   0.45451236307621,\n",
       "   0.45254709661006925,\n",
       "   0.45799564480781557,\n",
       "   0.4566332572698593,\n",
       "   0.4578428214788437,\n",
       "   0.45209786474704744,\n",
       "   0.4559337896108627,\n",
       "   0.4593955418467522],\n",
       "  'running_regression_total_loss': [0.21630936913192272,\n",
       "   0.18514768160879613,\n",
       "   0.18359230749309063,\n",
       "   0.18490133754909038,\n",
       "   0.18421886198222637,\n",
       "   0.18330230355262755,\n",
       "   0.1838180375099182,\n",
       "   0.18505897283554076,\n",
       "   0.1830949654430151,\n",
       "   0.18449525214731693]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'conv_stack': 10,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.5063708710670471,\n",
       "   1.0574069970846176,\n",
       "   0.8081619811058044,\n",
       "   0.6773627209663391,\n",
       "   0.6089559763669967,\n",
       "   0.6042555889487267,\n",
       "   0.5870012125372887,\n",
       "   0.5831013903021812,\n",
       "   0.5864773017168045,\n",
       "   0.5785175728797912],\n",
       "  'running_classification_total_loss': [1.287401887178421,\n",
       "   0.9020366382598877,\n",
       "   0.6682906797528267,\n",
       "   0.5411014291644096,\n",
       "   0.47621814876794816,\n",
       "   0.4733020851016045,\n",
       "   0.4575809323787689,\n",
       "   0.45504762649536135,\n",
       "   0.46058705300092695,\n",
       "   0.4532745918631554],\n",
       "  'running_regression_total_loss': [0.21896898671984671,\n",
       "   0.15537035517394543,\n",
       "   0.13987130038440226,\n",
       "   0.1362612910568714,\n",
       "   0.13273782826960087,\n",
       "   0.1309535042196512,\n",
       "   0.1294202820211649,\n",
       "   0.1280537649989128,\n",
       "   0.12589024603366852,\n",
       "   0.12524297930300235]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'conv_stack': 10,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [0.9135107243061066,\n",
       "   0.6470787110924721,\n",
       "   0.586956395804882,\n",
       "   0.5829129424691201,\n",
       "   0.5814148631691932,\n",
       "   0.5804217991232872,\n",
       "   0.5729822009801865,\n",
       "   0.5803190281987191,\n",
       "   0.5711248674988747,\n",
       "   0.5793385165929794],\n",
       "  'running_classification_total_loss': [0.7629878118634223,\n",
       "   0.5172912400960922,\n",
       "   0.46281607359647753,\n",
       "   0.4613470312952995,\n",
       "   0.46088834196329115,\n",
       "   0.45869203567504885,\n",
       "   0.45203969061374666,\n",
       "   0.4587039056420326,\n",
       "   0.4497376891970635,\n",
       "   0.4591379851102829],\n",
       "  'running_regression_total_loss': [0.1505229091644287,\n",
       "   0.1297874715179205,\n",
       "   0.12414032392203808,\n",
       "   0.12156591348350049,\n",
       "   0.12052652426064014,\n",
       "   0.1217297650873661,\n",
       "   0.12094251170754433,\n",
       "   0.12161512441933155,\n",
       "   0.12138717852532864,\n",
       "   0.120200532451272]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'conv_stack': 10,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.391790521144867,\n",
       "   0.9384122925996781,\n",
       "   0.773539679646492,\n",
       "   0.6865141743421554,\n",
       "   0.6389483040571213,\n",
       "   0.618154702782631,\n",
       "   0.6046057114005089,\n",
       "   0.6049254348874092,\n",
       "   0.6074821725487709,\n",
       "   0.592573588192463],\n",
       "  'running_classification_total_loss': [1.1680442297458649,\n",
       "   0.7745263624191284,\n",
       "   0.6386968791484833,\n",
       "   0.5558180958032608,\n",
       "   0.5107415387034416,\n",
       "   0.49170473486185073,\n",
       "   0.48002627164125444,\n",
       "   0.4808472940325737,\n",
       "   0.48350037813186647,\n",
       "   0.469075568318367],\n",
       "  'running_regression_total_loss': [0.22374629184603692,\n",
       "   0.16388593263924123,\n",
       "   0.13484279818832876,\n",
       "   0.13069607712328435,\n",
       "   0.12820676505565642,\n",
       "   0.12644996516406537,\n",
       "   0.12457943990826607,\n",
       "   0.12407814003527165,\n",
       "   0.12398179292678833,\n",
       "   0.12349802367389202]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'conv_stack': 10,\n",
       "  'dropout': 0.15,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [0.971825276017189,\n",
       "   0.6759800177812576,\n",
       "   0.6842890369892121,\n",
       "   0.6034072890877724,\n",
       "   0.6202917489409446,\n",
       "   0.6061066606640816,\n",
       "   0.5952529856562614,\n",
       "   0.5919439992308617,\n",
       "   0.5915551921725273,\n",
       "   0.5977844706177712],\n",
       "  'running_classification_total_loss': [0.7605015581846237,\n",
       "   0.4689611518383026,\n",
       "   0.4786963912844658,\n",
       "   0.4776560661196709,\n",
       "   0.49566996335983277,\n",
       "   0.48170942306518555,\n",
       "   0.47229561030864714,\n",
       "   0.4679596570134163,\n",
       "   0.4680624571442604,\n",
       "   0.4740162777900696],\n",
       "  'running_regression_total_loss': [0.21132372155785561,\n",
       "   0.20701886519789695,\n",
       "   0.20559264585375786,\n",
       "   0.1257512242347002,\n",
       "   0.1246217842400074,\n",
       "   0.12439723536372185,\n",
       "   0.12295737564563751,\n",
       "   0.12398434087634086,\n",
       "   0.1234927362203598,\n",
       "   0.12376819223165512]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'conv_stack': 10,\n",
       "  'dropout': 0.15,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [2.001921225786209,\n",
       "   1.2070113825798034,\n",
       "   0.9351819014549255,\n",
       "   0.7839005494117737,\n",
       "   0.7056220525503158,\n",
       "   0.6888649332523346,\n",
       "   0.6764516502618789,\n",
       "   0.673303073644638,\n",
       "   0.6811530619859696,\n",
       "   0.6767174571752548],\n",
       "  'running_classification_total_loss': [1.3560228657722473,\n",
       "   0.9871917295455933,\n",
       "   0.7226978811621666,\n",
       "   0.5737225040793419,\n",
       "   0.495947054028511,\n",
       "   0.47837317883968355,\n",
       "   0.4665971508622169,\n",
       "   0.4676247414946556,\n",
       "   0.47549513131380083,\n",
       "   0.4716326686739922],\n",
       "  'running_regression_total_loss': [0.6458983699977398,\n",
       "   0.21981964752078056,\n",
       "   0.21248401954770088,\n",
       "   0.21017804697155953,\n",
       "   0.20967499643564225,\n",
       "   0.21049175336956977,\n",
       "   0.20985450103878975,\n",
       "   0.2056783302128315,\n",
       "   0.2056579276919365,\n",
       "   0.20508478939533234]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'conv_stack': 20,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [0.7990097811818123,\n",
       "   0.6040704688429832,\n",
       "   0.5809108480811119,\n",
       "   0.5773942643404006,\n",
       "   0.5778636556863784,\n",
       "   0.5749955263733864,\n",
       "   0.577115781903267,\n",
       "   0.5795889854431152,\n",
       "   0.578697276711464,\n",
       "   0.5824201115965844],\n",
       "  'running_classification_total_loss': [0.6599629917740821,\n",
       "   0.4798827812075615,\n",
       "   0.4581113836169243,\n",
       "   0.45531311750411985,\n",
       "   0.45461235284805296,\n",
       "   0.4520798900723457,\n",
       "   0.4536283487081528,\n",
       "   0.4564458209276199,\n",
       "   0.45539775371551516,\n",
       "   0.45991118758916855],\n",
       "  'running_regression_total_loss': [0.13904679007828236,\n",
       "   0.12418768420815468,\n",
       "   0.12279946722090244,\n",
       "   0.12208114832639694,\n",
       "   0.12325130507349968,\n",
       "   0.12291563473641873,\n",
       "   0.12348743125796319,\n",
       "   0.12314316399395465,\n",
       "   0.12329952053725719,\n",
       "   0.12250892288982868]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'conv_stack': 20,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.586098656654358,\n",
       "   1.0996738690137864,\n",
       "   0.8906940650939942,\n",
       "   0.8071090811491013,\n",
       "   0.7345527827739715,\n",
       "   0.6298156416416169,\n",
       "   0.5959538730978966,\n",
       "   0.595283436179161,\n",
       "   0.5871778237819671,\n",
       "   0.5853691497445106],\n",
       "  'running_classification_total_loss': [1.262459748983383,\n",
       "   0.888410649895668,\n",
       "   0.6805529561638832,\n",
       "   0.5978173762559891,\n",
       "   0.5294030964374542,\n",
       "   0.48436938762664794,\n",
       "   0.4647052243351936,\n",
       "   0.4642283898591995,\n",
       "   0.4585961550474167,\n",
       "   0.4579297798871994],\n",
       "  'running_regression_total_loss': [0.32363890543580054,\n",
       "   0.2112632203102112,\n",
       "   0.2101411135494709,\n",
       "   0.20929170355200769,\n",
       "   0.20514968410134315,\n",
       "   0.14544625356793403,\n",
       "   0.1312486470490694,\n",
       "   0.13105504646897315,\n",
       "   0.12858166798949242,\n",
       "   0.1274393729865551]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'conv_stack': 20,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [0.9082319062948226,\n",
       "   0.7153708952665329,\n",
       "   0.721726998090744,\n",
       "   0.705964862704277,\n",
       "   0.7073640358448029,\n",
       "   0.7093438643217087,\n",
       "   0.7069018787145614,\n",
       "   0.7020867365598679,\n",
       "   0.699619779586792,\n",
       "   0.7082863914966583],\n",
       "  'running_classification_total_loss': [0.6571961271762848,\n",
       "   0.4682300791144371,\n",
       "   0.4735480144619942,\n",
       "   0.45911878973245623,\n",
       "   0.46057299643754956,\n",
       "   0.46210362076759337,\n",
       "   0.459405597448349,\n",
       "   0.4551948782801628,\n",
       "   0.45305496543645857,\n",
       "   0.46043877840042113],\n",
       "  'running_regression_total_loss': [0.2510357829928398,\n",
       "   0.2471408185362816,\n",
       "   0.24817898258566856,\n",
       "   0.24684607326984406,\n",
       "   0.24679103791713713,\n",
       "   0.24724024131894112,\n",
       "   0.24749627828598023,\n",
       "   0.2468918578326702,\n",
       "   0.2465648128092289,\n",
       "   0.24784761413931847]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'conv_stack': 20,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.6229872548580169,\n",
       "   1.3183708333969115,\n",
       "   0.9424404215812683,\n",
       "   0.783634917140007,\n",
       "   0.7279601299762726,\n",
       "   0.6764096942543983,\n",
       "   0.6476167023181916,\n",
       "   0.6083943676948548,\n",
       "   0.5978675645589828,\n",
       "   0.5938984319567681],\n",
       "  'running_classification_total_loss': [1.3613798928260803,\n",
       "   1.0727241843938828,\n",
       "   0.6965312454104423,\n",
       "   0.5382011270523072,\n",
       "   0.4877919715642929,\n",
       "   0.4710507470369339,\n",
       "   0.46447465270757676,\n",
       "   0.4627641710639,\n",
       "   0.4614650133252144,\n",
       "   0.46257350474596026],\n",
       "  'running_regression_total_loss': [0.26160735994577405,\n",
       "   0.24564664840698242,\n",
       "   0.2459091767668724,\n",
       "   0.24543378844857217,\n",
       "   0.24016815692186355,\n",
       "   0.20535894557833673,\n",
       "   0.1831420509517193,\n",
       "   0.14563019506633282,\n",
       "   0.13640255272388457,\n",
       "   0.13132492683827876]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'conv_stack': 20,\n",
       "  'dropout': 0.15,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [1.1653467762470244,\n",
       "   0.8014944964647293,\n",
       "   0.6566069388389587,\n",
       "   0.6479411143064499,\n",
       "   0.6511220291256905,\n",
       "   0.6358896717429161,\n",
       "   0.6322017624974251,\n",
       "   0.632246855199337,\n",
       "   0.6283640766143799,\n",
       "   0.6150068867206574],\n",
       "  'running_classification_total_loss': [0.9445510441064835,\n",
       "   0.6732079190015793,\n",
       "   0.5321061483025551,\n",
       "   0.5238897743821144,\n",
       "   0.5263148352503777,\n",
       "   0.5123630931973457,\n",
       "   0.5081328365206719,\n",
       "   0.5088157606124878,\n",
       "   0.5047023981809616,\n",
       "   0.49143083542585375],\n",
       "  'running_regression_total_loss': [0.220795731022954,\n",
       "   0.12828657522797585,\n",
       "   0.1245007897913456,\n",
       "   0.12405134305357933,\n",
       "   0.12480719216167926,\n",
       "   0.12352657876908779,\n",
       "   0.12406892791390418,\n",
       "   0.12343109644949436,\n",
       "   0.12366167262196541,\n",
       "   0.12357605241239071]},\n",
       " {'bi_mamba_stacks': 1,\n",
       "  'conv_stack': 20,\n",
       "  'dropout': 0.15,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.2816430199146271,\n",
       "   0.9035272294282913,\n",
       "   0.8234577095508575,\n",
       "   0.7535220623016358,\n",
       "   0.6589014101028442,\n",
       "   0.621269326210022,\n",
       "   0.6135707244277,\n",
       "   0.5999443769454956,\n",
       "   0.6008809667825699,\n",
       "   0.601784919500351],\n",
       "  'running_classification_total_loss': [1.0714772301912308,\n",
       "   0.7690755021572113,\n",
       "   0.6942290711402893,\n",
       "   0.6228457942605019,\n",
       "   0.5303447660803795,\n",
       "   0.4943365168571472,\n",
       "   0.48647275894880293,\n",
       "   0.47362771928310393,\n",
       "   0.47558654099702835,\n",
       "   0.47654630333185194],\n",
       "  'running_regression_total_loss': [0.21016578927636145,\n",
       "   0.13445172823965548,\n",
       "   0.12922863982617855,\n",
       "   0.13067627184092998,\n",
       "   0.12855664364993571,\n",
       "   0.1269328074157238,\n",
       "   0.12709796711802482,\n",
       "   0.12631665602326392,\n",
       "   0.1252944243699312,\n",
       "   0.12523861557245256]},\n",
       " {'bi_mamba_stacks': 10,\n",
       "  'conv_stack': 1,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [0.9323793321847915,\n",
       "   0.7267087715864181,\n",
       "   0.7337186580896378,\n",
       "   0.9835144156217575,\n",
       "   1.0582690119743348,\n",
       "   1.0480270552635194,\n",
       "   1.0542321199178695,\n",
       "   1.0549770867824555,\n",
       "   1.051388903260231,\n",
       "   1.0485525000095368],\n",
       "  'running_classification_total_loss': [0.7205614796280861,\n",
       "   0.5153986653685569,\n",
       "   0.5218238553404808,\n",
       "   0.7719123417139053,\n",
       "   0.847099461555481,\n",
       "   0.8367790853977204,\n",
       "   0.8426420497894287,\n",
       "   0.8434780544042587,\n",
       "   0.8397948795557022,\n",
       "   0.8373515903949738],\n",
       "  'running_regression_total_loss': [0.21181785315275192,\n",
       "   0.21131010457873345,\n",
       "   0.21189480066299438,\n",
       "   0.21160207331180572,\n",
       "   0.21116954520344733,\n",
       "   0.2112479680776596,\n",
       "   0.21159006983041764,\n",
       "   0.21149903789162636,\n",
       "   0.2115940272808075,\n",
       "   0.21120091140270233]},\n",
       " {'bi_mamba_stacks': 10,\n",
       "  'conv_stack': 1,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.2771202254295348,\n",
       "   0.8969823080301285,\n",
       "   0.7657287406921387,\n",
       "   0.6523623469471932,\n",
       "   0.6146401816606522,\n",
       "   0.593260723054409,\n",
       "   0.5882399618625641,\n",
       "   0.5917771419882775,\n",
       "   0.5774844637513161,\n",
       "   0.5786672928929328],\n",
       "  'running_classification_total_loss': [1.1266077929735183,\n",
       "   0.7511089807748794,\n",
       "   0.6222538316249847,\n",
       "   0.524937618970871,\n",
       "   0.4898462000489235,\n",
       "   0.46969895958900454,\n",
       "   0.46705196768045426,\n",
       "   0.472734816968441,\n",
       "   0.4598476877808571,\n",
       "   0.4617453673481941],\n",
       "  'running_regression_total_loss': [0.15051242887973784,\n",
       "   0.145873327255249,\n",
       "   0.14347491078078747,\n",
       "   0.12742472596466542,\n",
       "   0.12479398243129253,\n",
       "   0.1235617619752884,\n",
       "   0.12118799477815628,\n",
       "   0.11904232658445835,\n",
       "   0.1176367750018835,\n",
       "   0.11692192360758781]},\n",
       " {'bi_mamba_stacks': 10,\n",
       "  'conv_stack': 1,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [0.8926308888196945,\n",
       "   0.8947932833433151,\n",
       "   0.9611687606573105,\n",
       "   0.8705038464069367,\n",
       "   0.7250234279036522,\n",
       "   0.7271724098920822,\n",
       "   0.708972572684288,\n",
       "   0.7000005802512169,\n",
       "   0.7056906497478486,\n",
       "   0.700052689909935],\n",
       "  'running_classification_total_loss': [0.7008639323711395,\n",
       "   0.7096093732118607,\n",
       "   0.7771182465553284,\n",
       "   0.6871291339397431,\n",
       "   0.541915633380413,\n",
       "   0.5442887726426124,\n",
       "   0.525020288825035,\n",
       "   0.5157884612679482,\n",
       "   0.5223305314779282,\n",
       "   0.5168477791547775],\n",
       "  'running_regression_total_loss': [0.19176695942878724,\n",
       "   0.1851839079707861,\n",
       "   0.1840505113452673,\n",
       "   0.18337471179664136,\n",
       "   0.1831077968329191,\n",
       "   0.182883635237813,\n",
       "   0.18395228035748004,\n",
       "   0.1842121222615242,\n",
       "   0.1833601190149784,\n",
       "   0.1832049074023962]},\n",
       " {'bi_mamba_stacks': 10,\n",
       "  'conv_stack': 1,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.5206148397922516,\n",
       "   1.199701074361801,\n",
       "   0.9792072540521621,\n",
       "   0.8850430381298066,\n",
       "   0.7795249634981155,\n",
       "   0.7222936815023422,\n",
       "   0.7113557082414627,\n",
       "   0.7188274145126343,\n",
       "   0.7113040053844452,\n",
       "   0.6972828835248948],\n",
       "  'running_classification_total_loss': [1.303195823431015,\n",
       "   0.9877598661184311,\n",
       "   0.7675331354141235,\n",
       "   0.6735975790023804,\n",
       "   0.5682314437627792,\n",
       "   0.5111143973469734,\n",
       "   0.4997221624851227,\n",
       "   0.5074541348218918,\n",
       "   0.4998528069257736,\n",
       "   0.4862755364179611],\n",
       "  'running_regression_total_loss': [0.21741901218891144,\n",
       "   0.21194120228290558,\n",
       "   0.21167411655187607,\n",
       "   0.21144546195864677,\n",
       "   0.2112935195863247,\n",
       "   0.2111792831122875,\n",
       "   0.21163354575634002,\n",
       "   0.2113732822239399,\n",
       "   0.2114511986076832,\n",
       "   0.21100734457373618]},\n",
       " {'bi_mamba_stacks': 10,\n",
       "  'conv_stack': 1,\n",
       "  'dropout': 0.15,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [0.8578965067863464,\n",
       "   0.8309221482276916,\n",
       "   0.7920161563158036,\n",
       "   0.7301685297489167,\n",
       "   0.838251605629921,\n",
       "   0.818573489189148,\n",
       "   0.9163534778356552,\n",
       "   0.9636868172883988,\n",
       "   0.9724343973398208,\n",
       "   0.9285601991415023],\n",
       "  'running_classification_total_loss': [0.7109440076351166,\n",
       "   0.6850102230906486,\n",
       "   0.6464646750688553,\n",
       "   0.5845729759335518,\n",
       "   0.6923982480168343,\n",
       "   0.6724695193767548,\n",
       "   0.7706583487987518,\n",
       "   0.8178427714109421,\n",
       "   0.8264174371957779,\n",
       "   0.7827253186702728],\n",
       "  'running_regression_total_loss': [0.14695250131189824,\n",
       "   0.14591192215681076,\n",
       "   0.14555147968232632,\n",
       "   0.14559555269777774,\n",
       "   0.14585336357355116,\n",
       "   0.14610396966338157,\n",
       "   0.14569513104856013,\n",
       "   0.14584404267370701,\n",
       "   0.1460169618576765,\n",
       "   0.14583488330245017]},\n",
       " {'bi_mamba_stacks': 10,\n",
       "  'conv_stack': 1,\n",
       "  'dropout': 0.15,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.5923016548156739,\n",
       "   1.3569942319393158,\n",
       "   0.9571787232160568,\n",
       "   0.8298695594072342,\n",
       "   0.711668421626091,\n",
       "   0.6722530806064606,\n",
       "   0.6573649987578392,\n",
       "   0.6509184721112251,\n",
       "   0.6480943471193313,\n",
       "   0.6720929116010665],\n",
       "  'running_classification_total_loss': [1.4408344721794129,\n",
       "   1.2114975190162658,\n",
       "   0.8123145234584809,\n",
       "   0.6894648340344429,\n",
       "   0.5834278294444084,\n",
       "   0.5465952613949776,\n",
       "   0.5335287621617317,\n",
       "   0.5274447238445282,\n",
       "   0.5260671916604042,\n",
       "   0.5519213277101517],\n",
       "  'running_regression_total_loss': [0.15146717853844166,\n",
       "   0.14549671292304991,\n",
       "   0.14486419714987278,\n",
       "   0.14040472455322742,\n",
       "   0.12824059039354324,\n",
       "   0.12565781891345978,\n",
       "   0.1238362368941307,\n",
       "   0.12347374744713306,\n",
       "   0.12202715367078781,\n",
       "   0.12017158538103104]},\n",
       " {'bi_mamba_stacks': 10,\n",
       "  'conv_stack': 10,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [0.9756603801250457,\n",
       "   0.6424057918787003,\n",
       "   0.9129606902599334,\n",
       "   0.9920066916942596,\n",
       "   0.9895841634273529,\n",
       "   0.9822296559810638,\n",
       "   0.9553217804431915,\n",
       "   0.9588462257385254,\n",
       "   0.9521119797229767,\n",
       "   0.951204577088356],\n",
       "  'running_classification_total_loss': [0.8077189221978187,\n",
       "   0.49610794335603714,\n",
       "   0.7673610657453537,\n",
       "   0.8467507863044739,\n",
       "   0.843251246213913,\n",
       "   0.8449050891399383,\n",
       "   0.837437584400177,\n",
       "   0.8441704928874969,\n",
       "   0.8377191019058228,\n",
       "   0.8369463980197906],\n",
       "  'running_regression_total_loss': [0.16794145986437797,\n",
       "   0.146297847032547,\n",
       "   0.14559962041676044,\n",
       "   0.14525590218603612,\n",
       "   0.14633291743695737,\n",
       "   0.1373245654255152,\n",
       "   0.1178841932862997,\n",
       "   0.11467573754489421,\n",
       "   0.11439287766814232,\n",
       "   0.11425817884504795]},\n",
       " {'bi_mamba_stacks': 10,\n",
       "  'conv_stack': 10,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.4430440366268158,\n",
       "   0.9572447234392166,\n",
       "   0.7880234837532043,\n",
       "   0.6828154009580613,\n",
       "   0.6314439606666565,\n",
       "   0.6177993324398995,\n",
       "   0.6043443486094475,\n",
       "   0.5934907194972038,\n",
       "   0.5970693433284759,\n",
       "   0.5863456478714943],\n",
       "  'running_classification_total_loss': [1.2839292722940445,\n",
       "   0.8116493743658065,\n",
       "   0.642803470492363,\n",
       "   0.5487153643369674,\n",
       "   0.5075013566017151,\n",
       "   0.4954712963104248,\n",
       "   0.4872548767924309,\n",
       "   0.4785370257496834,\n",
       "   0.4832741117477417,\n",
       "   0.4723389649391174],\n",
       "  'running_regression_total_loss': [0.15911476954817771,\n",
       "   0.14559534832835197,\n",
       "   0.14522001080214977,\n",
       "   0.13410003520548344,\n",
       "   0.12394260399043561,\n",
       "   0.1223280356824398,\n",
       "   0.11708947144448757,\n",
       "   0.11495369344949723,\n",
       "   0.1137952321767807,\n",
       "   0.11400668032467365]},\n",
       " {'bi_mamba_stacks': 10,\n",
       "  'conv_stack': 10,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [1.0381061780452727,\n",
       "   0.9842490601539612,\n",
       "   0.9961951768398285,\n",
       "   0.9883311522006989,\n",
       "   0.9786602991819382,\n",
       "   0.9863089734315872,\n",
       "   0.9866072028875351,\n",
       "   0.9865879589319229,\n",
       "   0.9872689741849899,\n",
       "   0.9854406905174256],\n",
       "  'running_classification_total_loss': [0.8902557194232941,\n",
       "   0.8386424267292023,\n",
       "   0.8507936817407608,\n",
       "   0.8432427364587783,\n",
       "   0.8333231985569001,\n",
       "   0.8405338352918625,\n",
       "   0.8409860473871231,\n",
       "   0.8410759514570236,\n",
       "   0.8415975254774094,\n",
       "   0.8392059797048569],\n",
       "  'running_regression_total_loss': [0.1478504591435194,\n",
       "   0.14560662753880024,\n",
       "   0.14540149457752705,\n",
       "   0.14508841581642629,\n",
       "   0.14533709794282912,\n",
       "   0.1457751353085041,\n",
       "   0.14562115609645843,\n",
       "   0.14551200680434703,\n",
       "   0.14567144587635994,\n",
       "   0.1462347075343132]},\n",
       " {'bi_mamba_stacks': 10,\n",
       "  'conv_stack': 10,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.3803575074672698,\n",
       "   1.112957152724266,\n",
       "   0.9630758887529374,\n",
       "   0.808372477889061,\n",
       "   0.8658242124319077,\n",
       "   0.939849151968956,\n",
       "   0.7623074805736542,\n",
       "   0.7442630350589752,\n",
       "   0.7352619844675065,\n",
       "   0.7336399179697036],\n",
       "  'running_classification_total_loss': [1.1332982218265533,\n",
       "   0.8660244798660278,\n",
       "   0.7162820768356323,\n",
       "   0.5608361375331878,\n",
       "   0.6181825605034829,\n",
       "   0.6924841713905334,\n",
       "   0.5150428959727287,\n",
       "   0.49741684198379515,\n",
       "   0.48817960292100904,\n",
       "   0.48603854566812515],\n",
       "  'running_regression_total_loss': [0.24705928653478623,\n",
       "   0.24693266734480857,\n",
       "   0.24679380923509597,\n",
       "   0.24753634080290796,\n",
       "   0.24764165565371513,\n",
       "   0.24736498028039933,\n",
       "   0.24726458355784417,\n",
       "   0.24684619426727294,\n",
       "   0.24708238184452058,\n",
       "   0.24760137632489204]},\n",
       " {'bi_mamba_stacks': 10,\n",
       "  'conv_stack': 10,\n",
       "  'dropout': 0.15,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [1.119540857076645,\n",
       "   0.9596700149774552,\n",
       "   0.9586792427301407,\n",
       "   0.9603539901971817,\n",
       "   0.9726397931575775,\n",
       "   0.9584548580646515,\n",
       "   0.9581051009893418,\n",
       "   0.957312502861023,\n",
       "   0.9486623597145081,\n",
       "   0.9573576211929321],\n",
       "  'running_classification_total_loss': [0.9704404735565185,\n",
       "   0.8389132690429687,\n",
       "   0.8412523877620697,\n",
       "   0.8430049055814743,\n",
       "   0.8437875771522522,\n",
       "   0.841222807765007,\n",
       "   0.8420265930891037,\n",
       "   0.8416491419076919,\n",
       "   0.8330275022983551,\n",
       "   0.8428863072395325],\n",
       "  'running_regression_total_loss': [0.14910039089620114,\n",
       "   0.12075674973428249,\n",
       "   0.11742685385048389,\n",
       "   0.11734908290207385,\n",
       "   0.12885221540927888,\n",
       "   0.11723204500973225,\n",
       "   0.11607850693166256,\n",
       "   0.11566336020827293,\n",
       "   0.11563485518097877,\n",
       "   0.11447131037712097]},\n",
       " {'bi_mamba_stacks': 10,\n",
       "  'conv_stack': 10,\n",
       "  'dropout': 0.15,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.4759331297874452,\n",
       "   1.2013435196876525,\n",
       "   1.056375760436058,\n",
       "   0.9996269500255585,\n",
       "   0.9855147004127502,\n",
       "   0.9657934820652008,\n",
       "   0.9602481406927109,\n",
       "   0.9596785473823547,\n",
       "   0.9544645649194717,\n",
       "   0.9610056465864182],\n",
       "  'running_classification_total_loss': [1.3011363542079926,\n",
       "   1.056349462866783,\n",
       "   0.9230136066675186,\n",
       "   0.8749729442596436,\n",
       "   0.8617739009857178,\n",
       "   0.8445618492364884,\n",
       "   0.8409153658151627,\n",
       "   0.8430589407682418,\n",
       "   0.8394446510076523,\n",
       "   0.8455713671445847],\n",
       "  'running_regression_total_loss': [0.17479677453637124,\n",
       "   0.1449940549582243,\n",
       "   0.13336215376853944,\n",
       "   0.12465400226414204,\n",
       "   0.12374079756438733,\n",
       "   0.12123163558542728,\n",
       "   0.11933277636766433,\n",
       "   0.11661961041390896,\n",
       "   0.1150199170410633,\n",
       "   0.11543428346514702]},\n",
       " {'bi_mamba_stacks': 10,\n",
       "  'conv_stack': 20,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [0.8717885053157807,\n",
       "   0.6603063952922821,\n",
       "   0.9988446420431137,\n",
       "   0.9975773334503174,\n",
       "   0.9860195994377137,\n",
       "   0.9901386719942092,\n",
       "   0.9850807666778565,\n",
       "   0.9839653193950653,\n",
       "   0.983560642004013,\n",
       "   0.9881276947259903],\n",
       "  'running_classification_total_loss': [0.7225322592258453,\n",
       "   0.5140195989608765,\n",
       "   0.8539411681890487,\n",
       "   0.8516089218854904,\n",
       "   0.8405537444353104,\n",
       "   0.8441602468490601,\n",
       "   0.8396811419725418,\n",
       "   0.8384338414669037,\n",
       "   0.838023082613945,\n",
       "   0.8429329526424408],\n",
       "  'running_regression_total_loss': [0.14925624258816242,\n",
       "   0.14628679260611535,\n",
       "   0.14490347638726234,\n",
       "   0.14596841536462307,\n",
       "   0.14546585254371167,\n",
       "   0.14597842380404472,\n",
       "   0.1453996228426695,\n",
       "   0.14553147971630095,\n",
       "   0.14553755782544614,\n",
       "   0.1451947397738695]},\n",
       " {'bi_mamba_stacks': 10,\n",
       "  'conv_stack': 20,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.4179790711402893,\n",
       "   0.9805417919158935,\n",
       "   0.8217757153511047,\n",
       "   0.7773346257209778,\n",
       "   0.8519246810674668,\n",
       "   0.7372617357969284,\n",
       "   0.7315264993906021,\n",
       "   0.7271556878089904,\n",
       "   0.7190791290998458,\n",
       "   0.7121551787853241],\n",
       "  'running_classification_total_loss': [1.1712065196037293,\n",
       "   0.7335768291354179,\n",
       "   0.574961765408516,\n",
       "   0.5300848984718323,\n",
       "   0.6040829691290855,\n",
       "   0.4903528022766113,\n",
       "   0.4846484661102295,\n",
       "   0.4790943941473961,\n",
       "   0.4719594392180443,\n",
       "   0.4644859978556633],\n",
       "  'running_regression_total_loss': [0.24677254781126975,\n",
       "   0.2469649675488472,\n",
       "   0.24681395262479783,\n",
       "   0.24724972650408744,\n",
       "   0.24784171044826508,\n",
       "   0.24690893590450286,\n",
       "   0.2468780370056629,\n",
       "   0.24806129664182663,\n",
       "   0.24711969166994094,\n",
       "   0.2476691783964634]},\n",
       " {'bi_mamba_stacks': 10,\n",
       "  'conv_stack': 20,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [0.9249559438228607,\n",
       "   0.9374923223257065,\n",
       "   0.9813527911901474,\n",
       "   0.9739091593027115,\n",
       "   0.9645924544334412,\n",
       "   0.9673208451271057,\n",
       "   0.9660072165727616,\n",
       "   0.9685874956846238,\n",
       "   0.961594745516777,\n",
       "   nan],\n",
       "  'running_classification_total_loss': [0.7866702961921692,\n",
       "   0.8142040020227432,\n",
       "   0.8402553904056549,\n",
       "   0.8383568829298019,\n",
       "   0.8403954607248306,\n",
       "   0.8434714102745056,\n",
       "   0.8418100774288177,\n",
       "   0.844549252986908,\n",
       "   0.8379590541124344,\n",
       "   0.8396977800130844],\n",
       "  'running_regression_total_loss': [0.13828564621508121,\n",
       "   0.12328832007944585,\n",
       "   0.14109740547835828,\n",
       "   0.13555227980017662,\n",
       "   0.12419698782265186,\n",
       "   0.12384943544864654,\n",
       "   0.12419713966548443,\n",
       "   0.12403824470937252,\n",
       "   0.12363569132983684,\n",
       "   nan]},\n",
       " {'bi_mamba_stacks': 10,\n",
       "  'conv_stack': 20,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.5366678416728974,\n",
       "   1.0190022879838943,\n",
       "   0.8657108920812607,\n",
       "   0.7826792430877686,\n",
       "   0.7338709330558777,\n",
       "   0.6781255215406418,\n",
       "   0.6364979210495949,\n",
       "   0.6131249153614045,\n",
       "   0.7243914943933487,\n",
       "   0.7165388357639313],\n",
       "  'running_classification_total_loss': [1.3248315560817718,\n",
       "   0.8102613002061844,\n",
       "   0.705302004814148,\n",
       "   0.6588163349032402,\n",
       "   0.6120222297310829,\n",
       "   0.5607645183801651,\n",
       "   0.5196839141845703,\n",
       "   0.4970225006341934,\n",
       "   0.609166177213192,\n",
       "   0.6029667845368385],\n",
       "  'running_regression_total_loss': [0.2118362833559513,\n",
       "   0.20874099180102348,\n",
       "   0.16040888585150243,\n",
       "   0.12386291079223156,\n",
       "   0.12184870444238186,\n",
       "   0.11736100479960442,\n",
       "   0.11681400582194329,\n",
       "   0.11610241569578647,\n",
       "   0.11522531650960445,\n",
       "   0.11357205294072628]},\n",
       " {'bi_mamba_stacks': 10,\n",
       "  'conv_stack': 20,\n",
       "  'dropout': 0.15,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [1.134519498348236,\n",
       "   1.0589995789527893,\n",
       "   1.0503527045249939,\n",
       "   0.997590172290802,\n",
       "   0.982565404176712,\n",
       "   0.970480243563652,\n",
       "   0.9633224189281464,\n",
       "   0.9675895780324936,\n",
       "   0.9559991681575775,\n",
       "   0.9623686057329178],\n",
       "  'running_classification_total_loss': [0.9220087850093841,\n",
       "   0.8475609689950943,\n",
       "   0.8385481685400009,\n",
       "   0.8421802186965942,\n",
       "   0.846011797785759,\n",
       "   0.835353022813797,\n",
       "   0.8379572886228561,\n",
       "   0.8469163304567338,\n",
       "   0.8377421736717224,\n",
       "   0.8448757410049439],\n",
       "  'running_regression_total_loss': [0.21251071438193322,\n",
       "   0.21143861040472983,\n",
       "   0.21180453419685363,\n",
       "   0.15540995121002196,\n",
       "   0.1365536067634821,\n",
       "   0.13512722313404082,\n",
       "   0.12536512978374958,\n",
       "   0.12067324973642826,\n",
       "   0.11825699396431447,\n",
       "   0.11749286465346813]},\n",
       " {'bi_mamba_stacks': 10,\n",
       "  'conv_stack': 20,\n",
       "  'dropout': 0.15,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.3151157808303833,\n",
       "   1.0731900894641877,\n",
       "   0.982682256102562,\n",
       "   0.8873856544494629,\n",
       "   0.7618707376718521,\n",
       "   0.7003673988580704,\n",
       "   0.6810356098413467,\n",
       "   0.6775451648235321,\n",
       "   0.7199889934062957,\n",
       "   0.6997102612257003],\n",
       "  'running_classification_total_loss': [1.134672570824623,\n",
       "   0.9264626562595367,\n",
       "   0.8367340332269668,\n",
       "   0.7419173705577851,\n",
       "   0.6161642923951149,\n",
       "   0.5548418891429902,\n",
       "   0.5359629002213478,\n",
       "   0.5317705574631691,\n",
       "   0.5751997777819633,\n",
       "   0.5545452839136124],\n",
       "  'running_regression_total_loss': [0.18044321097433566,\n",
       "   0.14672743573784827,\n",
       "   0.1459482251852751,\n",
       "   0.1454682855308056,\n",
       "   0.14570644594728946,\n",
       "   0.14552551276981832,\n",
       "   0.14507270902395247,\n",
       "   0.14577460750937463,\n",
       "   0.1447892164438963,\n",
       "   0.1451649745553732]},\n",
       " {'bi_mamba_stacks': 20,\n",
       "  'conv_stack': 1,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [0.9404996120929718,\n",
       "   0.862705505490303,\n",
       "   0.9986916559934617,\n",
       "   1.0344664889574051,\n",
       "   1.0351419645547866,\n",
       "   1.0284322732686997,\n",
       "   1.036183738708496,\n",
       "   1.0216429936885834,\n",
       "   1.041670697927475,\n",
       "   1.0352897650003434],\n",
       "  'running_classification_total_loss': [0.7452893605828286,\n",
       "   0.6695455139875413,\n",
       "   0.8063271516561508,\n",
       "   0.8415137815475464,\n",
       "   0.8410175704956054,\n",
       "   0.8346520847082138,\n",
       "   0.8439104002714157,\n",
       "   0.8279014641046524,\n",
       "   0.8477791404724121,\n",
       "   0.8418043088912964],\n",
       "  'running_regression_total_loss': [0.1952102506905794,\n",
       "   0.19315999165177344,\n",
       "   0.1923645044863224,\n",
       "   0.19295270584523677,\n",
       "   0.19412439979612828,\n",
       "   0.19378018543124198,\n",
       "   0.19227333940565586,\n",
       "   0.1937415251880884,\n",
       "   0.19389156304299832,\n",
       "   0.19348545268177986]},\n",
       " {'bi_mamba_stacks': 20,\n",
       "  'conv_stack': 1,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.5018164134025573,\n",
       "   1.076529514193535,\n",
       "   0.8767411804199219,\n",
       "   0.8187411743402481,\n",
       "   0.7656250363588333,\n",
       "   0.7206983858346939,\n",
       "   0.6886845636367798,\n",
       "   0.6789194482564926,\n",
       "   0.6757266372442245,\n",
       "   0.6766342210769654],\n",
       "  'running_classification_total_loss': [1.306732646226883,\n",
       "   0.8836334455013275,\n",
       "   0.6831200671195984,\n",
       "   0.6244737949967384,\n",
       "   0.5720726844668388,\n",
       "   0.526482697725296,\n",
       "   0.4946553865075111,\n",
       "   0.4873892721533775,\n",
       "   0.481964695751667,\n",
       "   0.48313756436109545],\n",
       "  'running_regression_total_loss': [0.19508376732468605,\n",
       "   0.1928960730880499,\n",
       "   0.19362111121416092,\n",
       "   0.19426737844944,\n",
       "   0.193552353233099,\n",
       "   0.19421568915247917,\n",
       "   0.19402917623519897,\n",
       "   0.1915301737934351,\n",
       "   0.1937619385123253,\n",
       "   0.19349665768444538]},\n",
       " {'bi_mamba_stacks': 20,\n",
       "  'conv_stack': 1,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [1.145899659395218,\n",
       "   1.0320330691337585,\n",
       "   1.0326154232025146,\n",
       "   1.0345639371871949,\n",
       "   1.032320567369461,\n",
       "   1.0408334052562713,\n",
       "   1.033131983280182,\n",
       "   1.0347916448116303,\n",
       "   1.034093337059021,\n",
       "   1.0310491675138473],\n",
       "  'running_classification_total_loss': [0.9521206575632095,\n",
       "   0.8379793411493301,\n",
       "   0.8387993794679641,\n",
       "   0.8416815871000289,\n",
       "   0.8393638068437577,\n",
       "   0.8470118373632431,\n",
       "   0.8392023879289627,\n",
       "   0.8422910165786743,\n",
       "   0.8416719591617584,\n",
       "   0.8373845344781876],\n",
       "  'running_regression_total_loss': [0.19377900034189224,\n",
       "   0.1940537280589342,\n",
       "   0.1938160388916731,\n",
       "   0.19288234904408455,\n",
       "   0.19295676462352276,\n",
       "   0.19382156804203987,\n",
       "   0.19392959713935853,\n",
       "   0.19250062733888626,\n",
       "   0.1924213794618845,\n",
       "   0.1936646320670843]},\n",
       " {'bi_mamba_stacks': 20,\n",
       "  'conv_stack': 1,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.3052947771549226,\n",
       "   0.9887134397029876,\n",
       "   0.8595075768232345,\n",
       "   0.7723747563362121,\n",
       "   0.7093439981341362,\n",
       "   0.6842951953411103,\n",
       "   0.6706860953569412,\n",
       "   0.6609576544165612,\n",
       "   0.6508509939908982,\n",
       "   0.6510388103127479],\n",
       "  'running_classification_total_loss': [1.152439444065094,\n",
       "   0.8426753401756286,\n",
       "   0.7136673414707184,\n",
       "   0.6269159552454948,\n",
       "   0.5637196034193039,\n",
       "   0.5392692515254021,\n",
       "   0.5255198517441749,\n",
       "   0.515971205830574,\n",
       "   0.5056042698025703,\n",
       "   0.5059998714923859],\n",
       "  'running_regression_total_loss': [0.15285533428192138,\n",
       "   0.14603809893131256,\n",
       "   0.14584023505449295,\n",
       "   0.14545880183577536,\n",
       "   0.14562439367175103,\n",
       "   0.14502594158053397,\n",
       "   0.145166242942214,\n",
       "   0.14498644672334193,\n",
       "   0.1452467231452465,\n",
       "   0.14503893680870533]},\n",
       " {'bi_mamba_stacks': 20,\n",
       "  'conv_stack': 1,\n",
       "  'dropout': 0.15,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [1.1849770939350128,\n",
       "   1.0500290876626968,\n",
       "   1.0584772366285324,\n",
       "   1.0542851197719574,\n",
       "   1.057835019826889,\n",
       "   1.050161094069481,\n",
       "   1.0466820633411407,\n",
       "   1.0531487780809403,\n",
       "   1.0577564173936844,\n",
       "   1.0557338094711304],\n",
       "  'running_classification_total_loss': [0.9725709581375122,\n",
       "   0.8386486440896987,\n",
       "   0.8464441984891892,\n",
       "   0.843065676689148,\n",
       "   0.8466228646039963,\n",
       "   0.8388256317377091,\n",
       "   0.8349833524227143,\n",
       "   0.8417760610580445,\n",
       "   0.8459472101926804,\n",
       "   0.843792205452919],\n",
       "  'running_regression_total_loss': [0.21240613296627997,\n",
       "   0.21138044491410254,\n",
       "   0.21203303426504136,\n",
       "   0.211219445168972,\n",
       "   0.21121215522289277,\n",
       "   0.2113354603946209,\n",
       "   0.2116987118124962,\n",
       "   0.21137271359562873,\n",
       "   0.21180920913815499,\n",
       "   0.2119416005909443]},\n",
       " {'bi_mamba_stacks': 20,\n",
       "  'conv_stack': 1,\n",
       "  'dropout': 0.15,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.4137680733203888,\n",
       "   1.2024868553876877,\n",
       "   1.1218519121408463,\n",
       "   1.1053261291980743,\n",
       "   1.099150431752205,\n",
       "   1.091104587316513,\n",
       "   1.0900206190347672,\n",
       "   1.0873277860879897,\n",
       "   1.0860801839828491,\n",
       "   1.0846223312616348],\n",
       "  'running_classification_total_loss': [1.1665665221214294,\n",
       "   0.9552584511041641,\n",
       "   0.8741464346647263,\n",
       "   0.8578837341070176,\n",
       "   0.8519314754009247,\n",
       "   0.8440266388654709,\n",
       "   0.8419495725631714,\n",
       "   0.8391536128520966,\n",
       "   0.8388230508565903,\n",
       "   0.836887794137001],\n",
       "  'running_regression_total_loss': [0.2472015517950058,\n",
       "   0.24722840026021004,\n",
       "   0.24770547986030578,\n",
       "   0.24744240060448647,\n",
       "   0.247218956053257,\n",
       "   0.24707794994115828,\n",
       "   0.24807105094194412,\n",
       "   0.2481741738319397,\n",
       "   0.24725714087486267,\n",
       "   0.24773453786969185]},\n",
       " {'bi_mamba_stacks': 20,\n",
       "  'conv_stack': 10,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [1.1948030871152877,\n",
       "   1.092694799900055,\n",
       "   1.0918145716190337,\n",
       "   1.084585304260254,\n",
       "   1.0862626510858535,\n",
       "   1.0853433811664581,\n",
       "   1.0803391122817994,\n",
       "   1.0884536880254745,\n",
       "   1.0871094298362731,\n",
       "   1.0887226110696793],\n",
       "  'running_classification_total_loss': [0.9476774442195892,\n",
       "   0.8459777951240539,\n",
       "   0.8442803186178207,\n",
       "   0.8375001138448716,\n",
       "   0.8389382171630859,\n",
       "   0.8377984839677811,\n",
       "   0.8325942409038544,\n",
       "   0.8408586883544922,\n",
       "   0.8403498315811158,\n",
       "   0.8410965573787689],\n",
       "  'running_regression_total_loss': [0.24712564542889595,\n",
       "   0.24671700447797776,\n",
       "   0.24753425046801567,\n",
       "   0.24708518341183663,\n",
       "   0.24732443153858186,\n",
       "   0.2475449001789093,\n",
       "   0.24774486899375917,\n",
       "   0.2475949953496456,\n",
       "   0.24675960332155228,\n",
       "   0.24762605682015418]},\n",
       " {'bi_mamba_stacks': 20,\n",
       "  'conv_stack': 10,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.329303423166275,\n",
       "   1.032725021839142,\n",
       "   0.8745330673456192,\n",
       "   0.7590851950645446,\n",
       "   0.6974077427387237,\n",
       "   0.6809630525112152,\n",
       "   0.6907622003555298,\n",
       "   0.6855299007892609,\n",
       "   0.6810089337825775,\n",
       "   0.6806721487641334],\n",
       "  'running_classification_total_loss': [1.1143815833330155,\n",
       "   0.8211286920309067,\n",
       "   0.6632303917407989,\n",
       "   0.5476189795136451,\n",
       "   0.4861048424243927,\n",
       "   0.46969272762537,\n",
       "   0.4794159936904907,\n",
       "   0.4742457190155983,\n",
       "   0.46953464448452,\n",
       "   0.46914459943771364],\n",
       "  'running_regression_total_loss': [0.21492183655500413,\n",
       "   0.21159633055329322,\n",
       "   0.2113026773929596,\n",
       "   0.2114662155508995,\n",
       "   0.21130289867520333,\n",
       "   0.21127032607793808,\n",
       "   0.21134620845317842,\n",
       "   0.21128418505191804,\n",
       "   0.2114742910861969,\n",
       "   0.2115275454521179]},\n",
       " {'bi_mamba_stacks': 20,\n",
       "  'conv_stack': 10,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [1.0962124651670455,\n",
       "   1.0366021716594696,\n",
       "   1.0361399632692336,\n",
       "   1.035734760761261,\n",
       "   1.033105475306511,\n",
       "   1.0373390907049178,\n",
       "   1.0376730316877365,\n",
       "   1.03453398168087,\n",
       "   1.028800094127655,\n",
       "   1.0317397552728653],\n",
       "  'running_classification_total_loss': [0.9024865311384201,\n",
       "   0.8427487224340439,\n",
       "   0.8434930336475372,\n",
       "   0.8427081298828125,\n",
       "   0.8406665766239166,\n",
       "   0.8452648270130158,\n",
       "   0.844614970088005,\n",
       "   0.8414459443092346,\n",
       "   0.8351284372806549,\n",
       "   0.838413484096527],\n",
       "  'running_regression_total_loss': [0.19372593246400358,\n",
       "   0.19385345257818698,\n",
       "   0.19264693319797516,\n",
       "   0.1930266323685646,\n",
       "   0.19243889644742013,\n",
       "   0.19207426689565182,\n",
       "   0.19305806465446948,\n",
       "   0.1930880369246006,\n",
       "   0.19367165774106979,\n",
       "   0.19332627058029175]},\n",
       " {'bi_mamba_stacks': 20,\n",
       "  'conv_stack': 10,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.5753557503223419,\n",
       "   1.1265627294778824,\n",
       "   0.9367734676599503,\n",
       "   0.8515332919359208,\n",
       "   0.8245580464601516,\n",
       "   0.7474570792913436,\n",
       "   0.6894622093439102,\n",
       "   0.6820386111736297,\n",
       "   0.7286374944448472,\n",
       "   0.7301765143871307],\n",
       "  'running_classification_total_loss': [1.4261353540420532,\n",
       "   0.9810506701469421,\n",
       "   0.7917497795820236,\n",
       "   0.7061429762840271,\n",
       "   0.6797189456224442,\n",
       "   0.6021885058283806,\n",
       "   0.5446885585784912,\n",
       "   0.536881867647171,\n",
       "   0.583200948536396,\n",
       "   0.5845271965861321],\n",
       "  'running_regression_total_loss': [0.14922039538621903,\n",
       "   0.14551205486059188,\n",
       "   0.1450236899405718,\n",
       "   0.145390312820673,\n",
       "   0.14483910158276558,\n",
       "   0.14526857681572436,\n",
       "   0.14477365016937255,\n",
       "   0.14515674471855164,\n",
       "   0.14543654568493367,\n",
       "   0.14564931705594064]},\n",
       " {'bi_mamba_stacks': 20,\n",
       "  'conv_stack': 10,\n",
       "  'dropout': 0.15,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [1.117282167673111,\n",
       "   0.9902616590261459,\n",
       "   0.9897738116979599,\n",
       "   0.9827597212791442,\n",
       "   0.9878338301181793,\n",
       "   0.9880103540420532,\n",
       "   0.989752984046936,\n",
       "   0.9834659814834594,\n",
       "   0.984102720618248,\n",
       "   0.9863623404502868],\n",
       "  'running_classification_total_loss': [0.9669871211051941,\n",
       "   0.8449755215644836,\n",
       "   0.8440798306465149,\n",
       "   0.8375158196687699,\n",
       "   0.8413742309808732,\n",
       "   0.8423541522026062,\n",
       "   0.8441693860292435,\n",
       "   0.8379477882385253,\n",
       "   0.8384844738245011,\n",
       "   0.840398651957512],\n",
       "  'running_regression_total_loss': [0.15029504500329494,\n",
       "   0.14528613671660423,\n",
       "   0.1456939823180437,\n",
       "   0.1452439034730196,\n",
       "   0.14645959615707396,\n",
       "   0.14565620377659796,\n",
       "   0.14558359764516354,\n",
       "   0.14551819428801538,\n",
       "   0.14561824299395085,\n",
       "   0.14596369236707687]},\n",
       " {'bi_mamba_stacks': 20,\n",
       "  'conv_stack': 10,\n",
       "  'dropout': 0.15,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.5693025052547456,\n",
       "   1.2542353999614715,\n",
       "   1.1059165620803832,\n",
       "   1.0927387392520904,\n",
       "   1.091091343164444,\n",
       "   1.0899293810129165,\n",
       "   1.0859773355722426,\n",
       "   1.0923806911706924,\n",
       "   1.0843924987316131,\n",
       "   1.0913763934373855],\n",
       "  'running_classification_total_loss': [1.3213683843612671,\n",
       "   1.0067535424232483,\n",
       "   0.8593491047620774,\n",
       "   0.8459528195858002,\n",
       "   0.8436136585474014,\n",
       "   0.8425983798503875,\n",
       "   0.8389588457345962,\n",
       "   0.8447072726488113,\n",
       "   0.8368742620944977,\n",
       "   0.8442480885982513],\n",
       "  'running_regression_total_loss': [0.24793412327766418,\n",
       "   0.24748185753822327,\n",
       "   0.24656745210289954,\n",
       "   0.24678592160344123,\n",
       "   0.24747768566012382,\n",
       "   0.24733099803328515,\n",
       "   0.2470184940099716,\n",
       "   0.2476734161376953,\n",
       "   0.24751823857426644,\n",
       "   0.2471283096075058]},\n",
       " {'bi_mamba_stacks': 20,\n",
       "  'conv_stack': 20,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [1.0159094458818436,\n",
       "   1.0558468794822693,\n",
       "   1.0587836062908174,\n",
       "   1.0626334357261658,\n",
       "   1.0533608639240264,\n",
       "   1.0615032655000687,\n",
       "   1.0526612997055054,\n",
       "   1.0537234318256379,\n",
       "   1.0550837594270706,\n",
       "   1.055426789522171],\n",
       "  'running_classification_total_loss': [0.8044645634293556,\n",
       "   0.8443256819248199,\n",
       "   0.8472343331575394,\n",
       "   0.8513694190979004,\n",
       "   0.8418393617868424,\n",
       "   0.8499768620729446,\n",
       "   0.8414660984277725,\n",
       "   0.8425320541858673,\n",
       "   0.8438385504484177,\n",
       "   0.8441969460248947],\n",
       "  'running_regression_total_loss': [0.21144488126039504,\n",
       "   0.2115211981534958,\n",
       "   0.21154927283525468,\n",
       "   0.21126401662826538,\n",
       "   0.21152150452136995,\n",
       "   0.21152640700340272,\n",
       "   0.21119520649313928,\n",
       "   0.2111913749575615,\n",
       "   0.21124521017074585,\n",
       "   0.2112298445403576]},\n",
       " {'bi_mamba_stacks': 20,\n",
       "  'conv_stack': 20,\n",
       "  'dropout': 0.05,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.44778146982193,\n",
       "   0.9958978724479676,\n",
       "   0.8082415056228638,\n",
       "   0.737997088432312,\n",
       "   0.7029402709007263,\n",
       "   0.6987083810567856,\n",
       "   0.6976882421970367,\n",
       "   0.6931058621406555,\n",
       "   0.6948318475484848,\n",
       "   0.703686471581459],\n",
       "  'running_classification_total_loss': [1.228070278763771,\n",
       "   0.784404246211052,\n",
       "   0.5962621322274209,\n",
       "   0.5260590034723281,\n",
       "   0.4910400494933128,\n",
       "   0.4869519791007042,\n",
       "   0.4861854964494705,\n",
       "   0.48181700229644775,\n",
       "   0.4837837561964989,\n",
       "   0.4919974794983864],\n",
       "  'running_regression_total_loss': [0.21971119061112404,\n",
       "   0.21149363175034522,\n",
       "   0.2119793762266636,\n",
       "   0.2119380886852741,\n",
       "   0.21190022125840188,\n",
       "   0.21175640136003493,\n",
       "   0.21150274604558944,\n",
       "   0.21128885850310325,\n",
       "   0.211048092097044,\n",
       "   0.2116889898478985]},\n",
       " {'bi_mamba_stacks': 20,\n",
       "  'conv_stack': 20,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [1.000157458782196,\n",
       "   0.8220573890209198,\n",
       "   0.9841152995824813,\n",
       "   0.9806580638885498,\n",
       "   0.9873678201436996,\n",
       "   0.9904227423667907,\n",
       "   0.9874654650688172,\n",
       "   0.9873022729158402,\n",
       "   0.9833610481023789,\n",
       "   0.9868504494428635],\n",
       "  'running_classification_total_loss': [0.8001109808683395,\n",
       "   0.6761419859528541,\n",
       "   0.8388323408365249,\n",
       "   0.835764354467392,\n",
       "   0.8422995483875275,\n",
       "   0.8441383391618729,\n",
       "   0.8418947863578796,\n",
       "   0.8418686747550964,\n",
       "   0.8377584254741669,\n",
       "   0.8414874452352524],\n",
       "  'running_regression_total_loss': [0.20004647925496102,\n",
       "   0.14591539934277534,\n",
       "   0.1452829649299383,\n",
       "   0.14489371314644814,\n",
       "   0.14506827466189862,\n",
       "   0.1462844057381153,\n",
       "   0.14557067699730397,\n",
       "   0.14543359488248825,\n",
       "   0.14560262769460677,\n",
       "   0.14536300651729106]},\n",
       " {'bi_mamba_stacks': 20,\n",
       "  'conv_stack': 20,\n",
       "  'dropout': 0.1,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.3963361835479737,\n",
       "   1.0830199027061462,\n",
       "   0.997122243642807,\n",
       "   0.9588568979501724,\n",
       "   0.8918320262432098,\n",
       "   0.900535050034523,\n",
       "   1.1819309085607528,\n",
       "   0.9869960284233094,\n",
       "   0.9851710677146912,\n",
       "   0.969430084824562],\n",
       "  'running_classification_total_loss': [1.2411594474315644,\n",
       "   0.9364253932237625,\n",
       "   0.8515851587057114,\n",
       "   0.8135039299726486,\n",
       "   0.7464588558673859,\n",
       "   0.7555941066145897,\n",
       "   1.0370731985569,\n",
       "   0.8416211038827897,\n",
       "   0.8397166848182678,\n",
       "   0.8244053995609284],\n",
       "  'running_regression_total_loss': [0.15517674162983894,\n",
       "   0.14659450761973858,\n",
       "   0.14553708106279373,\n",
       "   0.14535296969115735,\n",
       "   0.14537317484617232,\n",
       "   0.14494094379246236,\n",
       "   0.14485770918428897,\n",
       "   0.1453749304264784,\n",
       "   0.14545438408851624,\n",
       "   0.14502468332648277]},\n",
       " {'bi_mamba_stacks': 20,\n",
       "  'conv_stack': 20,\n",
       "  'dropout': 0.15,\n",
       "  'learning_rate': 0.01,\n",
       "  'total_loss': [1.109405859708786,\n",
       "   1.054419830441475,\n",
       "   1.0540623658895492,\n",
       "   1.0592207860946656,\n",
       "   1.0522231304645537,\n",
       "   1.0491365778446198,\n",
       "   1.0513383346796035,\n",
       "   1.0521313792467117,\n",
       "   1.053464908003807,\n",
       "   1.0512594908475876],\n",
       "  'running_classification_total_loss': [0.8972562837600708,\n",
       "   0.8431380659341812,\n",
       "   0.8423592740297318,\n",
       "   0.8476972037553787,\n",
       "   0.8408077424764633,\n",
       "   0.8377809721231461,\n",
       "   0.8399801671504974,\n",
       "   0.8407090801000595,\n",
       "   0.8423675465583801,\n",
       "   0.8397058773040772],\n",
       "  'running_regression_total_loss': [0.21214957430958747,\n",
       "   0.21128176495432854,\n",
       "   0.21170309022068978,\n",
       "   0.2115235835313797,\n",
       "   0.21141538739204407,\n",
       "   0.21135560661554337,\n",
       "   0.2113581682741642,\n",
       "   0.21142229720950126,\n",
       "   0.21109736412763597,\n",
       "   0.21155361518263816]},\n",
       " {'bi_mamba_stacks': 20,\n",
       "  'conv_stack': 20,\n",
       "  'dropout': 0.15,\n",
       "  'learning_rate': 0.001,\n",
       "  'total_loss': [1.4196114885807036,\n",
       "   1.123545156121254,\n",
       "   1.02715705037117,\n",
       "   0.999772133231163,\n",
       "   0.9946352964639664,\n",
       "   0.9847000807523727,\n",
       "   0.981767189502716,\n",
       "   0.9805856627225876,\n",
       "   0.9884498655796051,\n",
       "   0.9834131193161011],\n",
       "  'running_classification_total_loss': [1.254637223482132,\n",
       "   0.9778489482402801,\n",
       "   0.8815479022264481,\n",
       "   0.8547607082128524,\n",
       "   0.8497125160694122,\n",
       "   0.8399195313453675,\n",
       "   0.8365720158815384,\n",
       "   0.8350842267274856,\n",
       "   0.8427328336238861,\n",
       "   0.8382016408443451],\n",
       "  'running_regression_total_loss': [0.16497426204383372,\n",
       "   0.14569620788097382,\n",
       "   0.1456091507524252,\n",
       "   0.14501142457127572,\n",
       "   0.14492277994751931,\n",
       "   0.1447805505990982,\n",
       "   0.1451951715350151,\n",
       "   0.1455014345794916,\n",
       "   0.14571702979505063,\n",
       "   0.14521148033440112]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7f35c0c-792e-46a4-a818-0b29ba0b2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c5c667c-d503-4754-8335-a38e62ed267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1ksample_20part_T200_100epoch.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d0668-ca4c-4c43-9f4a-dfadc0b248ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
